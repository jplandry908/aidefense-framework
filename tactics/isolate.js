export const isolateTactic = {
    "name": "Isolate",
    "purpose": "The \"Isolate\" tactic involves implementing measures to contain malicious activity and limit its potential spread or impact should an AI system or one of its components become compromised. This includes sandboxing AI processes, segmenting networks to restrict communication, and establishing mechanisms to quickly quarantine or throttle suspicious interactions or misbehaving AI entities.",
    "techniques": [
        {
            "id": "AID-I-001",
            "name": "AI Execution Sandboxing & Runtime Isolation",
            "description": "Execute AI models, autonomous agents, or individual AI tools and plugins within isolated environments such as sandboxes, containers, or microVMs. These environments must be configured with strict limits on resources, permissions, and network connectivity. The primary goal is that if an AI component is compromised or behaves maliciously, the impact is confined to the isolated sandbox, preventing harm to the host system or lateral movement.",
            "defendsAgainst": [
                {
                    "framework": "MITRE ATLAS",
                    "items": [
                        "AML.T0053: AI Agent Tool Invocation",
                        "AML.T0020: Poison Training Data",
                        "AML.T0072: Reverse Shell",
                        "AML.T0050 Command and Scripting Interpreter",
                        "AML.T0029 Denial of AI Service",
                        "AML.T0034 Cost Harvesting (limiting rates)"
                    ]
                },
                {
                    "framework": "MAESTRO",
                    "items": [
                        "Compromised Container Images (L4)",
                        "Lateral Movement (Cross-Layer)",
                        "Agent Tool Misuse (L7)",
                        "Resource Hijacking (L4)",
                        "Framework Evasion (L3)"
                    ]
                },
                {
                    "framework": "OWASP LLM Top 10 2025",
                    "items": [
                        "LLM05:2025 Improper Output Handling",
                        "LLM06:2025 Excessive Agency",
                        "LLM10:2025 Unbounded Consumption"
                    ]
                }
            ],
            "subTechniques": [
                {
                    "id": "AID-I-001.001",
                    "name": "Container-Based Isolation", "pillar": ["infra"], "phase": ["operation"],
                    "description": "Utilizes container technologies like Docker or Kubernetes to package and run AI workloads in isolated user-space environments. This approach provides process and filesystem isolation and allows for resource management and network segmentation.",
                    "toolsOpenSource": [
                        "Docker",
                        "Podman",
                        "Kubernetes",
                        "OpenShift (container platform)",
                        "Falco (container runtime security)",
                        "Trivy (container vulnerability scanner)",
                        "Sysdig (container monitoring & security)",
                        "Calico (for Network Policies)",
                        "Cilium (for Network Policies and eBPF)"
                    ],
                    "toolsCommercial": [
                        "Docker Enterprise",
                        "Red Hat OpenShift Container Platform",
                        "Aqua Security",
                        "Twistlock (Palo Alto Networks)",
                        "Prisma Cloud (Palo Alto Networks)",
                        "Microsoft Azure Kubernetes Service (AKS)",
                        "Google Kubernetes Engine (GKE)",
                        "Amazon Elastic Kubernetes Service (EKS)"
                    ],
                    "defendsAgainst": [
                        {
                            "framework": "MITRE ATLAS",
                            "items": [
                                "AML.T0053 AI Agent Tool Invocation",
                                "AML.T0072 Reverse Shell",
                                "AML.T0029 Denial of AI Service",
                                "AML.T0034 Cost Harvesting"
                            ]
                        },
                        {
                            "framework": "MAESTRO",
                            "items": [
                                "Compromised Container Images (L4)",
                                "Lateral Movement (Cross-Layer)",
                                "Agent Tool Misuse (L7)",
                                "Resource Hijacking (L4)",
                                "Runtime Code Injection (L4)"
                            ]
                        },
                        {
                            "framework": "OWASP LLM Top 10 2025",
                            "items": [
                                "LLM06:2025 Excessive Agency",
                                "LLM10:2025 Unbounded Consumption"
                            ]
                        },
                        {
                            "framework": "OWASP ML Top 10 2023",
                            "items": [
                                "ML06:2023 AI Supply Chain Attacks",
                                "ML05:2023 Model Theft",
                                "ML09:2023 Output Integrity Attack"
                            ]
                        }
                    ],
                    "implementationStrategies": [
                        {
                            "strategy": "Deploy AI models and services in hardened, minimal-footprint container images.",
                            "howTo": "<h5>Concept:</h5><p>The attack surface of a container is directly related to the number of packages and libraries inside it. A multi-stage Docker build creates a small, final production image that contains only the essential application code and dependencies, omitting build tools, development libraries, and shell access, thereby reducing the attack surface.</p><h5>Implement a Multi-Stage Dockerfile</h5><p>The first stage (`build-env`) installs all dependencies. The final stage copies *only* the necessary application files from the build stage into a minimal base image like `python:3.10-slim`.</p><pre><code># File: Dockerfile\\n\\n# --- Build Stage ---\\n# Use a full-featured image for building dependencies\\nFROM python:3.10 as build-env\\nWORKDIR /app\\nCOPY requirements.txt .\\n# Install dependencies, including build tools\\nRUN pip install --no-cache-dir -r requirements.txt\\n\\n# --- Final Stage ---\\n# Use a minimal, hardened base image for production\\nFROM python:3.10-slim\\nWORKDIR /app\\n\\n# Create a non-root user for the application to run as\\nRUN useradd --create-home appuser\\nUSER appuser\\n\\n# Copy only the installed packages and application code from the build stage\\nCOPY --from=build-env /usr/local/lib/python3.10/site-packages/ /usr/local/lib/python3.10/site-packages/\\nCOPY --from=build-env /app/requirements.txt .\\nCOPY ./src ./src\\n\\n# Set the entrypoint\\nCMD [\\\"python\\\", \\\"./src/main.py\\\"]</code></pre><p><strong>Action:</strong> Use multi-stage builds for all AI service containers. The final image should be based on a minimal parent image (e.g., `-slim`, `distroless`) and should not contain build tools, compilers, or a shell unless absolutely necessary for the application's function.</p>"
                        },
                        {
                            "strategy": "Apply Kubernetes security contexts to restrict container privileges (e.g., runAsNonRoot).",
                            "howTo": "<h5>Concept:</h5><p>A Kubernetes `securityContext` allows you to define granular privilege and access controls for your pods and containers. This is a critical mechanism for enforcing the principle of least privilege, ensuring that even if an attacker gains code execution within a container, they cannot perform privileged operations.</p><h5>Define a Restrictive Security Context</h5><p>In your Kubernetes Deployment or Pod manifest, apply a `securityContext` that drops all Linux capabilities, prevents privilege escalation, runs as a non-root user, and enables a read-only root filesystem.</p><pre><code># File: k8s/deployment.yaml\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n Â name: my-inference-server\\nspec:\\n Â template:\\n Â  Â spec:\\n Â  Â  Â # Pod-level security context\\n Â  Â  Â securityContext:\\n Â  Â  Â  Â runAsNonRoot: true\\n Â  Â  Â  Â runAsUser: 1001\\n Â  Â  Â  Â runAsGroup: 1001\\n Â  Â  Â  Â fsGroup: 1001\\n Â  Â  Â containers:\\n Â  Â  Â - name: inference-api\\n Â  Â  Â  Â image: my-ml-app:latest\\n Â  Â  Â  Â # Container-level security context for fine-grained control\\n Â  Â  Â  Â securityContext:\\n Â  Â  Â  Â  Â # Prevent the process from gaining more privileges than its parent\\n Â  Â  Â  Â  Â allowPrivilegeEscalation: false\\n Â  Â  Â  Â  Â # Drop all Linux capabilities, then add back only what is needed (if any)\\n Â  Â  Â  Â  Â capabilities:\\n Â  Â  Â  Â  Â  Â drop:\\n Â  Â  Â  Â  Â  Â - \\\"ALL\\\"\\n Â  Â  Â  Â  Â # Make the root filesystem immutable to prevent tampering\\n Â  Â  Â  Â  Â readOnlyRootFilesystem: true\\n Â  Â  Â  Â volumeMounts:\\n Â  Â  Â  Â  Â # Provide a writable temporary directory if the application needs it\\n Â  Â  Â  Â  Â - name: tmp-storage\\n Â  Â  Â  Â  Â  Â mountPath: /tmp\\n Â  Â  Â volumes:\\n Â  Â  Â  Â - name: tmp-storage\\n Â  Â  Â  Â  Â emptyDir: {}</code></pre><p><strong>Action:</strong> Apply a `securityContext` to all production AI workloads in Kubernetes. At a minimum, set `runAsNonRoot: true`, `allowPrivilegeEscalation: false`, and `readOnlyRootFilesystem: true`.</p>"
                        },
                        {
                            "strategy": "Use network policies to enforce least-privilege communication between AI pods.",
                            "howTo": "<h5>Concept:</h5><p>By default, all pods in a Kubernetes cluster can communicate with each other. A `NetworkPolicy` acts as a firewall for your pods, allowing you to define explicit rules about which pods can connect to which other pods. A 'default-deny' posture is a core principle of Zero Trust networking.</p><h5>Step 1: Implement a Default-Deny Ingress Policy</h5><p>First, apply a policy that selects all pods in a namespace and denies all incoming (ingress) traffic. This creates a secure baseline.</p><pre><code># File: k8s/policies/default-deny.yaml\\napiVersion: networking.k8s.io/v1\\nkind: NetworkPolicy\\nmetadata:\\n Â name: default-deny-all-ingress\\n Â namespace: ai-production\\nspec:\\n Â podSelector: {}\\n Â policyTypes:\\n Â - Ingress</code></pre><h5>Step 2: Create Explicit Allow Rules</h5><p>Now, create specific policies to allow only the required traffic. This example allows pods with the label `app: api-gateway` to connect to pods with the label `app: inference-server` on port 8080.</p><pre><code># File: k8s/policies/allow-gateway-to-inference.yaml\\napiVersion: networking.k8s.io/v1\\nkind: NetworkPolicy\\nmetadata:\\n Â name: allow-gateway-to-inference\\n Â namespace: ai-production\\nspec:\\n Â podSelector:\\n Â  Â matchLabels:\\n Â  Â  Â app: inference-server # This is the destination\\n Â policyTypes:\\n Â - Ingress\\n Â ingress:\\n Â - from:\\n Â  Â - podSelector:\\n Â  Â  Â  Â matchLabels:\\n Â  Â  Â  Â  Â app: api-gateway # This is the allowed source\\n Â  Â ports:\\n Â  Â - protocol: TCP\\n Â  Â  Â port: 8080</code></pre><p><strong>Action:</strong> In your Kubernetes namespaces, deploy a `default-deny-all-ingress` policy. Then, for each service, add a specific `NetworkPolicy` that only allows ingress from its required upstream sources, blocking all other network paths.</p>"
                        },
                        {
                            "strategy": "Set strict resource quotas (CPU, memory, GPU) to prevent resource exhaustion attacks.",
                            "howTo": "<h5>Concept:</h5><p>A compromised or buggy AI model could enter an infinite loop or process a malicious input that consumes an enormous amount of CPU, memory, or GPU resources. Setting resource `limits` prevents a single misbehaving container from causing a denial-of-service attack that affects the entire node or cluster.</p><h5>Define Requests and Limits</h5><p>In your Kubernetes Deployment manifest, specify both `requests` (the amount of resources guaranteed for the pod) and `limits` (the absolute maximum the container can use).</p><pre><code># File: k8s/deployment-with-resources.yaml\\napiVersion: apps/v1\\nkind: Deployment\\n# ... metadata ...\\nspec:\\n Â template:\\n Â  Â spec:\\n Â  Â  Â containers:\\n Â  Â  Â - name: gpu-inference-server\\n Â  Â  Â  Â image: my-gpu-ml-app:latest\\n Â  Â  Â  Â resources:\\n Â  Â  Â  Â  Â # Requesting resources helps Kubernetes with scheduling\\n Â  Â  Â  Â  Â requests:\\n Â  Â  Â  Â  Â  Â memory: \\\"4Gi\\\"\\n Â  Â  Â  Â  Â  Â cpu: \\\"1000m\\\" # 1 full CPU core\\n Â  Â  Â  Â  Â  Â nvidia.com/gpu: \\\"1\\\"\\n Â  Â  Â  Â  Â # Limits prevent resource exhaustion attacks\\n Â  Â  Â  Â  Â limits:\\n Â  Â  Â  Â  Â  Â memory: \\\"8Gi\\\"\\n Â  Â  Â  Â  Â  Â cpu: \\\"2000m\\\" # 2 full CPU cores\\n Â  Â  Â  Â  Â  Â nvidia.com/gpu: \\\"1\\\"</code></pre><p><strong>Action:</strong> For all production deployments, define explicit CPU, memory, and GPU resource `requests` and `limits`. The limit acts as a hard cap that will cause the container to be throttled or terminated if exceeded, protecting the rest of the system.</p>"
                        },
                        {
                            "strategy": "Mount filesystems as read-only wherever possible.",
                            "howTo": "<h5>Concept:</h5><p>Making the container's root filesystem read-only is a powerful security control. If an attacker gains code execution, they cannot write malware to disk, modify configuration files, install new packages, or tamper with the AI model files because the filesystem is immutable.</p><h5>Step 1: Set the readOnlyRootFilesystem Flag</h5><p>In the container's `securityContext` within your Kubernetes manifest, set `readOnlyRootFilesystem` to `true`.</p><h5>Step 2: Provide Writable Temporary Storage if Needed</h5><p>If your application legitimately needs to write temporary files, provide a dedicated writable volume using an `emptyDir` and mount it to a specific path (like `/tmp`).</p><pre><code># File: k8s/readonly-fs-deployment.yaml\\napiVersion: apps/v1\\nkind: Deployment\\n# ... metadata ...\\nspec:\\n Â template:\\n Â  Â spec:\\n Â  Â  Â containers:\\n Â  Â  Â - name: inference-api\\n Â  Â  Â  Â image: my-ml-app:latest\\n Â  Â  Â  Â securityContext:\\n Â  Â  Â  Â  Â # This is the primary control\\n Â  Â  Â  Â  Â readOnlyRootFilesystem: true\\n Â  Â  Â  Â  Â allowPrivilegeEscalation: false\\n Â  Â  Â  Â  Â capabilities:\\n Â  Â  Â  Â  Â  Â drop: [\\\"ALL\\\"]\\n Â  Â  Â  Â volumeMounts:\\n Â  Â  Â  Â  Â # Mount a dedicated, writable emptyDir volume for temporary files\\n Â  Â  Â  Â  Â - name: tmp-writable-storage\\n Â  Â  Â  Â  Â  Â mountPath: /tmp\\n Â  Â  Â volumes:\\n Â  Â  Â  Â # Define the emptyDir volume. Its contents are ephemeral.\\n Â  Â  Â  Â - name: tmp-writable-storage\\n Â  Â  Â  Â  Â emptyDir: {}</code></pre><p><strong>Action:</strong> Enable `readOnlyRootFilesystem` for all production containers. If temporary write access is required, provide an `emptyDir` volume mounted at a non-root path like `/tmp`.</p>"
                        }
                    ]
                },
                {
                    "id": "AID-I-001.002",
                    "name": "MicroVM & Low-Level Sandboxing", "pillar": ["infra"], "phase": ["operation"],
                    "description": "Employs lightweight Virtual Machines (MicroVMs) or kernel-level sandboxing technologies to provide a stronger isolation boundary than traditional containers. This is critical for running untrusted code or highly sensitive AI workloads.",
                    "warning": {
                        "level": "Low to Medium on Startup Time & CPU/Memory Overhead",
                        "description": "<p>Stronger isolation technologies like gVisor or Firecracker impose a greater performance penalty than standard containers. <p><strong>CPU Overhead:</strong> Can introduce a <strong>5% to 15% CPU performance overhead</strong> compared to running in a standard container. <p><strong>Startup Time:</strong> Adds a small but measurable delay, typically <strong>5ms to 50ms</strong> of additional startup time per instance."
                    },
                    "toolsOpenSource": [
                        "Kata Containers (using QEMU or Firecracker)",
                        "Firecracker (AWS open-source microVM monitor)",
                        "gVisor (Google open-source user-space kernel)",
                        "seccomp-bpf (Linux kernel feature)",
                        "Wasmtime (WebAssembly runtime)",
                        "Wasmer (WebAssembly runtime)",
                        "eBPF (Extended Berkeley Packet Filter)",
                        "Cloud Hypervisor"
                    ],
                    "toolsCommercial": [
                        "AWS Lambda (built on Firecracker)",
                        "Google Cloud Run (uses gVisor)",
                        "Azure Container Instances (ACI) with confidential computing options",
                        "Red Hat OpenShift Virtualization (for Kata Containers management)",
                        "WebAssembly-as-a-Service platforms"
                    ],
                    "defendsAgainst": [
                        {
                            "framework": "MITRE ATLAS",
                            "items": [
                                "AML.T0053 AI Agent Tool Invocation",
                                "AML.T0072 Reverse Shell",
                                "AML.T0017 Persistence",
                                "AML.T0029 Denial of AI Service",
                                "AML.T0034 Cost Harvesting",
                                "AML.T0018.002 Manipulate AI Model: Embed Malware"
                            ]
                        },
                        {
                            "framework": "MAESTRO",
                            "items": [
                                "Compromised Container Images (L4)",
                                "Lateral Movement (Cross-Layer)",
                                "Agent Tool Misuse (L7)",
                                "Resource Hijacking (L4)",
                                "Runtime Code Injection (L4)",
                                "Memory Corruption (L4)",
                                "Privilege Escalation (L6)"
                            ]
                        },
                        {
                            "framework": "OWASP LLM Top 10 2025",
                            "items": [
                                "LLM06:2025 Excessive Agency",
                                "LLM10:2025 Unbounded Consumption",
                                "LLM03:2025 Supply Chain"
                            ]
                        },
                        {
                            "framework": "OWASP ML Top 10 2023",
                            "items": [
                                "ML06:2023 AI Supply Chain Attacks",
                                "ML09:2023 Output Integrity Attack",
                                "ML05:2023 Model Theft"
                            ]
                        }
                    ],
                    "implementationStrategies": [
                        {
                            "strategy": "Use lightweight VMs like Firecracker or Kata Containers for strong hardware-virtualized isolation.",
                            "howTo": "<h5>Concept:</h5><p>When you need to run highly untrusted code, such as a code interpreter tool for an AI agent, standard container isolation may not be sufficient. MicroVMs like Kata Containers provide a full, lightweight hardware-virtualized environment for each pod, giving it its own kernel and isolating it from the host kernel. This provides a much stronger security boundary.</p><h5>Step 1: Define a Kata `RuntimeClass` in Kubernetes</h5><p>First, your cluster administrator must install the Kata Containers runtime. Then, they create a `RuntimeClass` object that makes this runtime available for pods to request.</p><pre><code># File: k8s/runtimeclass-kata.yaml\\napiVersion: node.k8s.io/v1\\nkind: RuntimeClass\\nmetadata:\\n Â name: kata-qemu # Name of the runtime class\\n# The handler name must match how it was configured in the CRI-O/containerd node setup\\nhandler: kata-qemu</code></pre><h5>Step 2: Request the Kata Runtime in Your Pod Spec</h5><p>In the Pod specification for your untrusted workload, you specify the `runtimeClassName` to instruct Kubernetes to run this pod inside a Kata MicroVM.</p><pre><code># File: k8s/kata-pod.yaml\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n Â name: untrusted-code-interpreter\\nspec:\\n Â # This line tells Kubernetes to use the Kata Containers runtime\\n Â runtimeClassName: kata-qemu\\n Â containers:\\n Â - name: code-runner\\n Â  Â image: my-secure-code-runner:latest\\n Â  Â # This container will now run in its own lightweight VM</code></pre><p><strong>Action:</strong> For workloads that execute arbitrary code from untrusted sources (e.g., an agentic 'code interpreter' tool), deploy them as pods that explicitly request a hardware-virtualized runtime like Kata Containers via a `RuntimeClass`.</p>"
                        },
                        {
                            "strategy": "Apply OS-level sandboxing with tools like gVisor to intercept and filter system calls.",
                            "howTo": "<h5>Concept:</h5><p>gVisor provides a strong isolation boundary without the overhead of a full VM. It acts as an intermediary 'guest kernel' written in a memory-safe language (Go), intercepting system calls from the sandboxed application and handling them safely in user space. This dramatically reduces the attack surface exposed to the application, as it can no longer directly interact with the host's real Linux kernel.</p><h5>Step 1: Define a gVisor `RuntimeClass`</h5><p>Similar to Kata Containers, your cluster administrator must first install gVisor (using the `runsc` runtime) on the cluster nodes and create a `RuntimeClass` to expose it.</p><pre><code># File: k8s/runtimeclass-gvisor.yaml\\napiVersion: node.k8s.io/v1\\nkind: RuntimeClass\\nmetadata:\\n Â name: gvisor\\nhandler: runsc # The gVisor runtime handler</code></pre><h5>Step 2: Request the gVisor Runtime in Your Pod Spec</h5><p>In the pod manifest for the workload you want to sandbox, set the `runtimeClassName` to `gvisor`.</p><pre><code># File: k8s/gvisor-pod.yaml\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n Â name: sandboxed-data-parser\\nspec:\\n Â # This pod will be sandboxed with gVisor\\n Â runtimeClassName: gvisor\\n Â containers:\\n Â - name: parser\\n Â  Â image: my-data-parser:latest\\n Â  Â # This container's syscalls will be intercepted by gVisor</code></pre><p><strong>Action:</strong> Use gVisor for applications that process complex, potentially malicious file formats or handle untrusted data where the primary risk is exploiting a vulnerability in the host OS kernel's system call interface.</p>"
                        },
                        {
                            "strategy": "Define strict seccomp-bpf profiles to whitelist only necessary system calls for model inference.",
                            "howTo": "<h5>Concept:</h5><p>Seccomp (Secure Computing Mode) is a Linux kernel feature that restricts the system calls a process can make. By creating a `seccomp` profile that explicitly whitelists only the syscalls your application needs to function, you can block an attacker from using dangerous syscalls (like `mount`, `reboot`, `ptrace`) even if they achieve code execution inside the container.</p><h5>Step 1: Generate a Seccomp Profile</h5><p>You can use tools like `strace` or specialized profile generators to trace your application during normal operation and automatically create a list of required syscalls.</p><h5>Step 2: Create the Profile JSON and Apply It</h5><p>The profile is a JSON file that lists the allowed syscalls. The `defaultAction` is set to `SCMP_ACT_ERRNO`, which means any syscall *not* on the list will be blocked.</p><pre><code># File: /var/lib/kubelet/seccomp/profiles/inference-profile.json\\n{\\n Â  Â \\\"defaultAction\\\": \\\"SCMP_ACT_ERRNO\\\",\\n Â  Â \\\"architectures\\\": [\\\"SCMP_ARCH_X86_64\\\"],\\n Â  Â \\\"syscalls\\\": [\\n Â  Â  Â  Â {\\\"names\\\": [\\\"accept4\\\", \\\"bind\\\", \\\"brk\\\", \\\"close\\\", \\\"epoll_wait\\\", \\\"futex\\\", \\\"mmap\\\", \\\"mprotect\\\", \\\"munmap\\\", \\\"read\\\", \\\"recvfrom\\\", \\\"sendto\\\", \\\"socket\\\", \\\"write\\\"], \\\"action\\\": \\\"SCMP_ACT_ALLOW\\\"}\\n Â  Â ]\\n}\\n</code></pre><p>This profile must be placed on the node. Then, you apply it to a pod via its `securityContext`.</p><pre><code># In your k8s/deployment.yaml\\n Â  Â  Â securityContext:\\n Â  Â  Â  Â seccompProfile:\\n Â  Â  Â  Â  Â # Use a profile saved on the node\\n Â  Â  Â  Â  Â type: Localhost\\n Â  Â  Â  Â  Â localhostProfile: profiles/inference-profile.json</code></pre><p><strong>Action:</strong> Generate a minimal `seccomp` profile for your AI inference server. Deploy this profile to all cluster nodes and apply it to your production pods via the `securityContext`. This provides a strong, kernel-enforced layer of defense against privilege escalation and container breakout attempts.</p>"
                        },
                        {
                            "strategy": "Utilize WebAssembly (WASM) runtimes to run AI models in a high-performance, secure sandbox.",
                            "howTo": "<h5>Concept:</h5><p>WebAssembly (WASM) provides a high-performance, sandboxed virtual instruction set. Code compiled to WASM cannot interact with the host system (e.g., read files, open network sockets) unless those capabilities are explicitly passed into the sandbox by the host runtime. This makes it an excellent choice for safely executing small, self-contained pieces of untrusted code, like an individual model's inference logic.</p><h5>Step 1: Compile Inference Code to WASM</h5><p>Write your inference logic in a language that can compile to WASM, such as Rust. Use a library like `tract` to run an ONNX model.</p><pre><code>// File: inference-engine/src/lib.rs (Rust)\\nuse tract_onnx::prelude::*;\n\\n#[no_mangle]\\npub extern \\\"C\\\" fn run_inference(input_ptr: *mut u8, input_len: usize) -> i64 {\\n Â  Â // ... code to read input from WASM memory ...\\n Â  Â let model = tract_onnx::onnx().model_for_path(\\\"model.onnx\\\").unwrap();\\n Â  Â // ... run inference ...\\n Â  Â // ... write output to WASM memory and return a pointer ...\\n Â  Â return prediction;\\n}\\n</code></pre><h5>Step 2: Run the WASM Module in a Secure Runtime</h5><p>Use a WASM runtime like `wasmtime` in a host application (e.g., written in Python) to load and execute the compiled `.wasm` file. Crucially, the host does not grant the WASM module any filesystem or network permissions.</p><pre><code># File: host_app/run_wasm.py\\nfrom wasmtime import Store, Module, Instance, Linker\\n\\n# 1. Create a store and load the compiled .wasm module\\nstore = Store()\\nmodule = Module.from_file(store.engine, \\\"./inference_engine.wasm\\\")\\n\\n# 2. Link imports. By providing an empty linker, we grant NO capabilities to the sandbox.\\nlinker = Linker(store.engine)\\ninstance = linker.instantiate(store, module)\\n\\n# 3. Get the exported inference function\\nrun_inference = instance.exports(store)[\\\"run_inference\\\"]\\n\\n# ... code to allocate memory in the sandbox, write the input data ...\\n\\n# 4. Call the sandboxed WASM function\\nprediction = run_inference(store, ...)\\nprint(f\\\"Inference result from WASM sandbox: {prediction}\\\")</code></pre><p><strong>Action:</strong> For well-defined, self-contained AI tasks, consider compiling the inference logic to WebAssembly. Run the resulting `.wasm` module in a secure runtime like Wasmtime, explicitly denying it access to the filesystem and network to create a high-performance, capabilities-based sandbox.</p>"
                        }
                    ]
                },
                {
                    "id": "AID-I-001.003",
                    "name": "Ephemeral Single-Use Sandboxes for Tools",
                    "pillar": ["infra"],
                    "phase": ["operation"],
                    "description": "Run tool executions inside strongly isolated, single-use sandboxes (e.g., microVMs). Destroy the environment immediately after one invocation to prevent persistence and cross-session contamination.",
                    "defendsAgainst": [
                        { "framework": "MITRE ATLAS", "items": ["AML.T0050 Command and Scripting Interpreter"] },
                        { "framework": "MAESTRO", "items": ["Runtime Code Injection (L4)", "Lateral Movement (Cross-Layer)"] },
                        { "framework": "OWASP LLM Top 10 2025", "items": ["LLM06:2025 Excessive Agency"] },
                        { "framework": "OWASP ML Top 10 2023", "items": ["ML06:2023 AI Supply Chain Attacks"] }
                    ],
                    "implementationStrategies": [
                        {
                            "strategy": "Provision a fresh, single-use sandbox (microVM / gVisor / Kata) for every tool invocation, execute once, then destroy it.",
                            "howTo": "<h5>Concept:</h5><p>Instead of keeping a long-running sandbox, the system should treat every high-risk tool execution (e.g., code interpreter, browser automation, filesystem access, network fetcher) as untrusted. For each invocation, it spins up a brand-new isolated runtime (microVM, gVisor-sandboxed pod, Kata-backed pod), injects only the minimum inputs needed for that call, executes the requested action, captures outputs, then force-destroys the entire sandbox including its temporary filesystem. <strong>No state is ever reused between calls.</strong></p><h5>Why this matters:</h5><p>This design prevents persistence and data residue. If an attacker manages to drop malware, credentials, API keys, model weights, or lateral movement tooling inside the sandbox, those artifacts disappear immediately after the single execution finishes. It also prevents tool-to-tool contamination between separate agent steps.</p><h5>Step-by-step Orchestration Flow:</h5><ol><li><strong>Spawn:</strong> Programmatically request a microVM-backed or gVisor-backed workload via a runtime class (e.g. Kata Containers in Kubernetes). This environment starts from a known-clean snapshot or base image.</li><li><strong>Inject Task Input:</strong> Copy only the minimal required data (the script, query, file chunk, etc.) into the sandbox. Do not mount shared long-lived volumes or host credentials.</li><li><strong>Execute:</strong> Run the tool inside that sandbox and capture stdout/stderr/result objects.</li><li><strong>Collect Result:</strong> Serialize the output (e.g. JSON result, processed file) and pass it back to the calling agent or service interface.</li><li><strong>Teardown:</strong> Immediately terminate the sandbox VM/pod and wipe all associated ephemeral storage. Never reuse the same instance for a later request.</li></ol><pre><code># Conceptual pseudocode for single-use sandbox execution\nimport sandbox_runtime\n\ndef run_tool_once(tool_payload: bytes) -> dict:\n    # 1. Create fresh sandbox instance (microVM / Kata / gVisor-backed pod)\n    sb = sandbox_runtime.spawn_isolated_instance()\n\n    try:\n        # 2. Copy the tool payload (e.g. Python script, shell snippet)\n        sb.copy_to_guest(content=tool_payload, guest_path=\"/app/task_input\")\n\n        # 3. Execute inside the sandbox\n        exit_code, stdout, stderr = sb.exec_guest(\"/usr/local/bin/run_tool /app/task_input\")\n\n        # 4. Capture results for the caller\n        result = {\n            \"exit_code\": exit_code,\n            \"stdout\": stdout,\n            \"stderr\": stderr\n        }\n        return result\n    finally:\n        # 5. Guaranteed teardown: destroy the sandbox and wipe ephemeral storage\n        sb.destroy()\n        # No sandbox state is reused for future calls</code></pre><p><strong>Action:</strong> Treat every agent tool call (code execution, file transform, web fetcher) as untrusted. Enforce a lifecycle policy of <code>spawn â†’ run â†’ collect â†’ destroy</code>. This eliminates persistence, prevents credential reuse, and blocks post-exploitation lateral movement across tool invocations.</p>"
                        }
                    ],
                    "toolsOpenSource": ["gVisor", "Kata Containers", "Firecracker"],
                    "toolsCommercial": ["AWS Firecracker-backed services (Lambda, Fargate)", "Google GKE Sandbox"]
                },
                {
                    "id": "AID-I-001.004",
                    "name": "Seccomp-bpf & Network Egress Restrictions",
                    "pillar": ["infra"],
                    "phase": ["operation"],
                    "description": "Minimize kernel/system call surface and restrict outbound network destinations for sandboxed executions to reduce post-exploitation blast radius.",
                    "defendsAgainst": [
                        { "framework": "MITRE ATLAS", "items": ["AML.T0072 Reverse Shell"] },
                        { "framework": "MAESTRO", "items": ["Runtime Code Injection (L4)", "Lateral Movement (Cross-Layer)"] },
                        { "framework": "OWASP LLM Top 10 2025", "items": ["LLM06:2025 Excessive Agency"] },
                        { "framework": "OWASP ML Top 10 2023", "items": ["ML06:2023 AI Supply Chain Attacks"] }
                    ],
                    "implementationStrategies": [
                        {
                            "strategy": "Apply minimal seccomp profiles and enforce egress-allowlist NetworkPolicies.",
                            "howTo": "<h5>Concept:</h5><p>This control locks down two of the most critical breakout vectors after code execution inside a sandbox: 1) kernel/syscall abuse and 2) uncontrolled outbound network access (exfiltration, reverse shells, C2 beacons). We do this in two layers: (a) a seccomp-bpf profile that only allows the small set of syscalls the workload truly needs, and (b) a Kubernetes NetworkPolicy that denies all egress by default and explicitly allows traffic only to known-safe internal services or addresses.</p><h5>Part 1: Minimal Seccomp Profile</h5><p>Generate (or handcraft) a seccomp profile that whitelists only the syscalls required for model inference or tool logic. Everything else is blocked with <code>SCMP_ACT_ERRNO</code>. Then attach that profile via <code>securityContext.seccompProfile</code> in the Pod spec.</p><pre><code># File: /var/lib/kubelet/seccomp/profiles/tool-sandbox.json\n{\n  \"defaultAction\": \"SCMP_ACT_ERRNO\",\n  \"architectures\": [\"SCMP_ARCH_X86_64\"],\n  \"syscalls\": [\n    {\n      \"names\": [\n        \"accept4\", \"bind\", \"brk\", \"close\", \"epoll_wait\",\n        \"futex\", \"mmap\", \"mprotect\", \"munmap\",\n        \"read\", \"recvfrom\", \"sendto\", \"socket\", \"write\"\n      ],\n      \"action\": \"SCMP_ACT_ALLOW\"\n    }\n  ]\n}</code></pre><pre><code># In your Pod/Deployment spec (excerpt)\nspec:\n  containers:\n  - name: my-tool-container\n    image: my-secure-runner:latest\n    securityContext:\n      allowPrivilegeEscalation: false\n      readOnlyRootFilesystem: true\n      capabilities:\n        drop: [\"ALL\"]\n      seccompProfile:\n        type: Localhost\n        localhostProfile: \"profiles/tool-sandbox.json\"</code></pre><p><strong>Effect:</strong> Even if an attacker achieves arbitrary code execution inside this container/pod, many dangerous syscalls (e.g. <code>ptrace</code>, <code>mount</code>, <code>reboot</code>, <code>clone3</code> variants used for container escapes) are simply not available. This sharply limits kernel exploitation and privilege escalation paths.</p><h5>Part 2: Default-Deny Egress with Explicit Allowlist</h5><p>By default, Kubernetes lets pods call out to anything on the network. That makes data exfiltration and reverse shells trivial. You must apply a NetworkPolicy that denies egress unless it matches an explicit allow rule. Then you selectively allow only the destinations this sandbox truly needs (for example, an internal inference API or a metadata service proxy).</p><pre><code># File: k8s/policies/default-deny-egress.yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny-all-egress\n  namespace: ai-sandbox\nspec:\n  podSelector: {}\n  policyTypes:\n  - Egress\n  egress: []  # No egress allowed by default</code></pre><pre><code># File: k8s/policies/allow-egress-to-internal-api.yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-sandbox-to-internal-api\n  namespace: ai-sandbox\nspec:\n  podSelector:\n    matchLabels:\n      role: sandboxed-tool\n  policyTypes:\n  - Egress\n  egress:\n  - to:\n    - namespaceSelector:\n        matchLabels:\n          name: ai-core-services\n      podSelector:\n        matchLabels:\n          app: inference-gateway\n    ports:\n    - protocol: TCP\n      port: 8443</code></pre><p><strong>Effect:</strong> The sandboxed pod can only talk to the explicitly approved service(s) on approved ports. Any attempt to open an arbitrary reverse shell to the internet, beacon to an attacker C2 host, or exfiltrate secrets to an external address will be blocked at the cluster network layer.</p><p><strong>Action:</strong> For every high-risk AI runtime (agent tool runner, model inference worker handling untrusted input, code-execution sandbox), attach: (1) a tight seccomp profile via <code>securityContext</code> to remove dangerous syscalls, and (2) a default-deny egress NetworkPolicy plus narrow allowlists. Together, these two controls drastically reduce the blast radius of post-exploitation activity, lateral movement, and data exfiltration.</p>"
                        }
                    ],
                    "toolsOpenSource": ["seccomp-bpf", "Kubernetes NetworkPolicy"],
                    "toolsCommercial": ["Calico Enterprise", "Cilium Enterprise"]
                },
                {
                    "id": "AID-I-001.005",
                    "name": "Pre-Execution Behavioral Analysis in Ephemeral Sandboxes",
                    "pillar": ["infra", "app"],
                    "phase": ["operation", "validation"],
                    "description": "This proactive defense technique subjects any AI-generated executable artifact (e.g., scripts, binaries, container images created by an agent) to mandatory behavioral analysis within a short-lived, strongly isolated sandbox (such as a microVM) *before* it is deployed or executed in a production context. This pre-execution security gate applies to artifacts originating from both automated CI/CD pipelines and interactive developer IDEs, serving as a final vetting step to contain threats from malicious AI-generated code before they can have any impact.",
                    "toolsOpenSource": [
                        "Firecracker",
                        "Kata Containers",
                        "gVisor",
                        "QEMU/KVM",
                        "Falco",
                        "Cilium Tetragon",
                        "strace",
                        "Sysdig",
                        "Wazuh (in-guest EDR)"
                    ],
                    "toolsCommercial": [
                        "Joe Sandbox",
                        "ANY.RUN",
                        "EDR/XDR platforms with sandboxing features",
                        "Execution Platforms (Note: AWS Lambda/Fargate are execution platforms that use microVMs; they can host a sandboxing service but do not provide behavioral analysis out-of-the-box.)"
                    ],
                    "defendsAgainst": [
                        {
                            "framework": "MITRE ATLAS",
                            "items": [
                                "AML.T0050 Command and Scripting Interpreter",
                                "AML.T0072 Reverse Shell",
                                "AML.T0018.002 Manipulate AI Model: Embed Malware",
                                "AML.T0025 Exfiltration via Cyber Means"
                            ]
                        },
                        {
                            "framework": "MAESTRO",
                            "items": [
                                "Runtime Code Injection (L4)",
                                "Agent Tool Misuse (L7)",
                                "Lateral Movement (Cross-Layer)",
                                "Resource Hijacking (L4)"
                            ]
                        },
                        {
                            "framework": "OWASP LLM Top 10 2025",
                            "items": [
                                "LLM06:2025 Excessive Agency",
                                "LLM05:2025 Improper Output Handling"
                            ]
                        },
                        {
                            "framework": "OWASP ML Top 10 2023",
                            "items": [
                                "ML06:2023 AI Supply Chain Attacks",
                                "ML09:2023 Output Integrity Attack"
                            ]
                        }
                    ],
                    "implementationStrategies": [
                        {
                            "strategy": "Orchestrate an automated analysis workflow using microVMs for strong isolation.",
                            "howTo": "<h5>Concept:</h5><p>Create a dedicated, fully automated pipeline for vetting AI-generated code. When an agent produces a script, the orchestrator spins up a new, clean microVM (using a technology like Firecracker), executes the script inside, monitors its behavior, and then destroys the VM. This ensures that each analysis is fresh and completely isolated from all other systems.</p><h5>Implement the Orchestration Logic</h5><pre><code># File: sandboxing_service/orchestrator.py\nimport firecracker_sdk\n\n# This is a conceptual workflow for a sandboxing service\ndef analyze_script_in_sandbox(script_content: str) -> bool:\n    # 1. Provision a new, ephemeral microVM from a clean snapshot\n    vm = firecracker_sdk.microvm.new()\n    # ... (Configure networking, kernel, rootfs for the VM) ...\n    vm.start()\n\n    # 2. Copy the AI-generated script into the running microVM\n    vm.copy_file_to_guest(host_path=script_content_path, guest_path=\"/app/run.py\")\n\n    # 3. Start a runtime security monitor (e.g., Falco) inside the VM or on the host's network interface\n    start_monitoring(vm.id)\n\n    # 4. Execute the script within the microVM\n    exit_code, stdout, stderr = vm.execute_command(\"python /app/run.py\")\n\n    # 5. Stop monitoring and analyze the collected logs\n    behavior_logs = stop_monitoring(vm.id)\n    is_malicious = analyze_behavior_logs(behavior_logs)\n\n    # 6. Destroy the microVM completely\n    vm.stop()\n\n    if is_malicious:\n        print(f\"ðŸš¨ Malicious behavior detected in script. Execution blocked.\")\n        return False\n    \n    print(\"âœ… Script behavior verified as safe.\")\n    return True\n</code></pre><p><strong>Action:</strong> Build a dedicated, API-driven sandboxing service using a microVM technology like Firecracker. All AI-generated code must be submitted to this service for analysis before it can be used, and the service must destroy and recreate the analysis environment for every request.</p>"
                        },
                        {
                            "strategy": "Define and enforce a strict behavioral security policy within the sandbox.",
                            "howTo": "<h5>Concept:</h5><p>The effectiveness of the sandbox depends on the rules used to judge behavior. A strict, default-deny policy should be created that defines what the generated code is allowed to do. Any action outside this narrow scope is considered malicious. All network egress should be forced through a monitored proxy.</p><h5>Implement a Falco Rule for a Code Interpreter</h5><p>Falco is a runtime security tool that can monitor kernel syscalls. This Falco rule defines the expected behavior for a sandboxed Python script. It allows file operations within `/tmp` but blocks network connections and file access elsewhere.</p><pre><code># File: sandbox_policies/falco_code_interpreter.yaml\n\n- rule: Disallowed Egress from AI-Sandbox\n  desc: Detects any outbound network connection from the sandboxed Python interpreter.\n  condition: >\n    evt.type = connect and evt.dir = > and proc.name = python3 and not proc.aname contains \"analysis_proxy\"\n  output: \"Disallowed network egress by AI-generated code (proc=%proc.name command=%proc.cmdline connection=%fd.name)\"\n  priority: CRITICAL\n  tags: [network, ai_sandbox]\n\n- rule: Disallowed File Write from AI-Sandbox\n  desc: Detects file writes outside of the /tmp directory by a sandboxed python process.\n  condition: >\n    (evt.type = openat or evt.type = open) and evt.dir = > and (fd.open_write=true) \n    and proc.name = python3 and not fd.name startswith /tmp/\n  output: \"Disallowed file write by AI-generated code (proc=%proc.name file=%fd.name)\"\n  priority: CRITICAL\n  tags: [filesystem, ai_sandbox]\n</code></pre><p><strong>Action:</strong> Define a strict behavioral policy for your pre-execution sandbox using a tool like Falco or Tetragon. The policy must deny all network egress by default (forcing traffic through a monitored proxy) and restrict file system writes to a designated temporary directory.</p>"
                        },
                        {
                            "strategy": "Generate a signed, verifiable analysis report for CI/CD admission control.",
                            "howTo": "<h5>Concept:</h5><p>The result of the sandbox analysis should be a formal, cryptographically signed report. This report acts as a 'passport' for the AI-generated artifact. A CI/CD pipeline or deployment orchestrator can then use this signed report as a verifiable prerequisite before admitting the artifact into a production environment.</p><h5>Implement a Report Generation and Signing Step</h5><pre><code># File: sandboxing_service/reporter.py\nimport json\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\n\ndef generate_signed_report(artifact_hash, verdict, analysis_logs):\n    # 1. Create the report payload\n    report = {\n        'artifact_sha256': artifact_hash,\n        'verdict': verdict, # 'ALLOWED' or 'DENIED'\n        'analysis_timestamp': datetime.utcnow().isoformat(),\n        'policy_version': '1.3',\n        'summary': analysis_logs # Summary of observed behaviors\n    }\n    report_bytes = json.dumps(report, sort_keys=True).encode('utf-8')\n\n    # 2. Sign the report with the sandbox service's private key\n    # private_key = load_sandbox_private_key()\n    signature = private_key.sign(\n        report_bytes,\n        padding.PSS(mgf=padding.MGF1(hashes.SHA256()), salt_length=padding.PSS.MAX_LENGTH),\n        hashes.SHA256()\n    )\n\n    return {'report': report, 'signature': signature.hex()}\n\n# The CI/CD pipeline would then check this signature before deploying the artifact.\n</code></pre><p><strong>Action:</strong> At the end of a sandbox analysis, generate a structured JSON report containing the artifact's hash and the verdict. Cryptographically sign this report with a key trusted by your CI/CD system. The deployment pipeline must include a step to verify this signature before allowing the artifact to be promoted.</p>"
                        },
                        {
                            "strategy": "Monitor for anti-analysis and sandbox evasion techniques.",
                            "howTo": "<h5>Concept:</h5><p>Advanced malicious code will actively try to detect if it's running in an analysis environment. The sandbox must be instrumented to detect these evasion attempts, such as detecting unusually long sleep calls, probing for VM-specific hardware IDs, or checking CPU features.</p><h5>Implement an Anti-Evasion Detection Policy</h5><p>This can be done with a combination of syscall monitoring and API hooking inside the sandbox.</p><pre><code># Conceptual Falco rule for detecting long sleep calls\n- rule: Suspicious Long Sleep\n  desc: An AI-generated script called sleep or usleep with a long duration, possibly to evade automated analysis.\n  condition: syscall.type = nanosleep and evt.arg.rqtp.tv_sec > 60 and proc.name = python3\n  output: \"Suspicious long sleep detected (duration=%evt.arg.rqtp.tv_sec) from AI-generated code.\"\n  priority: WARNING\n  tags: [anti-analysis, ai_sandbox]\n</code></pre><p><strong>Action:</strong> Enhance your sandbox's behavioral policy to include rules that detect common anti-analysis techniques. Flag any artifact that attempts to perform environment checks or exhibits unusually delayed execution as suspicious.</p>"
                        }
                    ]
                }
            ]
        },
        {
            "id": "AID-I-002",
            "name": "Network Segmentation & Isolation for AI Systems",
            "description": "Implement network segmentation and microsegmentation strategies using firewalls, proxies, private endpoints, and transport layer security to enforce strict communication boundaries for AI systems. This involves isolating internal components (e.g., training vs. inference environments, data stores) to limit lateral movement, and securing connections to external dependencies (e.g., MaaS APIs) to prevent data exfiltration, SSRF, and MitM attacks. The goal is to reduce the blast radius of a compromise by enforcing least-privilege network access both internally and externally.",
            "defendsAgainst": [
                {
                    "framework": "MITRE ATLAS",
                    "items": [
                        "AML.T0044 Full AI Model Access (limits access)",
                        "AML.T0036 Data from Information Repositories (limits access)",
                        "AML.T0025 Exfiltration via Cyber Means",
                        "AML.T0049 Exploit Public-Facing Application (e.g. SSRF-driven internal pivot / forced egress abuse)",
                        "AML.T0072 Reverse Shell"
                    ]
                },
                {
                    "framework": "MAESTRO",
                    "items": [
                        "Lateral Movement (Cross-Layer)",
                        "Compromised RAG Pipelines (L2, isolating internal DB access)",
                        "Data Exfiltration (Cross-Layer)",
                        "Orchestration Attacks (L4)"
                    ]
                },
                {
                    "framework": "OWASP LLM Top 10 2025",
                    "items": [
                        "LLM02:2025 Sensitive Information Disclosure (via exfiltration)",
                        "LLM06:2025 Excessive Agency",
                        "LLM03:2025 Supply Chain (reduces exposure to compromised upstream model providers / API dependencies)"
                    ]
                },
                {
                    "framework": "OWASP ML Top 10 2023",
                    "items": [
                        "ML05:2023 Model Theft",
                        "ML06:2023 ML Supply Chain Attacks"
                    ]
                }
            ],
            "subTechniques": [
                {
                    "id": "AID-I-002.001",
                    "name": "Internal AI Network Segmentation",
                    "pillar": ["infra"],
                    "phase": ["operation"],
                    "description": "Implement network segmentation and microsegmentation strategies to isolate AI systems and their *internal* components (e.g., training environments, model serving endpoints, data stores, agent control planes) from general corporate networks and other critical IT/OT systems. Enforces strict internal communication rules through firewalls, security groups, and network policies to limit lateral movement and reduce the internal blast radius of a compromise. This also isolates high-privilege agent backends (e.g. orchestration layers with access to credentials, vector DBs, or model registries) from lower-trust, user-facing inference frontends, so that a compromised public-facing agent cannot laterally move into data-rich components.",
                    "toolsOpenSource": [
                        "Linux Netfilter (iptables, nftables), firewalld",
                        "Kubernetes Network Policies",
                        "Service Mesh (Istio, Linkerd, Kuma) for internal policies",
                        "CNI plugins (Calico, Cilium)",
                        "Cloud provider CLIs/SDKs (AWS CLI, gcloud, Azure CLI)",
                        "Terraform, Ansible, CloudFormation, Pulumi (for IaC)"
                    ],
                    "toolsCommercial": [
                        "Microsegmentation platforms (Illumio, Guardicore [Akamai], Cisco Secure Workload)",
                        "Cloud-native firewall services (AWS Security Groups, Azure NSGs, GCP Firewall Rules)",
                        "Commercial Service Mesh offerings (e.g., Istio distributions with enterprise support)"
                    ],
                    "implementationStrategies": [
                        {
                            "strategy": "Host critical AI components on dedicated network segments (VLANs, VPCs).",
                            "howTo": "<h5>Concept:</h5><p>This is 'macro-segmentation'. By placing different environments (e.g., training, inference, data storage) in separate virtual networks, you create strong, high-level boundaries. A compromise in one segment, like a data science experimentation VPC, is prevented at the network level from accessing the production inference VPC.</p><h5>Define Separate VPCs with Infrastructure as Code</h5><p>Use a tool like Terraform to define distinct, non-overlapping Virtual Private Clouds (VPCs) for each environment. This ensures the separation is deliberate, version-controlled, and reproducible.</p><pre><code># File: infrastructure/networks.tf (Terraform)\n\n# VPC for the production AI inference service\nresource \"aws_vpc\" \"prod_vpc\" {\n  cidr_block = \"10.0.0.0/16\"\n  tags = { Name = \"aidefend-prod-vpc\" }\n}\n\n# A completely separate VPC for the AI model training environment\nresource \"aws_vpc\" \"training_vpc\" {\n  cidr_block = \"10.1.0.0/16\"\n  tags = { Name = \"aidefend-training-vpc\" }\n}\n\n# A VPC for the data science team's sandboxed experimentation\nresource \"aws_vpc\" \"sandbox_vpc\" {\n  cidr_block = \"10.2.0.0/16\"\n  tags = { Name = \"aidefend-sandbox-vpc\" }\n}\n\n# By default, these VPCs cannot communicate with each other.\n# Any connection (e.g., VPC Peering) must be explicitly defined and secured.</code></pre><p><strong>Action:</strong> Provision separate, dedicated VPCs for your production, staging, and development/training environments. Do not allow VPC peering between them by default. All promotion of artifacts (like models) between environments should happen through a secure, audited CI/CD pipeline that connects to registries, not by direct network access between the VPCs.</p>"
                        },
                        {
                            "strategy": "Apply least privilege to *internal* network communications for AI systems.",
                            "howTo": "<h5>Concept:</h5><p>Within a VPC, use firewall rules (like Security Groups in AWS) to enforce least-privilege access between components. A resource should only be able to receive traffic on the specific ports and from the specific internal sources it absolutely needs to function. All other traffic should be denied.</p><h5>Create Fine-Grained Security Group Rules</h5><p>This Terraform example defines two security groups. The first is for a model inference server, which only allows traffic on port 8080 from the second security group, which is attached to an internal API gateway. This prevents anyone else, including other services in the same VPC, from directly accessing the model. For default-deny egress in AWS Security Groups, explicitly set an empty egress rule list.</p><pre><code># File: infrastructure/security_groups.tf (Terraform)\ndata \"aws_vpc\" \"prod_vpc\" {\n  filter { name = \"tag:Name\" values = [\"aidefend-prod-vpc\"] }\n}\n\n# Security group for the Internal API Gateway\nresource \"aws_security_group\" \"internal_api_gateway_sg\" {\n  name_prefix = \"internal-api-gateway-sg-\"\n  vpc_id      = data.aws_vpc.prod_vpc.id\n  tags        = { Name = \"internal-api-gateway-sg\" }\n  # Define egress rules needed for the gateway itself as required\n}\n\n# Security group for the Model Inference service\nresource \"aws_security_group\" \"inference_sg\" {\n  name_prefix = \"inference-server-sg-\"\n  vpc_id      = data.aws_vpc.prod_vpc.id\n  tags        = { Name = \"inference-server-sg\" }\n\n  # Ingress Rule: Allow traffic ONLY from the Internal API Gateway on the app port\n  ingress {\n    description              = \"Allow TCP 8080 from Internal API Gateway\"\n    from_port                = 8080\n    to_port                  = 8080\n    protocol                 = \"tcp\"\n    source_security_group_id = aws_security_group.internal_api_gateway_sg.id\n  }\n\n  # Default-deny egress: explicitly empty egress list\n  egress = []\n}\n</code></pre><p><strong>Action:</strong> For each component of your AI system, create a dedicated security group. Define ingress rules that only allow traffic from the specific security groups of the internal services that need to connect to it. Implement default-deny egress by setting <code>egress = []</code>, then add narrowly-scoped outbound rules only if strictly required. Note: Some cloud consoles auto-add 'allow all egress' to new Security Groups. Using Infrastructure as Code (Terraform/CloudFormation) lets you enforce egress = [] and keep outbound default-deny, which many consoles won't let you set manually.</p>"
                        },
                        {
                            "strategy": "Implement microsegmentation (SDN, service mesh, host-based firewalls) for fine-grained internal control.",
                            "howTo": "<h5>Concept:</h5><p>Microsegmentation provides fine-grained, identity-aware traffic control between individual workloads (e.g., pods in Kubernetes). Even if two pods are on the same network segment, they cannot communicate unless an explicit policy allows it. This requires a baseline 'default-deny' policy to be effective.</p><h5>Implement Kubernetes NetworkPolicies (with Default Deny)</h5><p>First, apply a default-deny policy to the namespace. Then, create specific 'allow' policies for required traffic.</p><pre><code># File: k8s/default-deny-policy.yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny-all\n  namespace: ai-production\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  - Egress\n---\n# File: k8s/allow-gateway-to-inference.yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-gateway-to-inference\n  namespace: ai-production\nspec:\n  podSelector:\n    matchLabels:\n      app: model-server\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          app: api-gateway\n    ports:\n    - protocol: TCP\n      port: 8080</code></pre><p><strong>Action:</strong> Deploy a CNI plugin that supports <code>NetworkPolicy</code> enforcement (e.g., Calico, Cilium). Implement a 'default-deny-all' policy for each namespace containing AI workloads. Then, create specific, least-privilege policies to allow only the necessary communication paths between pods. Confirm your CNI (Calico, Cilium, etc.) is actually enforcing NetworkPolicy. Vanilla kube-proxy alone doesn't block traffic; without enforcement this policy is only documentation.</p>"
                        },
                        {
                            "strategy": "Separate development/testing environments from production using distinct accounts or projects.",
                            "howTo": "<h5>Concept:</h5><p>This is a fundamental security control that isolates volatile and less-secure development environments from the stable, hardened production environment using strong administrative boundaries provided by cloud providers.</p><h5>Implement a Multi-Account/Multi-Project Cloud Strategy with SCPs</h5><p>Structure your cloud organization and apply Service Control Policies (SCPs in AWS, Organization Policies in GCP, Azure Policy) to enforce separation.</p><pre><code># Conceptual Cloud Organization Structure (e.g., AWS Organizations)\n\nMy-AI-Organization/ (Root)\nâ””â”€â”€ OU: AI-Workloads\n    â”œâ”€â”€ Account: 111111111111 (AI-Prod)\n    â”œâ”€â”€ Account: 222222222222 (AI-Staging)\n    â””â”€â”€ Account: 333333333333 (AI-Dev/Sandbox)\n\n# --- Example AWS SCP to prevent Prod <-> Dev Peering ---\n# Attach this policy to the AI-Workloads OU or Root\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"DenyProdDevPeering\",\n      \"Effect\": \"Deny\",\n      \"Action\": [\n        \"ec2:AcceptVpcPeeringConnection\",\n        \"ec2:CreateVpcPeeringConnection\"\n      ],\n      \"Resource\": \"*\",\n      \"Condition\": {\n        \"StringEquals\": {\n          \"aws:PrincipalAccount\": [\"111111111111\"],\n          \"ec2:AccepterVpcInfo/OwnerId\": [\"333333333333\"],\n          \"ec2:RequesterVpcInfo/OwnerId\": [\"333333333333\"]\n        }\n      }\n    }\n  ]\n}</code></pre><p><strong>Action:</strong> Structure your cloud environment using separate accounts (AWS) or projects (GCP/Azure) for development, staging, and production. Use organization-level policies (SCPs, Org Policies) to programmatically prevent the creation of network paths (VPC Peering, Direct Connect, VPNs) between production and non-production environments.</p>"
                        },
                        {
                            "strategy": "Regularly review and audit internal network segmentation rules.",
                            "howTo": "<h5>Concept:</h5><p>Internal firewall rules and network policies can become outdated ('rule rot'). Regular, automated audits are necessary to find and remediate overly permissive internal rules, including rules allowing access between security groups.</p><h5>Implement an Automated Security Group Auditor (Improved)</h5><p>Write a script that uses your cloud provider's SDK to scan all security groups for high-risk misconfigurations, including overly broad internal CIDR access and risky SG-to-SG rules.</p><pre><code># File: audits/check_internal_security_groups_v2.py\nimport boto3\nimport json\nfrom ipaddress import ip_network\n\nINTERNAL_RANGES = [ip_network('10.0.0.0/8'), ip_network('172.16.0.0/12'), ip_network('192.168.0.0/16')]\nSENSITIVE_PORTS = [22, 3389, 6379, 27017]\nBROAD_PREFIX_THRESHOLD = 16\n\ndef is_internal(cidr):\n    try:\n        ip = ip_network(cidr)\n        return any(ip.subnet_of(internal_range) for internal_range in INTERNAL_RANGES)\n    except ValueError:\n        return False\n\ndef audit_internal_sg_rules(region):\n    ec2 = boto3.client('ec2', region_name=region)\n    offending_rules = []\n    all_groups = {g['GroupId']: g for g in ec2.describe_security_groups()['SecurityGroups']}\n\n    for group_id, group in all_groups.items():\n        for rule in group.get('IpPermissions', []):\n            from_port = rule.get('FromPort')\n            to_port = rule.get('ToPort')\n            is_sensitive_port_range = False\n            if rule.get('IpProtocol') == '-1':\n                is_sensitive_port_range = True\n            elif from_port is not None and to_port is not None:\n                is_sensitive_port_range = any(from_port <= p <= to_port for p in SENSITIVE_PORTS)\n\n            for ip_range in rule.get('IpRanges', []):\n                cidr = ip_range.get('CidrIp')\n                if cidr and is_internal(cidr) and is_sensitive_port_range:\n                    try:\n                        prefix = ip_network(cidr).prefixlen\n                        if prefix <= BROAD_PREFIX_THRESHOLD:\n                            offending_rules.append({'group_id': group_id, 'reason': f'Broad internal CIDR ({cidr}) allowed to sensitive port.'})\n                            break\n                    except ValueError:\n                        pass\n\n            for sg_source in rule.get('UserIdGroupPairs', []):\n                source_group_id = sg_source.get('GroupId')\n                # Optional: analyze source SG here if needed\n        \n    return offending_rules\n</code></pre><p><strong>Action:</strong> Schedule an automated script to run weekly that audits all internal firewall rules (Security Groups, NSGs, K8s NetworkPolicies). Enhance the script to check IPv4, IPv6, and group-based rules against your organization's internal network standards and sensitive port policies. Also audit SG-to-SG rules (UserIdGroupPairs), not just CIDR ranges. Send violations automatically to SecOps / SRE ticketing or Slack with severity tags, otherwise findings will quietly rot.</p>"
                        }
                    ],
                    "defendsAgainst": [
                        {
                            "framework": "MITRE ATLAS",
                            "items": [
                                "AML.T0044 Full AI Model Access (limits internal access)",
                                "AML.T0036 Data from Information Repositories (limits internal access)"
                            ]
                        },
                        {
                            "framework": "MAESTRO",
                            "items": [
                                "Lateral Movement (Cross-Layer)",
                                "Compromised RAG Pipelines (L2, isolating internal DB access)"
                            ]
                        },
                        {
                            "framework": "OWASP LLM Top 10 2025",
                            "items": [
                                "LLM02:2025 Sensitive Information Disclosure (limits internal data exposure)",
                                "LLM06:2025 Excessive Agency (limits internal reach of compromised agent)"
                            ]
                        },
                        {
                            "framework": "OWASP ML Top 10 2023",
                            "items": [
                                "ML05:2023 Model Theft (limits internal access to model artifacts)",
                                "ML06:2023 ML Supply Chain Attacks (limits blast radius of internally compromised component)"
                            ]
                        }
                    ]
                },
                {
                    "id": "AID-I-002.002",
                    "name": "Secure External AI Service Connectivity",
                    "pillar": ["infra"],
                    "phase": ["operation"],
                    "description": "Applies strict network path control, transport security, policy mediation, and monitoring specifically to connections originating from the AI system and targeting external services, particularly third-party or Model-as-a-Service (MaaS) foundation model APIs. Aims to prevent data exfiltration, Server-Side Request Forgery (SSRF), Man-in-the-Middle (MitM) attacks, and abuse of external dependencies. This also prevents prompt-injected agents from exfiltrating secrets or invoking arbitrary external services; they can only call approved upstreams through a governed path.",
                    "toolsOpenSource": [
                        "Open-source API Gateways (Kong, Tyk, APISIX)",
                        "Open-source Proxies (Squid, Nginx, HAProxy)",
                        "OpenSSL (as a library for verification logic)",
                        "SPIFFE/SPIRE (for workload identity for mTLS)",
                        "Falco, Cilium Tetragon, Sysdig (for egress monitoring)",
                        "Terraform, CloudFormation, Pulumi (for IaC of private endpoints)",
                        "Requests (Python library), cURL (as clients needing security)"
                    ],
                    "toolsCommercial": [
                        "Cloud Provider Private Connectivity (AWS PrivateLink, Azure Private Link, Google Private Service Connect)",
                        "Commercial API Gateway solutions (Apigee, MuleSoft, AWS API Gateway, Azure API Management)",
                        "Cloud-native firewall services (AWS Network Firewall, Azure Firewall Premium, Google Cloud Firewall)",
                        "Certificate Management Platforms (Venafi, DigiCert)",
                        "SIEM/Log Analytics Platforms (Splunk, Datadog, Sentinel, Chronicle)"
                    ],
                    "implementationStrategies": [
                        {
                            "strategy": "Integrate external MaaS/API endpoints via private network connections.",
                            "howTo": "<h5>Concept:</h5><p>Keep traffic to critical external AI services off the public internet. Use cloud provider services like AWS PrivateLink, Azure Private Link, or Google Private Service Connect to create a private, secure endpoint for the MaaS provider within your own VPC. All traffic then flows over the cloud provider's backbone network.</p><h5>Create a Private Endpoint (Example: AWS PrivateLink)</h5><p>Use Infrastructure as Code to provision a VPC Endpoint for the MaaS provider's service, assuming they offer a PrivateLink-compatible service and you have completed the necessary subscription/acceptance flow.</p><pre><code># File: infrastructure/external_connectivity.tf (Terraform)\n\ndata \"aws_vpc\" \"prod_vpc\" {\n  filter { name = \"tag:Name\" values = [\"aidefend-prod-vpc\"] }\n}\n\ndata \"aws_subnets\" \"private_subnets\" {\n  filter { name = \"vpc-id\" values = [data.aws_vpc.prod_vpc.id] }\n  filter { name = \"tag:Tier\" values = [\"Private\"] }\n}\n\n# Find the MaaS provider's VPC Endpoint Service Name (must be obtained from provider)\ndata \"aws_vpc_endpoint_service\" \"maas_service\" {\n  service_name = \"com.amazonaws.vpce.us-east-1.provider-specific-service-name\"\n}\n\n# SG for the Interface Endpoint ENIs: allow ingress 443 from the AI workloads' SG\nresource \"aws_security_group\" \"maas_endpoint_sg\" {\n  name_prefix = \"maas-endpoint-sg-\"\n  vpc_id      = data.aws_vpc.prod_vpc.id\n  ingress {\n    from_port   = 443\n    to_port     = 443\n    protocol    = \"tcp\"\n    # Example: allow from an existing AI workload SG (replace with your SG id)\n    security_groups = [aws_security_group.allow_maas_client_egress_sg.id]\n  }\n  egress = []\n  tags = { Name = \"maas-endpoint-sg\" }\n}\n\n# SG for AI clients that will call the endpoint: restrict egress to 443 only\nresource \"aws_security_group\" \"allow_maas_client_egress_sg\" {\n  name_prefix = \"allow-maas-client-egress-\"\n  vpc_id      = data.aws_vpc.prod_vpc.id\n  egress {\n    from_port   = 443\n    to_port     = 443\n    protocol    = \"tcp\"\n    # Narrow further at firewall/NACL or by resolving the endpoint ENI IPs\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n  tags = { Name = \"allow-maas-client-egress-sg\" }\n}\n\n# Create a VPC Endpoint (Interface) in your private subnets\nresource \"aws_vpc_endpoint\" \"maas_endpoint\" {\n  vpc_id              = data.aws_vpc.prod_vpc.id\n  service_name        = data.aws_vpc_endpoint_service.maas_service.service_name\n  vpc_endpoint_type   = \"Interface\"\n  subnet_ids          = data.aws_subnets.private_subnets.ids\n  security_group_ids  = [aws_security_group.maas_endpoint_sg.id]\n  private_dns_enabled = true\n}\n\n# Ensure client instances/pods use the endpoint's private DNS. Lock down routing and DNS split-horizon as needed.\n</code></pre><p><strong>Action:</strong> Connect to external MaaS providers using private endpoints. Configure endpoint SG to accept only from client SGs and restrict client egress to 443 (further narrowed by firewall/DNS policies). Complete provider-side acceptance as required.</p>"
                        },
                        {
                            "strategy": "Enforce strict egress controls using firewalls and proxies with verified DNS/SNI allow-lists.",
                            "howTo": "<h5>Concept:</h5><p>Implement a default-deny egress policy. Explicitly allow connections only to approved external domains needed by the AI system. Use Layer 7 inspection (TLS SNI, FQDN filtering) for greater precision than IP-based rules.</p><h5>Step 1: Configure Firewall/Proxy Allowlist with TLS Inspection</h5><p>Use a stateful firewall that supports TLS inspection (like Azure Firewall Premium, AWS Network Firewall, or a dedicated proxy like Squid with SSL Bump) to filter outbound HTTPS traffic based on the FQDN.</p><pre><code># Conceptual Azure Firewall Policy Application Rule Collection (requires TLS Inspection enabled)\n{\n  \"ruleCollectionType\": \"ApplicationRuleCollection\",\n  \"name\": \"AllowExternalAIServicesHTTPS\",\n  \"priority\": 200,\n  \"action\": { \"type\": \"Allow\" },\n  \"rules\": [\n    {\n      \"name\": \"AllowOpenAIAPI\",\n      \"protocols\": [ { \"protocolType\": \"Https\", \"port\": 443 } ],\n      \"sourceAddresses\": [ \"10.0.1.0/24\" ],\n      \"targetFqdns\": [ \"api.openai.com\" ]\n    },\n    {\n      \"name\": \"AllowWeatherAPI\",\n      \"protocols\": [ { \"protocolType\": \"Https\", \"port\": 443 } ],\n      \"sourceAddresses\": [ \"10.0.1.0/24\" ],\n      \"targetFqdns\": [ \"api.weather.com\" ]\n    }\n  ]\n}\n# Deny-all outbound TCP/UDP rule collection should exist with lower priority.\n</code></pre><h5>Step 2: Monitor for Violations with Name/IP Fallback</h5><p>Use runtime tools or SIEM to detect connections to destinations not on the allowlist. Include a fallback check by IP when FQDN is unavailable.</p><pre><code># File: falco_rules/ai_egress_violation.yaml\n- list: approved_domains\n  items: [api.openai.com, api.weather.com, internal.registry.corp]\n- list: approved_ips\n  items: [\"203.0.113.10\", \"203.0.113.11\"]\n\n- rule: Prod AI Pod Egress Violation\n  desc: Egress from prod AI workloads to disallowed destinations\n  condition: >\n    evt.type=connect and evt.dir=> and fd.l4proto in (tcp, udp) and\n    (container.image.repository contains \"ai-workload\" or k8s.ns.name in (ai-prod, ai-inference)) and\n    (\n      (fd.sip.name exists and fd.sip.name not in (approved_domains)) or\n      (not fd.sip.name exists and fd.sip not in (approved_ips))\n    )\n  output: >\n    Disallowed egress detected (proc=%proc.name cmd=%proc.cmdline container=%container.name image=%container.image.repository k8s.ns=%k8s.ns.name k8s.pod=%k8s.pod.name dstip=%fd.sip dstdomain=%fd.sip.name)\n  priority: CRITICAL\n  tags: [network, aidefend]\n</code></pre><p><strong>Action:</strong> Enforce default-deny at the perimeter and allow only approved FQDNs/SNIs. Monitor violations using Falco/Tetragon or SIEM ingestion of firewall logs with both name and IP checks. Explicitly deny access to cloud instance metadata services (e.g. 169.254.169.254) unless you're routing through a hardened metadata proxy that strips credentials. This is critical for blocking SSRF-style credential theft.</p>"
                        },
                        {
                            "strategy": "Implement transport layer security (e.g., mTLS, Certificate Pinning) for critical egress connections.",
                            "howTo": "<h5>Concept:</h5><p>Encrypt traffic with TLS and strongly authenticate the external server using Certificate Pinning (SPKI hash) or Mutual TLS (mTLS) if supported. Note: post-handshake SPKI checks have TOCTOU limitations; prefer integrating pinning into the TLS stack or using mTLS.</p><h5>Implement Certificate Pinning Verification (Post-Handshake Check - Caution)</h5><pre><code># File: external_clients/pinned_client_spki_check.py\nimport requests\nimport ssl\nimport socket\nimport hashlib\nimport base64\nfrom cryptography import x509\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.backends import default_backend\n\nEXPECTED_SPKI_HASH_B64 = 'YOUR_EXPECTED_BASE64_SPKI_HASH=='\nEXPECTED_SPKI_HASH_BYTES = base64.b64decode(EXPECTED_SPKI_HASH_B64)\nTARGET_HOST = 'maas.example.com'\nTARGET_PORT = 443\n\ndef get_cert_spki_hash(hostname, port):\n    context = ssl.create_default_context()\n    conn_sock = context.wrap_socket(socket.create_connection((hostname, port)), server_hostname=hostname)\n    cert_der = conn_sock.getpeercert(binary_form=True)\n    conn_sock.close()\n    cert = x509.load_der_x509_certificate(cert_der, default_backend())\n    spki_der = cert.public_key().public_bytes(\n        encoding=serialization.Encoding.DER,\n        format=serialization.PublicFormat.SubjectPublicKeyInfo\n    )\n    return hashlib.sha256(spki_der).digest()\n\nactual = get_cert_spki_hash(TARGET_HOST, TARGET_PORT)\nif actual == EXPECTED_SPKI_HASH_BYTES:\n    print(\"âœ… Certificate SPKI pin verified successfully (post-handshake).\")\nelse:\n    raise ConnectionRefusedError(\"Certificate pin mismatch\")\n</code></pre><p><strong>Action:</strong> Use SPKI pinning for high-sensitivity APIs when feasible, understanding the operational costs. Prefer mTLS with short-lived client certs where both parties support it. Pinned keys/certs must be rotated in a controlled CI/CD process; hardcoding pins without rotation planning will cause self-inflicted outages.</p>"
                        },
                        {
                            "strategy": "Utilize API Gateways to mediate external AI service traffic and apply security policies.",
                            "howTo": "<h5>Concept:</h5><p>An API Gateway provides a centralized control point for outbound traffic toward external AI services. It can enforce authentication, rate limits, schema validation, and resilience patterns. Configure health checks on the upstream entity.</p><h5>Configure Kong Gateway for External Upstream (Revised)</h5><pre><code># File: kong_config_external.yaml (Kong declarative configuration)\n_format_version: \"3.0\"\n\nupstreams:\n- name: maas-upstream\n  targets:\n  - target: api.maas-provider.com:443\n  healthchecks:\n    active:\n      https_verify_certificate: true\n      healthy:\n        http_statuses: [200, 201]\n        successes: 2\n      unhealthy:\n        http_statuses: [429, 500, 503]\n        timeouts: 3\n        http_failures: 3\n      interval: 10\n      timeout: 2\n\nservices:\n- name: external-maas-service\n  host: maas-upstream\n  port: 443\n  protocol: https\n  plugins:\n  - name: request-transformer\n    config:\n      add:\n        headers:\n        - \"Authorization: Bearer ${KONG_MAAS_API_KEY}\"\n  - name: rate-limiting\n    config:\n      minute: 500\n      policy: local\n\nroutes:\n- name: maas-route\n  paths:\n  - /internal/proxy/maas\n  strip_path: true\n  service: { name: external-maas-service }\n</code></pre><p><strong>Action:</strong> Route outbound MaaS traffic through the gateway. Inject secrets via environment variables or a supported secret store. Use upstream health checks for resilience. The gateway must also sanitize/override outbound headers so the agent cannot smuggle alternative destinations or custom Authorization headers to bypass policy.</p>"
                        },
                        {
                            "strategy": "Monitor external egress traffic for anomalies and policy violations.",
                            "howTo": "<h5>Concept:</h5><p>Continuously monitor volume, destinations, timing, and TLS metadata of outbound connections. Deviations can indicate exfiltration, C2, or policy bypass.</p><h5>Create a SIEM Alert for Anomalous Egress Volume (Boundary-Focused)</h5><pre><code># Conceptual Splunk SPL (AWS VPC Flow Logs with boundary focus)\nindex=vpcflowlogs sourcetype=aws:vpcflowlogs direction=egress \n(srcaddr=10.0.1.0/24 OR srcaddr=10.0.2.0/24) AND (interface_id=nat-* OR interface_id=igw-*)\nNOT (dstaddr=10.0.0.0/8 OR dstaddr=172.16.0.0/12 OR dstaddr=192.168.0.0/16)\n| timechart span=1h sum(bytes) as bytes_out by srcaddr\n| streamstats window=24 global=f avg(bytes_out) as avg_bytes_out stddev(bytes_out) as stddev_bytes_out by srcaddr\n| eval threshold_upper = avg_bytes_out + (3 * stddev_bytes_out)\n| where bytes_out > threshold_upper AND avg_bytes_out > 1000000 \n| table _time, srcaddr, bytes_out, avg_bytes_out, stddev_bytes_out, threshold_upper\n</code></pre><p><strong>Action:</strong> Ingest boundary egress logs and baseline normal volumes and destinations. Alert on significant deviations and newly observed ASNs/domains for AI subnets.</p>"
                        }
                    ],
                    "defendsAgainst": [
                        {
                            "framework": "MITRE ATLAS",
                            "items": [
                                "AML.T0025 Exfiltration via Cyber Means",
                                "AML.T0049 Exploit Public-Facing Application (SSRF)",
                                "AML.T0072 Reverse Shell",
                                "AML.T0034 Cost Harvesting"
                            ]
                        },
                        {
                            "framework": "MAESTRO",
                            "items": [
                                "Data Exfiltration (Cross-Layer)",
                                "Orchestration Attacks (L4)"
                            ]
                        },
                        {
                            "framework": "OWASP LLM Top 10 2025",
                            "items": [
                                "LLM02:2025 Sensitive Information Disclosure (via exfiltration)",
                                "LLM03:2025 Supply Chain (securing external connections)"
                            ]
                        },
                        {
                            "framework": "OWASP ML Top 10 2023",
                            "items": [
                                "ML05:2023 Model Theft (securing API access)",
                                "ML06:2023 ML Supply Chain Attacks (securing external component access)"
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "id": "AID-I-003",
            "name": "Quarantine & Throttling of AI Interactions", "pillar": ["infra", "app"], "phase": ["response"],
            "description": "Implement mechanisms to automatically or manually isolate, rate-limit, or place into a restricted \\\"safe mode\\\" specific AI system interactions when suspicious activity is detected. This could apply to individual user sessions, API keys, IP addresses, or even entire AI agent instances. The objective is to prevent potential attacks from fully executing, spreading, or causing significant harm by quickly containing or degrading the capabilities of the suspicious entity. This is an active response measure triggered by detection systems. This can be applied pre-emptively (automatic) or under human approval (SOAR analyst click-to-quarantine) depending on confidence score, and all actions must be logged/auditable for compliance and forensic review.",
            "toolsOpenSource": [
                "Fail2Ban (adapted for AI logs)",
                "Custom scripts (Lambda, Azure Functions, Cloud Functions) for automated actions",
                "API Gateways (Kong, Tyk, Nginx) for rate limiting",
                "Kubernetes for resource quotas/isolation"
            ],
            "toolsCommercial": [
                "API Security and Bot Management solutions (Cloudflare, Akamai, Imperva)",
                "ThreatWarrior (automated detection/response)",
                "SIEM/SOAR platforms (Splunk SOAR, Palo Alto XSOAR, IBM QRadar SOAR)",
                "WAFs with advanced rate limiting"
            ],
            "defendsAgainst": [
                {
                    "framework": "MITRE ATLAS",
                    "items": [
                        "AML.T0024.002 Invert AI Model (rate-limiting)",
                        "AML.T0029 Denial of AI Service (throttling)",
                        "AML.T0034 Cost Harvesting (limiting rates)",
                        "AML.T0040 AI Model Inference API Access",
                        "AML.T0046 Spamming AI System with Chaff Data"
                    ]
                },
                {
                    "framework": "MAESTRO",
                    "items": [
                        "Model Stealing (L1, throttling)",
                        "DoS on Framework APIs / Data Infrastructure (L3/L2)",
                        "Resource Hijacking (L4, containing processes)",
                        "Agent Pricing Model Manipulation (L7, rate limiting)",
                        "Model Extraction of AI Security Agents (L6)"
                    ]
                },
                {
                    "framework": "OWASP LLM Top 10 2025",
                    "items": [
                        "LLM10:2025 Unbounded Consumption (throttling/quarantining)",
                        "LLM01:2025 Prompt Injection (containment of repeated prompt-injection-driven abuse after detection)"
                    ]
                },
                {
                    "framework": "OWASP ML Top 10 2023",
                    "items": [
                        "ML05:2023 Model Theft (throttling excessive queries)"
                    ]
                }
            ],
            "implementationStrategies": [
                {
                    "strategy": "Automated quarantine based on high-risk behavior alerts (cut access, move to honeypot, disable key/account).",
                    "howTo": "<h5>Concept:</h5><p>When your SIEM or detection pipeline fires a high-confidence alert for a specific entity (API key, user ID, IP address, or agent instance), an automated workflow should immediately take containment actions: block the source at the edge (WAF / firewall), suspend its API credential, optionally redirect it to a deception/honeypot environment, and open an investigation ticket. This prevents the attacker from continuing while you investigate.</p><h5>Step 1: Make Alerts Actionable</h5><p>Your detection system (model extraction detector, anomaly engine, egress monitor, etc.) must emit structured alerts to something invokable, e.g. an SQS queue, webhook, or SOAR trigger. The alert should include: offending principal (IP / user / key), reason (exfil, abuse, cost-harvest), and confidence score.</p><h5>Step 2: Create a Serverless Quarantine Function</h5><p>A serverless function (e.g. AWS Lambda) consumes those alerts and updates your edge security controls in real time. Example: add the IP to an AWS WAF IP blocklist. The same flow could also revoke the user's API key or mark their session as suspended in your IdP. NOTE: Quarantine actions must be logged (timestamp, triggering alert, automation vs human approval) for audit and forensic review.</p><pre><code># File: quarantine_lambda/main.py\nimport boto3\nimport json\n\n# NOTE: Pseudocode. You must adapt IPSet IDs, scopes, and quotas.\n\ndef lambda_handler(event, context):\n    \"\"\"Triggered by SQS messages from SIEM/monitoring alerts.\"\"\"\n    waf_client = boto3.client('wafv2')\n\n    for record in event['Records']:\n        alert = json.loads(record['body'])\n        action = alert.get('action')  # e.g. \"QUARANTINE_IP\", \"DISABLE_API_KEY\"\n        offender_ip = alert.get('source_ip')\n        offender_key = alert.get('api_key_id')\n        reason = alert.get('reason')  # e.g. \"model_extraction\", \"cost_abuse\"\n        confidence = alert.get('confidence')\n\n        # Basic policy: only auto-block if high confidence\n        if action == 'QUARANTINE_IP' and offender_ip and confidence >= 0.9:\n            print(f\"[AUTO-QUARANTINE] Blocking IP {offender_ip} for reason={reason}\")\n            try:\n                # Fetch current IPSet to get LockToken\n                ipset = waf_client.get_ip_set(\n                    Name='aidefend-ip-blocklist',\n                    Scope='REGIONAL',  # or CLOUDFRONT depending on placement\n                    Id='REPLACE_ME_IPSET_ID'\n                )\n\n                lock_token = ipset['LockToken']\n                current_addrs = ipset['IPSet']['Addresses']\n\n                # Prepend new IP, maintaining /32; consider aging out stale IPs to avoid quota exhaustion\n                updated_addrs = [f\"{offender_ip}/32\"] + current_addrs\n\n                waf_client.update_ip_set(\n                    Name='aidefend-ip-blocklist',\n                    Scope='REGIONAL',\n                    Id='REPLACE_ME_IPSET_ID',\n                    LockToken=lock_token,\n                    Addresses=updated_addrs\n                )\n\n                # Emit an auditable security event (SIEM / log)\n                print(json.dumps({\n                    'event': 'QUARANTINE_IP',\n                    'ip': offender_ip,\n                    'reason': reason,\n                    'confidence': confidence,\n                    'ts': context.aws_request_id\n                }))\n            except Exception as e:\n                print(f\"[ERROR] Failed to block IP {offender_ip}: {e}\")\n\n        if action == 'DISABLE_API_KEY' and offender_key:\n            # Call your key-management / API gateway admin interface to revoke that key.\n            # Also log this action for audit.\n            print(f\"[AUTO-QUARANTINE] Disabling API key {offender_key} (reason={reason})\")\n\n    return {'statusCode': 200}\n</code></pre><p><strong>Action:</strong> Build an automated quarantine function that can (a) update WAF/IP blocklists, (b) revoke or suspend a specific API key or OAuth client ID, and (c) optionally redirect that principal to a deception/honeypot environment. Every quarantine action (who/what got blocked, why, confidence level) MUST be logged to SIEM / ticketing so Incident Response and compliance can prove proper handling.</p>"
                },
                {
                    "strategy": "Dynamic rate limiting for anomalous behavior (query spikes, complex queries).",
                    "howTo": "<h5>Concept:</h5><p>Instead of a single global rate limit, apply adaptive throttling per user / tenant / agent ID based on actual resource stress. For example, model extraction, cost harvesting, or data exfil attempts often involve bursts of long, high-cost prompts. By tracking a per-identity 'complexity score' over a sliding time window and cutting them off once they exceed a threshold, you contain abuse without harming normal users.</p><h5>Implementation Pattern (Redis Sliding Window):</h5><p>We keep (timestamp, complexity_score) entries in Redis ZSET keyed per user. We expire old entries (older than TIME_WINDOW_SECONDS), sum the recent complexity scores, and if the threshold would be exceeded we reject. NOTE: The ZSET <em>score</em> is the timestamp. The <em>member</em> encodes the complexity value. This avoids the bug where you accidentally sum timestamps instead of complexity.</p><pre><code># File: api/dynamic_rate_limiter.py\nimport time\nimport redis\n\nr = redis.Redis()\n\nCOMPLEXITY_THRESHOLD = 500    # max total 'cost' allowed in the window\nTIME_WINDOW_SECONDS = 60      # sliding window size\n\ndef check_dynamic_limit(user_id: str, prompt: str) -> bool:\n    \"\"\"Return False if this request should be throttled for this user.\"\"\"\n    now = time.time()\n    complexity_score = len(prompt)  # simplistic cost metric; replace with real cost model\n\n    key = f\"user_complexity:{user_id}\"\n\n    # 1. Drop old events outside the window\n    r.zremrangebyscore(key, 0, now - TIME_WINDOW_SECONDS)\n\n    # 2. Get recent events (member encodes complexity, score is timestamp)\n    entries = r.zrange(key, 0, -1, withscores=True)\n\n    current_total_complexity = 0.0\n    for member, ts in entries:\n        # member looks like \"<complexity>:<timestamp>\"\n        try:\n            member_decoded = member.decode('utf-8') if isinstance(member, bytes) else str(member)\n            complexity_str = member_decoded.split(\":\")[0]\n            current_total_complexity += float(complexity_str)\n        except Exception:\n            # ignore malformed entries\n            pass\n\n    # 3. Will adding this prompt blow past threshold?\n    if current_total_complexity + complexity_score > COMPLEXITY_THRESHOLD:\n        # Emit security log / SIEM event for potential extraction or abuse\n        print({\n            'event': 'RATE_LIMIT_TRIP',\n            'user_id': user_id,\n            'total_complexity': current_total_complexity,\n            'threshold': COMPLEXITY_THRESHOLD,\n            'ts': now\n        })\n        return False  # throttle (HTTP 429)\n\n    # 4. Otherwise record this event and allow\n    member_value = f\"{complexity_score}:{now}\"\n    r.zadd(key, {member_value: now})\n    return True\n</code></pre><p><strong>Action:</strong> Put this check in API middleware in front of inference/tool calls. If <code>check_dynamic_limit()</code> returns <code>False</code>, immediately respond with HTTP 429 and mark that identity as 'abuse-suspect' in logs/SIEM for possible quarantine. Log these trips; they are security-relevant signals that can feed automated quarantine (Strategy 1) or human review.</p>"
                },
                {
                    "strategy": "Stricter rate limits for unauthenticated/less trusted users.",
                    "howTo": "<h5>Concept:</h5><p>Anonymous or low-trust tenants are the riskiest for brute-force model extraction, DoS, cost harvesting, and prompt-injection-driven tool abuse. Give them a very conservative rate limit and smaller concurrency budget. Give higher limits only to identities you've verified (paid customers, known workforce identities, allowlisted service accounts). Every elevation in trust tier is itself a governed, auditable security event.</p><h5>Tiered Rate Limiting via API Gateway (Example: Kong)</h5><p>Define multiple rate-limiting plugin instances (e.g. <code>rate-limit-free</code> vs <code>rate-limit-premium</code>). Attach the strict default limit globally, then explicitly attach the more generous limit to specific trusted consumer groups. Log any movement of a user into a higher-trust tier as a privileged action.</p><pre><code># File: kong_config.yaml (declarative example)\n\nplugins:\n- name: rate-limiting\n  instance_name: rate-limit-premium\n  config:\n    minute: 1000        # Premium / trusted users\n    policy: local\n- name: rate-limiting\n  instance_name: rate-limit-free\n  config:\n    minute: 20          # Anonymous / low-trust users\n    policy: local\n\nconsumers:\n- username: premium_user_group\n  plugins:\n  - name: rate-limiting\n    instance_name: rate-limit-premium   # Attach high quota to trusted group\n\nservices:\n- name: my-ai-service\n  url: http://inference-server:8080\n  plugins:\n  - name: rate-limiting\n    instance_name: rate-limit-free      # Default (strict) limit applies to everyone else\n</code></pre><p><strong>Action:</strong> Enforce extremely low default limits on anonymous traffic. Grant higher limits only to vetted groups, and log that elevation (who approved, when, and for which account) because that trust-tier change is a new attack surface. This helps contain scraping, model theft (ML05:2023), and economic abuse, while preserving capacity for real customers.</p>"
                },
                {
                    "strategy": "Design AI systems with a 'safe mode' or degraded functionality state.",
                    "howTo": "<h5>Concept:</h5><p>During an active attack or anomaly (cost spike, mass prompt injection attempts, suspicious tool usage), the AI service should be able to enter a 'safe mode'. In safe mode, high-risk capabilities are disabled: agent tool execution, filesystem writes, network egress, or high-cost model calls. Requests are optionally routed to a cheaper / more controllable fallback model. Safe mode is essentially emergency containment with graceful degradation, not a full shutdown.</p><h5>Feature-Flag Controlled Safe Mode</h5><p>Use a runtime flag (feature flag service / config toggle) so SecOps/SRE can flip the system into safe mode instantly without redeploying. Entering or exiting safe mode MUST emit an auditable event to SIEM/SOAR, because it's effectively an incident response state change.</p><pre><code># File: api/inference_logic.py\nimport feature_flags  # your feature flag SDK / config service\n\nstate = {\"mode\": \"normal\"}\n\n# primary_llm = load_primary_model()\n# safe_llm    = load_safe_fallback_model()\n\ndef generate_response(prompt: str):\n    # Check central kill-switch / safe-mode flag at request time\n    is_safe_mode = feature_flags.get_flag('ai-safe-mode', default=False)\n\n    if is_safe_mode:\n        state[\"mode\"] = \"safe_mode\"\n        # In safe mode:\n        #  - Call only the safe fallback model\n        #  - Disable high-risk agent tools / side effects\n        #  - Enforce read-only or dry-run behavior\n        print(\"[SAFE MODE] Routing to fallback model; tools disabled.\")\n        # return safe_llm.generate(prompt)\n    else:\n        state[\"mode\"] = \"normal\"\n        print(\"[NORMAL MODE] Full features enabled.\")\n        # return primary_llm.generate_with_tools(prompt)\n</code></pre><p><strong>Action:</strong> Implement a feature-flag-driven safe mode that (1) downgrades the agent/model to a restricted profile, (2) disables tool invocation and write-side effects, and (3) reduces expensive calls. Every toggle of safe mode (who triggered it, why, timestamp) should be logged to SIEM/SOAR for audit and incident timeline reconstruction. Treat safe mode entry as a partial containment action, not just an ops toggle.</p>"
                },
                {
                    "strategy": "Utilize SOAR platforms to automate quarantine/throttling actions.",
                    "howTo": "<h5>Concept:</h5><p>A SOAR (Security Orchestration, Automation, and Response) platform is your incident-response brain. It ingests high-confidence alerts (model extraction attempt, abnormal egress, mass prompt injection) and executes a playbook: block IPs, suspend sessions, lower rate limits, move a tenant into 'safe mode', and open an investigation ticket. For high-value users or production tenants, require a human approval step before final lockout to avoid accidental mass-customer impact.</p><h5>Automated Response Playbook (Conceptual YAML)</h5><pre><code>name: \"Automated AI User Quarantine Playbook\"\ntrigger:\n  siem_alert_name: \"AI_Model_Extraction_Attempt_Detected\"\n\nsteps:\n- name: Enrich Data\n  actions:\n  - command: get_ip_from_alert\n    output: ip_address\n  - command: get_user_id_from_alert\n    output: user_id\n\n- name: Get User Reputation\n  actions:\n  - service: trust_score_api\n    command: get_score\n    inputs: { \"agent_id\": \"{{user_id}}\" }\n    output: user_trust_score\n\n- name: Conditional Quarantine\n  condition: \"{{user_trust_score}} < 0.3\"\n  actions:\n  - service: aws_waf\n    command: block_ip\n    inputs: { \"ip\": \"{{ip_address}}\" }\n  - service: okta\n    command: suspend_user_session\n    inputs: { \"user_id\": \"{{user_id}}\" }\n  - service: jira\n    command: create_ticket\n    inputs:\n      project: \"SOC\"\n      title: \"User {{user_id}} quarantined for suspected AI abuse\"\n      assignee: \"security_on_call\"\n</code></pre><p><strong>Action:</strong> Integrate AI abuse detection signals into SOAR. The playbook should (a) enrich context (who is this?), (b) score trust/risk, (c) automatically quarantine low-trust entities, and (d) open a ticket with full telemetry. For privileged / high-value tenants, require an analyst approval gate in the SOAR workflow before suspension. All actions, including who approved, become part of the audit trail.</p>"
                },
                {
                    "strategy": "Hallucination Circuit Breaker & Degrade Modes.",
                    "howTo": "<h5>Concept:</h5><p>Continuously compute safety/quality metrics for each AI agent or session, such as <code>consensus_score</code> (agreement across validators), <code>evidence_coverage</code> (ratio of claims backed by retrieved sources), <code>contradiction_rate</code> (self-inconsistency), <code>schema_error_rate</code> (output violates required schema), and <code>unknown_tool_calls</code> (attempts to invoke tools outside policy). If those metrics cross thresholds, immediately trip a circuit breaker. The breaker forces the agent/session into a containment posture: read-only, safe-tools-only, human-in-the-loop required for writes, or full quarantine. This limits downstream blast radius (data corruption, unwanted tool calls, money spend) from a compromised or drifting agent.</p><h5>Express / Node.js Middleware Example</h5><pre><code>// File: breaker.js\nexport function breaker(req, res, next) {\n  const m = req.app.locals.metrics;\n  const tripped = (\n    (m.consensus < 0.6) ||\n    (m.evidence_coverage < 0.5) ||\n    (m.contradiction_rate > 0.2) ||\n    (m.schema_error_rate > 0.1) ||\n    (m.unknown_tool_calls > 0)\n  );\n\n  if (tripped) {\n    req.app.locals.mode = \"safe_tools_only\"; // degraded mode\n    // Emit a structured security event for SIEM/SOAR\n    console.log(JSON.stringify({\n      event: \"BREAKER_TRIPPED\",\n      mode: req.app.locals.mode,\n      ts: Date.now()\n    }));\n  } else {\n    req.app.locals.mode = \"normal\";\n  }\n\n  res.set(\"X-Agent-Mode\", req.app.locals.mode);\n  next();\n}\n</code></pre><h5>FastAPI Middleware + Write Gate Example</h5><pre><code># File: app.py\nfrom fastapi import FastAPI, Request\n\napp = FastAPI()\napp.state.metrics = {\n    'consensus': 1.0,\n    'evidence_coverage': 1.0,\n    'contradiction_rate': 0.0\n}\nstate = {\"mode\": \"normal\"}\n\n@app.middleware(\"http\")\nasync def breaker_mw(req: Request, call_next):\n    m = req.app.state.metrics\n    tripped = (\n        (m['consensus'] < 0.6) or\n        (m['evidence_coverage'] < 0.5) or\n        (m['contradiction_rate'] > 0.2)\n    )\n    if tripped:\n        state[\"mode\"] = \"read_only\"\n        # Emit security/IR signal: system entered containment mode\n        print({\n            'event': 'BREAKER_TRIPPED',\n            'new_mode': state[\"mode\"],\n            'ts': 'now()'\n        })\n    else:\n        state[\"mode\"] = \"normal\"\n\n    resp = await call_next(req)\n    resp.headers[\"X-Agent-Mode\"] = state[\"mode\"]\n    return resp\n\n@app.post(\"/kb/write\")\nasync def write_kb(item: dict):\n    # Block destructive side effects if we're in degraded/containment mode\n    if state[\"mode\"] in (\"read_only\", \"safe_tools_only\"):\n        return {\"status\": \"quarantined\"}\n    # Normal write path would go here\n    return {\"status\": \"ok\"}\n</code></pre><p><strong>Action:</strong> Run breaker logic at the orchestration / API boundary (gateway, middleware, controller), not inside the agent's own prompt, so a compromised agent cannot silently bypass it. When the breaker trips, immediately (1) downgrade capabilities (read-only, safe-tools-only, HITL-required), (2) stop dangerous tool use and KB writes, and (3) emit a SIEM/SOAR event so SecOps/SRE know the system is now in containment mode. Treat this as part of incident response.</p>"
                },
                {
                    "strategy": "Throttle GPU/CPU for a suspicious tenant namespace in Kubernetes.",
                    "howTo": "<h5>Concept:</h5><p>Sometimes you don't want to fully block or ban a tenant/agent yet (false positives are expensive), but you also can't let them keep burning GPU, hammering vector search, or spawning aggressive jobs. A fast, surgical containment move is to dynamically clamp that tenant's compute budget. You can (a) apply a tighter <code>ResourceQuota</code> / <code>LimitRange</code> to their namespace or (b) evict / reschedule them onto low-priority nodes with throttled GPU/CPU. This acts like 'gardening the blast radius': you buy time for human review while capping cost, data access, and lateral movement potential.</p><h5>Example: Apply/Update a ResourceQuota on a Suspicious Namespace</h5><p>In Kubernetes, each tenant/agent (or each AI tool-execution sandbox) can run in its own namespace. When anomalous behavior is detected (cost spike, extraction attempt, mining behavior, runaway agent loops), your response automation can patch that namespace with a stricter <code>ResourceQuota</code> and <code>LimitRange</code>, effectively throttling CPU/GPU/memory. This is a live containment control that doesn't require deleting the workload immediately.</p><pre><code># File: k8s/quarantine-quota.yaml\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: quarantine-quota\n  namespace: suspicious-tenant-ns\nspec:\n  hard:\n    requests.cpu: \"2\"           # cap total requested CPU cores\n    limits.cpu: \"4\"             # cap total CPU limit\n    requests.memory: \"8Gi\"      # cap total requested memory\n    limits.memory: \"16Gi\"       # cap total memory\n    requests.nvidia.com/gpu: \"0\"  # deny new GPU requests (freeze expensive inference)\n    limits.nvidia.com/gpu: \"0\"\n---\napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: quarantine-limits\n  namespace: suspicious-tenant-ns\nspec:\n  limits:\n  - type: Container\n    max:\n      cpu: \"2\"\n      memory: \"8Gi\"\n    default:\n      cpu: \"1\"\n      memory: \"4Gi\"\n    defaultRequest:\n      cpu: \"500m\"\n      memory: \"2Gi\"\n</code></pre><p><strong>Action:</strong> When a tenant/agent is flagged as suspicious (excessive tool calls, GPU burn, reverse shell attempt, cost harvesting), have your SOAR / Lambda-style responder automatically <code>kubectl apply</code> (or patch via API) a restrictive <code>ResourceQuota</code> and <code>LimitRange</code> onto that tenant's namespace. This enforces (1) zero new GPU allocation, (2) heavily throttled CPU/memory, and (3) containment to low-cost resources while still preserving logs/forensics. Emit a SIEM/SOAR audit event every time you clamp or later restore those quotas, and require human approval to fully delete workloads in regulated environments.</p>"
                }
            ]
        },
        {
            "id": "AID-I-004",
            "name": "Agent Memory & State Isolation",
            "description": "Manage the lifecycle, integrity, and isolation of agent memory in agentic AI systems. Agent memory (runtime context, tool traces, and persistent vector/RAG stores) is uniquely susceptible to (1) prompt injection persistence, (2) memory/KB poisoning, and (3) cross-session or cross-tenant contamination. This technique family enforces isolation across four layers: Runtime Hygiene (App), Persistent Partitioning (Data), Cryptographic Integrity (Security), and Transactional Promotion Gates (Ops/Governance).",
            "defendsAgainst": [
                {
                    "framework": "MITRE ATLAS",
                    "items": [
                        "AML.T0051 LLM Prompt Injection",
                        "AML.T0061 LLM Prompt Self-Replication",
                        "AML.T0070 RAG Poisoning",
                        "AML.T0080.000 AI Agent Context Poisoning: Memory",
                        "AML.T0018.001 Manipulate AI Model: Poison LLM Memory"
                    ]
                },
                {
                    "framework": "MAESTRO",
                    "items": [
                        "Agent Goal Manipulation (L7) (via persistent instruction/memory poisoning)",
                        "Agent Tool Misuse (L7) (via poisoned recalled context)",
                        "Data Poisoning (L2) (when memory/KB is treated as data)",
                        "Compromised RAG Pipelines (L2)"
                    ]
                },
                {
                    "framework": "OWASP LLM Top 10 2025",
                    "items": [
                        "LLM01:2025 Prompt Injection",
                        "LLM02:2025 Sensitive Information Disclosure",
                        "LLM04:2025 Data and Model Poisoning",
                        "LLM08:2025 Vector and Embedding Weaknesses",
                        "LLM10:2025 Unbounded Consumption"
                    ]
                },
                {
                    "framework": "OWASP ML Top 10 2023",
                    "items": [
                        "ML02:2023 Data Poisoning Attack",
                        "ML06:2023 AI Supply Chain Attacks",
                        "ML09:2023 Output Integrity Attack"
                    ]
                }
            ],
            "subTechniques": [
                {
                    "id": "AID-I-004.001",
                    "name": "Runtime Context Isolation & Hygiene",
                    "pillar": ["app"],
                    "phase": ["operation"],
                    "description": "Enforces strict boundaries and hygiene for volatile working memory (RAM/Redis). Prevents cross-session/cross-tenant context bleed, limits the temporal blast radius of prompt injections (via windowing/resets), and prevents resource exhaustion (DoS/cost burn) via size/token ceilings and TTL.",
                    "toolsOpenSource": [
                        "Redis (key TTL, eviction policies)",
                        "Memcached",
                        "OpenTelemetry (distributed tracing for memory events)",
                        "LangChain (memory modules) / Semantic Kernel (memory abstractions)"
                    ],
                    "toolsCommercial": [
                        "Redis Enterprise",
                        "Momento (serverless cache)"
                    ],
                    "defendsAgainst": [
                        {
                            "framework": "MITRE ATLAS",
                            "items": [
                                "AML.T0051 LLM Prompt Injection",
                                "AML.T0061 LLM Prompt Self-Replication"
                            ]
                        },
                        {
                            "framework": "OWASP LLM Top 10 2025",
                            "items": [
                                "LLM01:2025 Prompt Injection",
                                "LLM02:2025 Sensitive Information Disclosure",
                                "LLM10:2025 Unbounded Consumption"
                            ]
                        }
                    ],
                    "implementationStrategies": [
                        {
                            "strategy": "Enforce per-session isolation with mandatory TTL, deterministic size/token limits, and fail-closed serialization to prevent cross-tenant bleed and DoS.",
                            "howTo": "<h5>Concept:</h5><p>Never store agent runtime context in a global in-process object. Use a shared store (e.g., Redis) keyed by <code>tenant_id + session_id</code> and enforce <strong>TTL</strong>, <strong>max bytes</strong>, and (optionally) <strong>max tokens</strong>. All reads/writes must be schema-validated and serialized deterministically. On violation, fail closed and emit an audit event.</p><h5>Example: Redis-backed Secure Runtime Context Store</h5><pre><code># File: memory/runtime_context_store.py\nimport json\nimport time\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Optional\n\nMAX_CONTEXT_BYTES = 256 * 1024   # 256KB hard ceiling (tune per model/context budget)\nSESSION_TTL_SECONDS = 3600       # 1 hour idle TTL\nALLOWED_ROLES = {\"user\", \"assistant\", \"tool\"}  # do NOT allow persisted 'system' from untrusted paths\n\n@dataclass(frozen=True)\nclass AuditEvent:\n    event: str\n    tenant_id: str\n    session_id: str\n    details: Dict[str, Any]\n    ts: int\n\n\ndef emit_audit(evt: AuditEvent) -&gt; None:\n    # Production: ship to SIEM via OTel logs, Kafka, or your logging pipeline\n    print(json.dumps(evt.__dict__, sort_keys=True))\n\n\ndef _key(tenant_id: str, session_id: str) -&gt; str:\n    return f\"tenant:{tenant_id}:session:{session_id}:runtime_context\"\n\n\ndef _serialize_context(messages: List[Dict[str, str]]) -&gt; bytes:\n    # Deterministic JSON prevents ambiguous size checks and simplifies forensics\n    return json.dumps(messages, sort_keys=True, separators=(\",\", \":\")).encode(\"utf-8\")\n\n\ndef _validate_messages(messages: Any) -&gt; List[Dict[str, str]]:\n    if not isinstance(messages, list):\n        raise ValueError(\"context must be a list\")\n\n    out: List[Dict[str, str]] = []\n    for m in messages:\n        if not isinstance(m, dict):\n            raise ValueError(\"each message must be an object\")\n        role = m.get(\"role\")\n        content = m.get(\"content\")\n        if role not in ALLOWED_ROLES:\n            raise ValueError(f\"role not allowed: {role}\")\n        if not isinstance(content, str):\n            raise ValueError(\"content must be a string\")\n        out.append({\"role\": role, \"content\": content})\n    return out\n\n\nclass RuntimeContextStore:\n    def __init__(self, redis_client):\n        self.redis = redis_client\n\n    def load(self, *, tenant_id: str, session_id: str) -&gt; List[Dict[str, str]]:\n        raw = self.redis.get(_key(tenant_id, session_id))\n        if raw is None:\n            return []\n\n        try:\n            obj = json.loads(raw)\n            messages = _validate_messages(obj)\n            return messages\n        except Exception as e:\n            emit_audit(AuditEvent(\n                event=\"RUNTIME_CONTEXT_LOAD_REJECTED\",\n                tenant_id=tenant_id,\n                session_id=session_id,\n                details={\"reason\": str(e)},\n                ts=int(time.time())\n            ))\n            # Fail closed: treat corrupted/tampered context as empty\n            return []\n\n    def store(self, *, tenant_id: str, session_id: str, messages: List[Dict[str, str]]) -&gt; None:\n        validated = _validate_messages(messages)\n        blob = _serialize_context(validated)\n\n        if len(blob) &gt; MAX_CONTEXT_BYTES:\n            emit_audit(AuditEvent(\n                event=\"RUNTIME_CONTEXT_WRITE_BLOCKED_OVERSIZE\",\n                tenant_id=tenant_id,\n                session_id=session_id,\n                details={\"bytes\": len(blob), \"max\": MAX_CONTEXT_BYTES},\n                ts=int(time.time())\n            ))\n            raise ValueError(\"context exceeds size ceiling\")\n\n        self.redis.set(_key(tenant_id, session_id), blob, ex=SESSION_TTL_SECONDS)\n        emit_audit(AuditEvent(\n            event=\"RUNTIME_CONTEXT_WRITE_OK\",\n            tenant_id=tenant_id,\n            session_id=session_id,\n            details={\"bytes\": len(blob), \"ttl\": SESSION_TTL_SECONDS},\n            ts=int(time.time())\n        ))\n</code></pre><p><strong>Action:</strong> Enforce (1) per-tenant/per-session keys, (2) deterministic serialization, (3) strict role allowlist (block persisted fake <code>system</code>), (4) TTL, and (5) hard ceilings. Emit structured audit events for both blocked writes and rejected loads.</p>"
                        },
                        {
                            "strategy": "Use sliding windows and controlled volatile resets for long-running agents; reseed only from a trusted baseline goal/config (fail-closed).",
                            "howTo": "<h5>Concept:</h5><p>Long-running agents accumulate state and can develop persistent poisoned instructions. Apply a sliding window for normal sessions, and enforce a hard reset for high-risk agents. The reset must reseed only from a trusted baseline that the agent cannot mutate.</p><h5>Example: Windowed Memory + Controlled Reset Hook</h5><pre><code># File: memory/resettable_memory.py\nfrom langchain.memory import ConversationBufferWindowMemory\n\nclass ResettableMemory:\n    def __init__(self, *, baseline_statement: str, k: int = 10):\n        self._baseline = baseline_statement\n        self._mem = ConversationBufferWindowMemory(k=k)\n        self.reset(reason=\"INIT\")\n\n    def reset(self, reason: str) -&gt; None:\n        # Production: emit audit event (who/what/when/why)\n        self._mem.clear()\n        # Seed a non-negotiable baseline (keep it short)\n        self._mem.save_context(\n            {\"input\": \"BASELINE\"},\n            {\"output\": self._baseline}\n        )\n\n    @property\n    def memory(self):\n        return self._mem\n</code></pre><p><strong>Action:</strong> Treat resets as a containment control: trigger resets periodically (e.g., daily) and immediately on drift/high-risk detections. Ensure the baseline is stored in configuration control and is not editable by the agent.</p>"
                        }
                    ]
                },
                {
                    "id": "AID-I-004.002",
                    "name": "Persistent Memory Partitioning (Trust & Tenant Isolation)",
                    "pillar": ["data"],
                    "phase": ["building", "operation"],
                    "description": "Defines structural isolation for long-term memory (Vector DB/RAG). Uses namespaces/collections partitioned by tenant and trust tier, and enforces retrieval authorization via a centralized policy decision (never by agent self-assertion).",
                    "toolsOpenSource": [
                        "Qdrant (collections/tenancy patterns)",
                        "Weaviate (multi-tenancy features)",
                        "Milvus (partitions/collections)",
                        "OPA (policy-as-code for retrieval authorization)"
                    ],
                    "toolsCommercial": [
                        "Pinecone (indexes/namespaces)",
                        "Zilliz Cloud"
                    ],
                    "defendsAgainst": [
                        {
                            "framework": "MITRE ATLAS",
                            "items": [
                                "AML.T0070 RAG Poisoning",
                                "AML.T0080.000 AI Agent Context Poisoning: Memory"
                            ]
                        },
                        {
                            "framework": "MAESTRO",
                            "items": [
                                "Compromised RAG Pipelines (L2)",
                                "Data Poisoning (L2)"
                            ]
                        },
                        {
                            "framework": "OWASP LLM Top 10 2025",
                            "items": [
                                "LLM02:2025 Sensitive Information Disclosure",
                                "LLM08:2025 Vector and Embedding Weaknesses"
                            ]
                        }
                    ],
                    "implementationStrategies": [
                        {
                            "strategy": "Partition long-term memory by tenant + trust tier; retrieval must consult a central entitlement/policy service and be fully audited.",
                            "howTo": "<h5>Concept:</h5><p>Do not use a flat global vector index. Create partitions that encode <strong>tenant boundary</strong> and <strong>trust tier</strong> (e.g., <code>tenant123:public</code>, <code>tenant123:internal</code>, <code>tenant123:trusted</code>, <code>tenant123:quarantined</code>). The application decides readable namespaces based on identity + policy; the agent must not self-select namespaces.</p><h5>Example: Policy-Gated Retrieval</h5><pre><code># File: memory/retrieval_gate.py\nfrom typing import Dict, List\n\n\ndef get_allowed_namespaces(*, tenant_id: str, principal: Dict) -&gt; List[str]:\n    # Production: call OPA/ABAC service; do not hardcode roles inside the agent\n    roles = set(principal.get(\"roles\", []))\n    allowed = [f\"{tenant_id}:public\"]\n    if \"EMPLOYEE\" in roles:\n        allowed.append(f\"{tenant_id}:internal\")\n    if \"AI_PLATFORM_ADMIN\" in roles:\n        allowed.append(f\"{tenant_id}:trusted\")\n    return allowed\n\n\ndef secure_vector_search(*, vector_db, tenant_id: str, principal: Dict, query_vector: List[float]) -&gt; List[Dict]:\n    namespaces = get_allowed_namespaces(tenant_id=tenant_id, principal=principal)\n    results: List[Dict] = []\n\n    for ns in namespaces:\n        # Production: emit audit (principal, tenant, namespace, query hash)\n        hits = vector_db.search(collection=\"agent_memory\", namespace=ns, query_vector=query_vector, limit=5)\n        results.extend(hits)\n\n    return results\n</code></pre><p><strong>Action:</strong> Make namespace selection a backend authorization decision, not an LLM decision. Log every cross-namespace retrieval for forensics and compliance.</p>"
                        }
                    ]
                },
                {
                    "id": "AID-I-004.003",
                    "name": "Cryptographic Memory Integrity (Signed Write/Verify Read)",
                    "pillar": ["app", "data"],
                    "phase": ["operation"],
                    "description": "Establishes an end-to-end integrity loop for persistent memory: a controlled writer issues signed records (content-hash + metadata + key id), and an integrity-first loader verifies signatures and hashes before any content can re-enter agent context. This prevents direct-to-DB poisoning/tampering and forces memory provenance to be verifiable.",
                    "toolsOpenSource": [
                        "HashiCorp Vault (Transit) / SPIFFE-SVID for workload identity",
                        "Sigstore/cosign (attestation patterns)",
                        "Python stdlib (hashlib, hmac) / PyCA cryptography (asymmetric signing)"
                    ],
                    "toolsCommercial": [
                        "AWS KMS",
                        "Azure Key Vault",
                        "Google Cloud KMS"
                    ],
                    "defendsAgainst": [
                        {
                            "framework": "MITRE ATLAS",
                            "items": [
                                "AML.T0018.001 Manipulate AI Model: Poison LLM Memory",
                                "AML.T0080.000 AI Agent Context Poisoning: Memory"
                            ]
                        },
                        {
                            "framework": "OWASP LLM Top 10 2025",
                            "items": [
                                "LLM04:2025 Data and Model Poisoning"
                            ]
                        },
                        {
                            "framework": "OWASP ML Top 10 2023",
                            "items": [
                                "ML02:2023 Data Poisoning Attack",
                                "ML09:2023 Output Integrity Attack"
                            ]
                        }
                    ],
                    "implementationStrategies": [
                        {
                            "strategy": "Controlled writer: sign canonical metadata containing content-hash + namespace + issuer + timestamp + key-id; store detached signature alongside the record.",
                            "howTo": "<h5>Concept:</h5><p>To avoid canonicalization pitfalls, sign a small, canonical metadata payload that includes a <code>sha256(content)</code>. Store a <code>kid</code> (key id) to support key rotation. The writer service should run under workload identity and be the only component permitted to write into trusted namespaces.</p><h5>Example: Signed Record (HMAC for simplicity; swap to KMS/Asymmetric for enterprise)</h5><pre><code># File: memory/integrity/signed_record.py\nimport hashlib\nimport hmac\nimport json\nimport time\nfrom typing import Dict\n\n\ndef canonical_json(obj: Dict) -&gt; bytes:\n    return json.dumps(obj, sort_keys=True, separators=(\",\", \":\")).encode(\"utf-8\")\n\n\ndef make_signed_record(*, content: str, namespace: str, issuer: str, kid: str, signing_key: bytes) -&gt; Dict:\n    content_hash = hashlib.sha256(content.encode(\"utf-8\")).hexdigest()\n\n    meta = {\n        \"ver\": \"v1\",\n        \"kid\": kid,\n        \"ns\": namespace,\n        \"iss\": issuer,\n        \"ts\": int(time.time()),\n        \"chash\": content_hash\n    }\n\n    sig = hmac.new(signing_key, canonical_json(meta), hashlib.sha256).hexdigest()\n\n    return {\n        \"content\": content,\n        \"meta\": meta,\n        \"sig\": sig\n    }\n</code></pre><p><strong>Action:</strong> Require that only the controlled writer can write to trusted namespaces (enforce at API and DB/collection permissions). Include <code>kid</code> for rotation, and always sign canonical metadata, not arbitrary JSON formatting.</p>"
                        },
                        {
                            "strategy": "Integrity-first loader: verify signature (constant-time) -> re-hash content -> enforce schema/size -> only then return content to the agent (fail-closed).",
                            "howTo": "<h5>Concept:</h5><p>Anything loaded from persistent memory is untrusted until proven otherwise. Verify the signature using the key referenced by <code>kid</code>, then recompute the content hash. Only after integrity checks pass should schema/size checks run. On any failure, reject the record and emit a high-signal audit event.</p><h5>Example: Fail-Closed Verification Pipeline</h5><pre><code># File: memory/integrity/verify_and_load.py\nimport hashlib\nimport hmac\nimport json\nfrom typing import Dict, Optional\n\nMAX_CONTENT_CHARS = 10_000\nALLOWED_META_KEYS = {\"ver\", \"kid\", \"ns\", \"iss\", \"ts\", \"chash\"}\n\n\ndef canonical_json(obj: Dict) -&gt; bytes:\n    return json.dumps(obj, sort_keys=True, separators=(\",\", \":\")).encode(\"utf-8\")\n\n\ndef verify_and_load(*, record: Dict, keyring: Dict[str, bytes]) -&gt; Optional[str]:\n    try:\n        meta = record[\"meta\"]\n        sig = record[\"sig\"]\n        content = record[\"content\"]\n\n        if set(meta.keys()) != ALLOWED_META_KEYS:\n            raise ValueError(\"meta schema violation\")\n\n        kid = meta[\"kid\"]\n        key = keyring.get(kid)\n        if not key:\n            raise ValueError(\"unknown key id\")\n\n        expected_sig = hmac.new(key, canonical_json(meta), hashlib.sha256).hexdigest()\n        if not hmac.compare_digest(expected_sig, sig):\n            raise ValueError(\"signature mismatch\")\n\n        actual_hash = hashlib.sha256(content.encode(\"utf-8\")).hexdigest()\n        if actual_hash != meta[\"chash\"]:\n            raise ValueError(\"content hash mismatch\")\n\n        if not isinstance(content, str) or len(content) &gt; MAX_CONTENT_CHARS:\n            raise ValueError(\"content size/type violation\")\n\n        return content\n\n    except Exception:\n        # Production: emit audit event with reason category, record id/hash, namespace, issuer\n        return None\n</code></pre><p><strong>Action:</strong> Implement fail-closed verification. Use <code>hmac.compare_digest</code> (constant-time) to avoid timing oracles. Reject unknown <code>kid</code> to enforce rotation hygiene.</p>"
                        }
                    ]
                },
                {
                    "id": "AID-I-004.004",
                    "name": "Transactional Promotion Gates (Quarantine -> Trusted)",
                    "pillar": ["app", "data"],
                    "phase": ["operation"],
                    "description": "Implements a strict state machine and atomic promotion workflow for high-risk memory writes. Items routed into quarantine cannot influence agent behavior until reviewed and promoted. Promotion must be transactional, auditable, and typically re-signed as trusted. Aligns with trust-tiered memory write-gates (e.g., trusted/probation/quarantined).",
                    "toolsOpenSource": [
                        "PostgreSQL (transactions, row locks, RLS)",
                        "Kafka / Redis Streams (promotion queues)",
                        "Temporal / Celery (workflow execution)",
                        "OPA (policy-as-code for approval rules)"
                    ],
                    "toolsCommercial": [
                        "ServiceNow (approval workflows)",
                        "Jira Service Management"
                    ],
                    "defendsAgainst": [
                        {
                            "framework": "MITRE ATLAS",
                            "items": [
                                "AML.T0070 RAG Poisoning",
                                "AML.T0080.000 AI Agent Context Poisoning: Memory"
                            ]
                        },
                        {
                            "framework": "MAESTRO",
                            "items": [
                                "Data Poisoning (L2)",
                                "Compromised RAG Pipelines (L2)"
                            ]
                        },
                        {
                            "framework": "OWASP LLM Top 10 2025",
                            "items": [
                                "LLM04:2025 Data and Model Poisoning",
                                "LLM08:2025 Vector and Embedding Weaknesses"
                            ]
                        }
                    ],
                    "implementationStrategies": [
                        {
                            "strategy": "Quarantine state machine with atomic promotion: enforce allowed transitions and re-sign on promotion (fail-closed).",
                            "howTo": "<h5>Concept:</h5><p>All untrusted memory candidates land in <code>QUARANTINED</code> (or <code>PENDING</code>). Retrieval must exclude quarantined by default. Promotion must be an atomic transaction that: (1) locks the row, (2) validates current state, (3) records approver identity and rationale, (4) writes to trusted namespace via the controlled writer (<code>AID-I-004.003</code>), (5) marks the quarantined item as <code>PROMOTED</code> with immutable audit fields.</p><h5>Example: Transactional Promotion (PostgreSQL)</h5><pre><code># File: memory/promotion/promote.py\nfrom typing import Any, Dict\n\n\ndef promote_item(*, item_id: str, approver_id: str, db, controlled_writer) -&gt; Dict[str, Any]:\n    with db.transaction():\n        # 1) Lock row to prevent race conditions\n        item = db.select_for_update(\"quarantine_items\", where={\"id\": item_id})\n        if item[\"state\"] != \"PENDING\":\n            raise ValueError(\"item not pending\")\n\n        # 2) Write into trusted namespace ONLY via controlled writer (re-sign)\n        controlled_writer.write(\n            content=item[\"content\"],\n            namespace=item[\"target_trusted_namespace\"],\n            issuer=approver_id\n        )\n\n        # 3) Update state + immutable audit fields\n        db.update(\n            \"quarantine_items\",\n            where={\"id\": item_id},\n            values={\n                \"state\": \"PROMOTED\",\n                \"approved_by\": approver_id,\n                \"approved_at\": db.now_utc()\n            }\n        )\n\n    return {\"status\": \"PROMOTED\", \"id\": item_id}\n</code></pre><p><strong>Action:</strong> Enforce promotion via DB transactions + row locks. Never allow an agent to bypass quarantine and write directly into trusted memory. Treat promotion as a security boundary: auditable, policy-gated, and cryptographically sealed.</p>"
                        }
                    ]
                }
            ]
        },
        {
            "id": "AID-I-005",
            "name": "Emergency \"Kill-Switch\" / AI System Halt", "pillar": ["infra", "app"], "phase": ["response"],
            "description": "Establish and maintain a reliable, rapidly invokable mechanism to immediately halt, disable, or severely restrict the operation of an AI model or autonomous agent if it exhibits confirmed critical malicious behavior, goes \\\"rogue\\\" (acts far outside its intended parameters in a harmful way), or if a severe, ongoing attack is detected and other containment measures are insufficient. This is a last-resort containment measure designed to prevent catastrophic harm or further compromise.",
            "toolsOpenSource": [
                "Custom scripts/automation playbooks (Ansible, cloud CLIs) to stop/delete resources",
                "Circuit breaker patterns in microservices"
            ],
            "toolsCommercial": [
                "\\\"Red Button\\\" solutions from AI platform vendors",
                "Edge AI Safeguard solutions",
                "EDR/XDR solutions (SentinelOne, CrowdStrike) to kill processes/isolate hosts"
            ],
            "defendsAgainst": [
                {
                    "framework": "MITRE ATLAS",
                    "items": [
                        "AML.T0048 External Harms (Societal, Financial, Reputational, User)",
                        "AML.T0029 Denial of AI Service (by runaway agent)"
                    ]
                },
                {
                    "framework": "MAESTRO",
                    "items": [
                        "Agent acting on compromised goals/tools leading to severe harm (L7)",
                        "Runaway/critically malfunctioning foundation models (L1)"
                    ]
                },
                {
                    "framework": "OWASP LLM Top 10 2025",
                    "items": [
                        "LLM06:2025 Excessive Agency (ultimate backstop)",
                        "LLM10:2025 Unbounded Consumption (preventing catastrophic costs)"
                    ]
                },
                {
                    "framework": "OWASP ML Top 10 2023",
                    "items": [
                        "Any ML attack scenario causing immediate, unacceptable harm requiring emergency shutdown."
                    ]
                }
            ],
            "implementationStrategies": [
                {
                    "strategy": "Automated safety monitors and triggers for critical deviations.",
                    "howTo": "<h5>Concept:</h5><p>The kill-switch should not depend only on a human noticing a problem. An automated monitor can detect catastrophic runaway conditions (financial burn, mass failure, abusive behavior) and immediately trigger a halt. The halt is enforced globally and/or per-tenant by setting a shared halt flag that all inference and agent entrypoints must check (see Strategy 7).</p><h5>Step 1: Define Catastrophic Thresholds</h5><p>Define a very small set of metrics that mean \"this is absolutely not normal\" (e.g., hourly cost $>> expected, 90%+ error rate across requests, abnormally high volume of irreversible actions). These thresholds should be intentionally high, so they are only tripped under truly emergency conditions.</p><h5>Step 2: Automated Halt Monitor</h5><p>The monitor runs on a fast cadence (e.g., every minute as a cron job or serverless function), evaluates those metrics, and, if a threshold is breached, declares an emergency halt. That halt is recorded, auditable, and enforced by all traffic paths.</p><pre><code># File: safety_monitor/automated_halt.py\nimport time\nimport redis\nimport json\nimport datetime\n\n# These thresholds represent catastrophic conditions that should never\n# be hit during normal operation. If we hit them, we hard-stop.\nCATASTROPHIC_COST_THRESHOLD_USD = 1000      # per hour\nCATASTROPHIC_ERROR_RATE_PERCENT = 90        # 90%+ failures in last 5 min\n\n# Example: a multi-tenant service might have tenant-specific limits\nTENANT_COST_THRESHOLD_USD = 300             # per hour per tenant\n\n# Stubbed metric queries (replace with Prometheus/Datadog/etc.)\ndef get_estimated_cost_last_hour_global() -> float:\n    return 50.0\n\ndef get_estimated_cost_last_hour_by_tenant() -> dict:\n    # Return something like {\"tenantA\": 42.0, \"tenantB\": 1200.0}\n    return {\"tenantA\": 42.0, \"tenantB\": 1200.0}\n\ndef get_error_rate_last_5_minutes_global() -> float:\n    return 5.0\n\nr = redis.Redis()\n\ndef audit_halt_event(reason: str, scope: str, tenant_id: str = None):\n    \"\"\"Record halt trigger details for forensics and compliance.\"\"\"\n    event = {\n        \"timestamp\": datetime.datetime.utcnow().isoformat() + \"Z\",\n        \"scope\": scope,  # \"global\" or \"tenant\"\n        \"tenant_id\": tenant_id,\n        \"reason\": reason,\n        \"trigger_type\": \"automated\"\n    }\n    print(f\"HALT AUDIT: {json.dumps(event)}\")\n    # In production, also send to SIEM / incident channel.\n\n\ndef initiate_global_halt(reason: str):\n    r.set(\"SYSTEM_HALT_GLOBAL\", \"true\")\n    audit_halt_event(reason=reason, scope=\"global\")\n\n\ndef initiate_tenant_halt(tenant_id: str, reason: str):\n    r.set(f\"TENANT:{tenant_id}:HALT\", \"true\")\n    audit_halt_event(reason=reason, scope=\"tenant\", tenant_id=tenant_id)\n\n\ndef check_safety_metrics_once():\n    # 1. Global cost check\n    cost_global = get_estimated_cost_last_hour_global()\n    if cost_global > CATASTROPHIC_COST_THRESHOLD_USD:\n        initiate_global_halt(\n            reason=(\n                f\"Global hourly cost ${cost_global} exceeded \"\n                f\"threshold ${CATASTROPHIC_COST_THRESHOLD_USD}\"\n            )\n        )\n\n    # 2. Global error rate check\n    err_rate = get_error_rate_last_5_minutes_global()\n    if err_rate > CATASTROPHIC_ERROR_RATE_PERCENT:\n        initiate_global_halt(\n            reason=(\n                f\"Global error rate {err_rate}% exceeded \"\n                f\"threshold {CATASTROPHIC_ERROR_RATE_PERCENT}%\"\n            )\n        )\n\n    # 3. Per-tenant catastrophic spend check\n    tenant_costs = get_estimated_cost_last_hour_by_tenant()\n    for tenant_id, cost in tenant_costs.items():\n        if cost > TENANT_COST_THRESHOLD_USD:\n            initiate_tenant_halt(\n                tenant_id=tenant_id,\n                reason=(\n                    f\"Tenant {tenant_id} hourly cost ${cost} exceeded \"\n                    f\"threshold ${TENANT_COST_THRESHOLD_USD}\"\n                )\n            )\n\n# This function would be invoked by cron / scheduler every minute.\n</code></pre><p><strong>Action:</strong> Deploy an automated safety monitor that (1) evaluates catastrophic metrics, (2) sets either <code>SYSTEM_HALT_GLOBAL</code> or <code>TENANT:&lt;id&gt;:HALT</code> in a shared config/redis store, and (3) logs an auditable record including timestamp, trigger_type=\"automated\", and the justification. All inference/agent entrypoints MUST check these halt flags before doing work (see Strategy 7), so the stop is immediately enforced.</p>"
                },
                {
                    "strategy": "Provide secure, MFA-protected manual override for human operators.",
                    "howTo": "<h5>Concept:</h5><p>Security and SRE leadership need a \"red button\" to halt the AI system on demand. That button must be tightly controlled: MFA-protected, role-restricted, auditable, and ideally dual-control for high-value environments. When a human triggers it, we record who did it, why, and when. The manual trigger uses the same underlying halt flags as automated triggers, so enforcement is consistent.</p><h5>Hardened Admin Endpoint (FastAPI Example)</h5><p>This example shows an <code>/admin/emergency-halt</code> endpoint. It requires: (1) an admin role, (2) fresh MFA, (3) a justification string for audit. The handler sets the global halt flag and logs to SIEM. In production, you can also enforce dual-approval: two distinct admins must confirm within 60 seconds before the halt is finalized.</p><pre><code># File: api/admin_controls.py\nfrom fastapi import FastAPI, Request, Depends, HTTPException\nfrom pydantic import BaseModel\nimport redis\nimport datetime\nimport json\n\napp = FastAPI()\nr = redis.Redis()\n\nclass HaltRequest(BaseModel):\n    justification: str\n    scope: str = \"global\"        # \"global\" or \"tenant\"\n    tenant_id: str | None = None  # required if scope == \"tenant\"\n\nclass UserContext(BaseModel):\n    id: str\n    is_admin: bool\n    mfa_recent: bool\n    # You can also attach 'second_approver_confirmed' for dual-control flows.\n\n# Dependency to extract/validate the caller's identity and MFA status.\nasync def require_admin_user(request: Request) -> UserContext:\n    user = getattr(request.state, \"user\", None)\n    if user is None:\n        raise HTTPException(status_code=401, detail=\"Not authenticated\")\n    if not user.is_admin:\n        raise HTTPException(status_code=403, detail=\"Admin role required\")\n    if not user.mfa_recent:\n        raise HTTPException(status_code=403, detail=\"Fresh MFA required\")\n    return user\n\n# Simple audit helper\ndef audit_manual_halt(user: UserContext, scope: str, justification: str, tenant_id: str | None):\n    event = {\n        \"timestamp\": datetime.datetime.utcnow().isoformat() + \"Z\",\n        \"trigger_type\": \"manual\",\n        \"operator_id\": user.id,\n        \"scope\": scope,\n        \"tenant_id\": tenant_id,\n        \"justification\": justification\n    }\n    print(f\"HALT AUDIT: {json.dumps(event)}\")\n    # Also send to SIEM / paging channel / incident Slack.\n\n@app.post(\"/admin/emergency-halt\")\nasync def trigger_manual_halt(body: HaltRequest, user: UserContext = Depends(require_admin_user)):\n    # Optionally enforce dual-control here before proceeding.\n    if body.scope == \"global\":\n        r.set(\"SYSTEM_HALT_GLOBAL\", \"true\")\n    elif body.scope == \"tenant\":\n        if not body.tenant_id:\n            raise HTTPException(status_code=400, detail=\"tenant_id required for tenant halt\")\n        r.set(f\"TENANT:{body.tenant_id}:HALT\", \"true\")\n    else:\n        raise HTTPException(status_code=400, detail=\"Invalid scope\")\n\n    audit_manual_halt(\n        user=user,\n        scope=body.scope,\n        justification=body.justification,\n        tenant_id=body.tenant_id,\n    )\n\n    # Notify incident channel immediately (pager/slack/etc.)\n    # send_critical_alert(\"MANUAL KILL-SWITCH ACTIVATED\", event)\n\n    return {\"status\": \"halt_flag_set\", \"scope\": body.scope}\n</code></pre><p><strong>Action:</strong> Expose a protected admin-only kill-switch endpoint. Require admin role + fresh MFA each time. Log who triggered it, for what reason, at what time, and (optionally) who co-approved it. Setting the halt flag should immediately propagate to runtime enforcement (see Strategy 7), not rely on humans to manually scale services down.</p>"
                },
                {
                    "strategy": "Design agents with internal watchdog threads that terminate unresponsive or runaway behavior.",
                    "howTo": "<h5>Concept:</h5><p>Sometimes a single agent process goes rogue: infinite loop, self-reinforcing tool calls, or it stops responding but is still consuming GPU/CPU. A watchdog thread inside the agent monitors liveness via a heartbeat timestamp. If the main loop stops updating its heartbeat, the watchdog forcefully kills the process (<code>os._exit(1)</code>). The orchestrator (Kubernetes, supervisor, etc.) will restart the agent â€” but in quarantine / safe mode, not full trust.</p><h5>Watchdog Implementation</h5><pre><code># File: agent/base_agent.py\nimport threading\nimport time\nimport os\nimport json\nimport datetime\n\nclass WatchdogAgent:\n    def __init__(self, agent_id: str, tenant_id: str, heartbeat_timeout: int = 60):\n        self.agent_id = agent_id\n        self.tenant_id = tenant_id\n        self.last_heartbeat = time.time()\n        self.timeout = heartbeat_timeout\n        self.is_running = True\n\n        # Start watchdog thread\n        self.watchdog_thread = threading.Thread(\n            target=self._watchdog_loop,\n            daemon=True\n        )\n        self.watchdog_thread.start()\n\n    def _audit_watchdog_terminate(self, reason: str):\n        event = {\n            \"timestamp\": datetime.datetime.utcnow().isoformat() + \"Z\",\n            \"trigger_type\": \"watchdog_local\",\n            \"agent_id\": self.agent_id,\n            \"tenant_id\": self.tenant_id,\n            \"reason\": reason\n        }\n        print(f\"WATCHDOG AUDIT: {json.dumps(event)}\")\n        # Send to SIEM / incident channel in production.\n\n    def _watchdog_loop(self):\n        while self.is_running:\n            time.sleep(self.timeout / 2)\n            if time.time() - self.last_heartbeat > self.timeout:\n                self._audit_watchdog_terminate(\n                    reason=\"main loop heartbeat timeout\"\n                )\n                # Hard-kill the process. K8s / supervisor will restart it.\n                os._exit(1)\n\n    def main_loop(self):\n        while self.is_running:\n            # --- Agent core logic here ---\n            time.sleep(5)\n            # --- End agent logic ---\n\n            # Refresh heartbeat at the end of each cycle\n            self.last_heartbeat = time.time()\n\n    def stop(self):\n        self.is_running = False\n</code></pre><p><strong>Action:</strong> Embed an internal watchdog in each autonomous agent process. When the watchdog kills the process, the orchestrator (e.g. Kubernetes) <em>must</em> restart that agent in a degraded, quarantined, or safe mode namespace (see AID-I-003 \"safe mode\") rather than restoring full privileges. All watchdog-triggered terminations should emit an auditable event that links <code>agent_id</code>, <code>tenant_id</code>, timestamp, and reason, to support later forensics.</p>"
                },
                {
                    "strategy": "Define and version-control a formal Kill-Switch Activation SOP.",
                    "howTo": "<h5>Concept:</h5><p>Because a kill-switch has huge operational and business impact, you cannot improvise under stress. You need a written Standard Operating Procedure (SOP) that defines when, who, and how to activate the kill-switch. This reduces hesitation in real crises and prevents abuse during false alarms. The SOP must live in version control, and edits must require security/governance approval.</p><h5>Example SOP Structure</h5><pre><code># File: docs/sop/KILL_SWITCH_PROTOCOL.md\n\n# SOP: AI System Emergency Halt Protocol\n\n## 1. Activation Criteria (ANY of the following)\n- A. Confirmed Data Breach: Active, unauthorized exfiltration of sensitive data (PII, financial) via an AI component.\n- B. Confirmed Financial Loss: Uncontrolled agent behavior causing financial loss > $10,000 USD.\n- C. Critical System Manipulation: Core agent's signed goal (see AID-D-010) bypassed; agent performing ungoverned high-risk actions.\n- D. Catastrophic Resource Consumption: Automated alert (see AID-I-005.001) indicates runaway cost or failure state.\n\n## 2. Authorized Personnel (MFA required for each activation)\n- On-Call SRE Lead\n- Director of Security Operations\n- CISO\n\n## 3. Activation Procedure\n1. Open the Admin Control Panel.\n2. Complete MFA.\n3. Select Global Halt or Tenant-Specific Halt.\n4. Enter justification with incident ticket link.\n5. (If policy requires) obtain second approver confirmation within 60 seconds.\n6. Confirm to set the halt flag.\n\n## 4. Immediate Communication Protocol\n- Immediately notify #ai-incident-response (or equivalent) with @here.\n- Include justification, scope (global vs tenant), and timestamp.\n\n## 5. Governance Note\n- This SOP is stored in version control.\n- Any change requires signoff from Security + Engineering leadership.\n</code></pre><p><strong>Action:</strong> Write and maintain an Emergency Halt SOP in version control. Clearly define activation criteria, authorized roles, MFA/dual-control requirements, and notification steps. Treat SOP edits as controlled changes that require formal approval from Security and Engineering leadership, so auditors can verify that governance was followed.</p>"
                },
                {
                    "strategy": "Develop a controlled post-halt restart and verification checklist (cold start procedure).",
                    "howTo": "<h5>Concept:</h5><p>After an emergency halt, you cannot just \"turn it back on.\" You need a structured cold start process focused on safety and containment. The process confirms that the root cause is fixed, memory/agent state is clean, and risky capabilities are initially disabled. Only after staged verification does the system regain full autonomy. This protects you from instantly re-triggering the same failure.</p><h5>Post-Halt Restart Checklist</h5><pre><code># File: docs/sop/POST_HALT_RESTART_CHECKLIST.md\n\n# Checklist: AI System Cold Start Procedure\n\n**Incident Ticket:** [Link to JIRA/incident]\n\n## Phase 1: Remediation & Verification\n- [ ] 1.1 Root Cause Identified: The vulnerability / exploit path / misalignment cause is documented.\n- [ ] 1.2 Patch Deployed: All relevant fixes are applied (code, firewall, IAM, etc.).\n- [ ] 1.3 Artifact Integrity Verified: Run integrity/attestation checks (see AID-D-004) on model weights, container images, runtime code.\n- [ ] 1.4 State Cleared: Flush or quarantine all volatile agent memory stores, RAG caches, conversation histories that could carry poisoning forward.\n\n## Phase 2: Staged Restart (Safe Mode)\n- [ ] 2.1 Bring Up in Safe Mode: Start services with high-risk agent tools disabled, external actions disabled, rate limits tightened.\n- [ ] 2.2 Health Checks Pass: All core health checks are green.\n- [ ] 2.3 Targeted Exploit Regression Test: Re-run tests specifically crafted to reproduce the original incident.\n\n## Phase 3: Service Restoration\n- [ ] 3.1 Dual Sign-Off: On-call SRE Lead AND Security Duty Officer both approve exiting safe mode.\n- [ ] 3.2 Restore Full Functionality: Re-enable normal capability only after approval.\n- [ ] 3.3 Heightened Monitoring (24h): For the next 24h, automatically post status (safe mode status, cost burn, error rates) to the incident channel and SIEM.\n- [ ] 3.4 Schedule Post-Mortem: A blameless post-mortem is scheduled.\n</code></pre><p><strong>Action:</strong> Require a cold start checklist after any kill-switch event. Demand dual sign-off (operations + security) before leaving safe mode. For at least 24 hours after restoration, continuously surface telemetry to the incident channel (cost burn, error rate, external tool usage) so that leadership and audit have assurance that the system is stable and still under control.</p>"
                },
                {
                    "strategy": "Document and govern the kill-switch within Human-in-the-Loop (HITL) control mapping (AID-M-006).",
                    "howTo": "<h5>Concept:</h5><p>The emergency halt is not \"outside the system\" â€” it is actually the most extreme Human-in-the-Loop (HITL) checkpoint. You should represent it explicitly in your HITL registry (AID-M-006), just like any other high-risk approval step. That gives auditors and internal governance teams a single source of truth for all manual intervention points, including who can invoke them, expected response times, and what happens if nobody responds in time.</p><h5>HITL Checkpoint Definition</h5><pre><code># File: design/hitl_checkpoints.yaml (excerpt; see AID-M-006)\n\nhitl_checkpoints:\n  - id: \"HITL-CP-001\"\n    name: \"High-Value Financial Transaction Approval\"\n    # ...\n\n  # ... other checkpoints ...\n\n  - id: \"HITL-CP-999\"  # Special ultimate override\n    name: \"Emergency System Halt (Manual Kill-Switch)\"\n    description: \"A manual control to immediately halt all AI agent operations across the entire system or for a specific tenant.\"\n    component: \"system-wide\"\n    trigger:\n      type: \"Manual\"\n      condition: \"An authorized operator with MFA invokes /admin/emergency-halt or presses the physical red button UI in the admin console.\"\n    decision_type: \"Confirm Halt\"\n    required_data: [\n      \"operator_id\",\n      \"incident_ticket_id\",\n      \"justification_text\"\n    ]\n    operator_role: \"AI_System_Admin_L3\"\n    sla_seconds: 60  # Time allowed to complete the halt action\n    default_action_on_timeout: \"Halt\"  # Fail-safe: if approval flow times out, we halt\n    accountability:\n      on_call_role: \"SRE_OnCall_L3\"\n      security_role: \"Security_Duty_Officer\"\n      audit_log_target: \"SIEM+IncidentChannel\"\n    version_control:\n      governance: \"Changes to HITL-CP-999 require joint approval by Security and Engineering leadership.\"\n</code></pre><p><strong>Action:</strong> Add the emergency kill-switch as <code>HITL-CP-999</code> in your HITL registry. Define the authorized operator role, time-to-act SLA, fail-safe default (<code>default_action_on_timeout: Halt</code>), audit logging destination, and which on-call rotations are accountable. Require security+engineering leadership approval for any changes to this checkpoint so governance can prove that control of the kill-switch is tightly managed.</p>"
                },
                {
                    "strategy": "Enforce global/tenant halt flags at every inference request path and agent loop (fail-closed).",
                    "howTo": "<h5>Concept:</h5><p>This is where the kill-switch actually \"bites.\" Every inference API endpoint and every agent execution loop must check a shared halt flag <em>before</em> doing any work. If <code>SYSTEM_HALT_GLOBAL</code> is set, nobody runs. If <code>TENANT:&lt;id&gt;:HALT</code> is set, that tenant gets blocked or forced into degraded safe mode. The request is rejected or downgraded automatically, without waiting for humans to scale deployments down. This makes the system fail-closed under emergency conditions.</p><h5>FastAPI Middleware Gate for Inference Requests</h5><pre><code># File: runtime/halt_gate.py\nfrom fastapi import FastAPI, Request\nfrom fastapi.responses import JSONResponse\nimport redis\n\napp = FastAPI()\nr = redis.Redis()\n\n@app.middleware(\"http\")\nasync def halt_gate(request: Request, call_next):\n    # 1. Global halt check\n    global_halt = r.get(\"SYSTEM_HALT_GLOBAL\")\n    if global_halt and global_halt.decode(\"utf-8\") == \"true\":\n        # Immediately deny all work. Fail-closed.\n        return JSONResponse(\n            status_code=503,\n            content={\n                \"error\": \"SYSTEM_HALT_ACTIVE\",\n                \"message\": \"Service temporarily disabled by emergency kill-switch.\",\n            },\n            headers={\"X-System-Halt\": \"true\"}\n        )\n\n    # 2. Tenant-specific halt check (example header or auth-derived)\n    tenant_id = request.headers.get(\"X-Tenant-ID\")\n    if tenant_id:\n        tenant_flag = r.get(f\"TENANT:{tenant_id}:HALT\")\n        if tenant_flag and tenant_flag.decode(\"utf-8\") == \"true\":\n            # Option A: hard block this tenant\n            return JSONResponse(\n                status_code=429,\n                content={\n                    \"error\": \"TENANT_HALTED\",\n                    \"tenant\": tenant_id,\n                    \"message\": \"Tenant temporarily quarantined by kill-switch.\",\n                },\n                headers={\"X-Tenant-Halt\": \"true\"}\n            )\n            # Option B instead: downgrade to 'safe mode' with no external tools,\n            # fewer capabilities, etc. (See AID-I-003 safe mode.)\n\n    # 3. If no halt applies, continue normally\n    response = await call_next(request)\n    return response\n</code></pre><h5>Agent Loop Check (Non-HTTP Autonomous Agent)</h5><pre><code># File: runtime/agent_loop_gate.py\nimport time\nimport redis\n\nr = redis.Redis()\n\nclass HaltAwareAgent:\n    def __init__(self, agent_id: str, tenant_id: str):\n        self.agent_id = agent_id\n        self.tenant_id = tenant_id\n        self.running = True\n\n    def should_halt(self) -> bool:\n        # Check global halt first\n        global_halt = r.get(\"SYSTEM_HALT_GLOBAL\")\n        if global_halt and global_halt.decode(\"utf-8\") == \"true\":\n            return True\n        # Check tenant-level halt\n        tenant_flag = r.get(f\"TENANT:{self.tenant_id}:HALT\")\n        if tenant_flag and tenant_flag.decode(\"utf-8\") == \"true\":\n            return True\n        return False\n\n    def run_forever(self):\n        while self.running:\n            if self.should_halt():\n                print(\n                    f\"Agent {self.agent_id} for tenant {self.tenant_id} \"\n                    f\"stopping due to HALT flag.\"\n                )\n                # Immediately stop doing work. Fail-closed.\n                break\n\n            # --- normal agent reasoning / tool calls / actions here ---\n            time.sleep(2)\n            # --- end normal agent work ---\n</code></pre><p><strong>Action:</strong> Add a universal halt gate in front of <em>all</em> inference endpoints and agent loops. Before doing any inference, tool invocation, or autonomous action, check the global (<code>SYSTEM_HALT_GLOBAL</code>) and tenant (<code>TENANT:&lt;id&gt;:HALT</code>) flags in your shared config store. If either is active, immediately block the request or force the agent to stop / fall back to safe mode. Include a clear response header (e.g. <code>X-System-Halt</code>) and log events for auditing. This guarantees kill-switch enforcement is automatic, immediate, and fail-closed.</p>"
                }
            ]
        },
        {
            "id": "AID-I-006",
            "name": "Malicious Participant Isolation in Federated Unlearning",
            "pillar": ["model"],
            "phase": ["response"],
            "description": "Identifies and logically isolates the influence of malicious clients within a Federated Learning (FL) system, particularly during a machine unlearning or model restoration process. Once identified, the malicious participants' data contributions and model updates are excluded from the unlearning or retraining calculations. This technique is critical for preventing attackers from sabotaging the model recovery process and ensuring the final restored model is not corrupted.",
            "implementationStrategies": [
                {
                    "strategy": "Identify malicious participants by clustering their historical model updates.",
                    "howTo": "<h5>Concept:</h5><p>Before beginning an unlearning process, the server can analyze the historical updates from all clients to identify outliers. The assumption is that malicious clients who sent poisoned updates will have submitted updates that are statistically different from the majority of honest clients. These malicious clients will form small, anomalous clusters in the high-dimensional space of model weights.</p><h5>Analyze and Cluster Historical Updates</h5><p>Use a clustering algorithm like DBSCAN on the flattened model update vectors from all clients over several previous rounds, and label those in outlier clusters as suspicious.</p><pre><code># File: isolate/fl_participant_analysis.py\nfrom sklearn.cluster import DBSCAN\nimport numpy as np\n\n# Example input: list of (client_id, update_vector) pairs\n# all_historical_updates = [\n#     (\"client_A\", [0.01, -0.03, 0.04, ...]),\n#     (\"client_B\", [0.02, -0.02, 0.05, ...]),\n#     ...\n# ]\n\nclient_ids = [item[0] for item in all_historical_updates]\nupdate_vectors = np.array([item[1] for item in all_historical_updates])\n\n# DBSCAN will assign cluster labels (0,1,2,...) to dense groups and -1 to outliers\nclustering = DBSCAN(eps=0.7, min_samples=5).fit(update_vectors)\n\nmalicious_client_ids = [\n    client_ids[i]\n    for i, label in enumerate(clustering.labels_)\n    if label == -1  # -1 means 'outlier' in DBSCAN\n]\n\nif malicious_client_ids:\n    print(f\"Potentially malicious clients to isolate: {malicious_client_ids}\")\n\n# This list will be used in subsequent isolation steps.\n</code></pre><p><strong>Action:</strong> Implement a pre-unlearning analysis step that clusters historical client updates and extracts an isolation list of suspicious participants. Treat that list as untrusted until proven otherwise, and feed it into later filtering/enforcement logic.</p>"
                },
                {
                    "strategy": "Logically exclude contributions from isolated clients during the unlearning or retraining process.",
                    "howTo": "<h5>Concept:</h5><p>Once a list of malicious participants is identified, the core isolation step is to filter out their data from the set used for model restoration. The goal is to rebuild or unlearn the model <em>without</em> their influence, so their updates and samples cannot bias or corrupt the recovered model.</p><h5>Filter the Dataset Before Unlearning</h5><p>Before running unlearning/retraining, generate a 'clean' dataset sourced only from trusted clients. The unlearning algorithm should operate exclusively on this filtered dataset.</p><pre><code># File: isolate/fl_unlearning_isolation.py\n\n# full_historical_dataset is a list of (client_id, data_sample) pairs\n# Example:\n# full_historical_dataset = [\n#     (\"client_A\", {\"x\": [...], \"y\": [...]}),\n#     (\"client_B\", {\"x\": [...], \"y\": [...]}),\n#     ...\n# ]\n\n# malicious_ids is the set of clients marked as isolated/untrusted\n# malicious_ids = {\"client_Z\", \"client_Q\"}\n\nclean_dataset_for_unlearning = [\n    (client_id, sample)\n    for (client_id, sample) in full_historical_dataset\n    if client_id not in malicious_ids\n]\n\ntrusted_client_count = len(set([cid for (cid, _) in clean_dataset_for_unlearning]))\nprint(\n    f\"Isolating {len(malicious_ids)} clients. \"\n    f\"Proceeding with unlearning using data from {trusted_client_count} trusted clients.\"\n)\n\n# Now feed only the clean dataset into the unlearning / restorative retraining flow.\n# restored_model = perform_unlearning(clean_dataset_for_unlearning)\n</code></pre><p><strong>Action:</strong> At the start of the federated unlearning job, construct a filtered view of historical training data that excludes any client on the isolation list. The unlearning / restoration should treat that filtered view as the source of truth.</p>"
                },
                {
                    "strategy": "Apply a real-time filtering strategy during the unlearning process to isolate new malicious updates.",
                    "howTo": "<h5>Concept:</h5><p>Attackers may attempt to sabotage the unlearning process itself by injecting new poisoned updates (for example, extreme gradients that try to distort the restored model). The unlearning loop must therefore apply a robust aggregation rule that automatically down-weights or discards outliers. This is similar to Byzantine-robust aggregation in federated learning.</p><h5>Use a Robust Aggregator in the Unlearning Loop</h5><p>A Trimmed Mean aggregator removes the highest and lowest values along each dimension before averaging, which isolates extreme, adversarial updates.</p><pre><code># File: isolate/fl_unlearning_loop.py\nimport numpy as np\nfrom scipy.stats import trim_mean\n\n# Pseudocode for an unlearning step\n# unlearning_steps = 10\n# for step in range(unlearning_steps):\n#     # 1. Collect new 'negative influence' updates from clients\n#     #    (gradients or parameter deltas intended to remove certain data effects)\n#     unlearning_updates = get_updates_from_clients()\n#     # unlearning_updates: np.ndarray shape [num_clients, num_params]\n#\n#     # 2. Apply a trimmed mean across each parameter dimension.\n#     #    The 0.1 here trims 10% off each tail (top and bottom).\n#     aggregated_update = trim_mean(unlearning_updates, 0.1, axis=0)\n#\n#     # 3. Apply this robustly aggregated update to the global model.\n#     global_model.apply_update(aggregated_update)\n\n# Note: trim_mean returns an array representing the mean after trimming extremes.\n</code></pre><p><strong>Action:</strong> During each unlearning iteration, aggregate client updates using a robust estimator (e.g. trimmed mean, median-of-means, coordinate-wise median) rather than naive averaging. This isolates and suppresses malicious outliers trying to poison the restoration.</p>"
                },
                {
                    "strategy": "Maintain a dynamic reputation score and exclude any client whose score falls below a critical threshold.",
                    "howTo": "<h5>Concept:</h5><p>Instead of treating isolation as a one-time event, maintain an evolving reputation score for each client. Clients that repeatedly submit anomalous, low-quality, or adversarial updates see their score decay. Once their score drops below a threshold, they are automatically blocked from future training and unlearning rounds. This enforces continuous isolation of bad actors.</p><h5>Implement a Reputation-Based Isolation Policy</h5><p>Before selecting clients for a round, check their reputation score and exclude any below threshold. This is enforced in code at participant selection time, not just documented as a policy.</p><pre><code># File: isolate/fl_reputation_gating.py\nimport random\n\nREPUTATION_THRESHOLD = 0.2  # Scores are normalized 0.0 - 1.0\n\nclass ReputationManager:\n    def __init__(self):\n        # Example internal store: {client_id: score}\n        self.scores = {}\n\n    def get_score(self, client_id: str) -> float:\n        return self.scores.get(client_id, 1.0)\n\n    def decay_score(self, client_id: str, amount: float) -> None:\n        current = self.get_score(client_id)\n        new_score = max(0.0, current - amount)\n        self.scores[client_id] = new_score\n\n# Example usage:\n# reputation_manager = ReputationManager()\n# reputation_manager.scores = {\n#     \"client_A\": 0.9,\n#     \"client_B\": 0.75,\n#     \"client_Z\": 0.05  # repeatedly malicious\n# }\n\n\ndef select_clients_for_round(all_clients, num_to_select, reputation_manager: ReputationManager):\n    trusted_clients = []\n    for client_id in all_clients:\n        score = reputation_manager.get_score(client_id)\n        if score >= REPUTATION_THRESHOLD:\n            trusted_clients.append(client_id)\n        else:\n            print(f\"Isolating client {client_id} due to low reputation score {score}.\")\n\n    if not trusted_clients:\n        return []\n\n    return random.sample(trusted_clients, min(num_to_select, len(trusted_clients)))\n</code></pre><p><strong>Action:</strong> Integrate a reputation manager directly into the federated learning server's client selection logic. Any client with a score below the defined threshold is automatically excluded (fail-closed), without requiring a human approval step.</p>"
                },
                {
                    "strategy": "Require cryptographic attestation or signed client updates before accepting them into training or unlearning.",
                    "howTo": "<h5>Concept:</h5><p>If isolation is based purely on a 'client_id' string, an attacker can simply reconnect using a new ID and continue poisoning. To make isolation meaningful, each client's update must be cryptographically tied to a verifiable identity. The server should only accept updates that include a valid signature or attestation proving they come from an approved runtime (for example, a specific enclave, container identity, or SPIFFE/SPIRE workload identity). Unsigned or unverifiable updates are rejected immediately. This enforces that 'banned' clients cannot trivially respawn.</p><h5>Server-Side Signature Verification</h5><p>Below is an example of verifying an Ed25519 signature on a client's submitted update vector before accepting it. If verification fails, the update is discarded and the client is effectively isolated at the transport boundary.</p><pre><code># File: isolate/verify_signed_update.py\nimport nacl.signing\nimport nacl.exceptions\nimport numpy as np\n\n# Server stores a registry of approved client public keys\n# approved_keys = {\n#     \"client_A\": b\"\\x12\\x34...\",  # Ed25519 public key bytes\n#     \"client_B\": b\"\\xab\\xcd...\",\n# }\n\n\ndef verify_client_update(client_id: str, update_vector: np.ndarray, signature: bytes, approved_keys: dict) -> bool:\n    \"\"\"\n    Returns True if the update is cryptographically valid and trusted.\n    Returns False if signature fails or client_id is unknown.\n    \"\"\"\n    if client_id not in approved_keys:\n        print(f\"Rejecting update: unknown client {client_id}.\")\n        return False\n\n    pubkey_bytes = approved_keys[client_id]\n    verify_key = nacl.signing.VerifyKey(pubkey_bytes)\n\n    # Serialize the update vector in a deterministic way\n    payload = update_vector.astype(np.float32).tobytes()\n\n    try:\n        verify_key.verify(payload, signature)\n        return True\n    except nacl.exceptions.BadSignatureError:\n        print(f\"Rejecting update: bad signature from {client_id}.\")\n        return False\n\n# Usage in the training / unlearning ingestion path:\n# if not verify_client_update(client_id, update_vector, signature, approved_keys):\n#     # Fail-closed: do not even consider this update for aggregation\n#     continue\n</code></pre><p><strong>Action:</strong> Enforce that all client updates (including those used for unlearning) are signed with a trusted key or attested identity. At ingestion time, verify the signature. If verification fails, drop the update immediately. This prevents banned actors from simply reappearing under a new logical name.</p>"
                },
                {
                    "strategy": "Enforce a persistent isolation/blocklist at training-time and unlearning-time (fail-closed).",
                    "howTo": "<h5>Concept:</h5><p>Isolation is only effective if it is consistently enforced at every trust boundary. That means the server must consult an authoritative blocklist (isolation list) <em>before</em> it accepts a client's participation in any round (training or unlearning), and must refuse that client's gradients/updates at ingestion time. The default posture is fail-closed: if a client is on the isolation list, that client cannot influence the model.</p><h5>Runtime Blocklist Enforcement</h5><p>The blocklist should live in persistent storage (for example, a database table or etcd key). Every time the server receives a new update or a participation request, it checks the blocklist first and immediately rejects any client that appears there.</p><pre><code># File: isolate/blocklist_enforcement.py\nimport json\nfrom typing import Set\n\n# Example persistent store: a JSON file, DB table, etc.\n# In production, load this from a secure central store, not a local file.\nISOLATION_LIST_PATH = \"/etc/federated/isolation_list.json\"\n\n\ndef load_isolation_list() -> Set[str]:\n    try:\n        with open(ISOLATION_LIST_PATH, \"r\", encoding=\"utf-8\") as f:\n            data = json.load(f)\n            # data is expected like: {\"isolated_clients\": [\"client_Z\", \"client_Q\"]}\n            return set(data.get(\"isolated_clients\", []))\n    except FileNotFoundError:\n        return set()\n\n\ndef is_client_blocked(client_id: str, isolation_set: Set[str]) -> bool:\n    return client_id in isolation_set\n\n\ndef accept_client_update(client_id: str, update_vector, isolation_set: Set[str]) -> bool:\n    \"\"\"\n    Returns True if the server should accept this client's update,\n    False if the client is isolated and must be rejected (fail-closed).\n    \"\"\"\n    if is_client_blocked(client_id, isolation_set):\n        print(f\"Client {client_id} is isolated. Rejecting update.\")\n        return False\n    return True\n\n# Example usage in aggregation pipeline:\n# isolation_set = load_isolation_list()\n# for (client_id, update_vector) in incoming_updates:\n#     if not accept_client_update(client_id, update_vector, isolation_set):\n#         continue  # skip this malicious/suspended client entirely\n#     process_update(update_vector)\n</code></pre><p><strong>Action:</strong> Maintain a persistent isolation list (blocklist) of client IDs that are considered malicious. On every round of training or unlearning, load this list and enforce it programmatically at ingestion time. If a client is on the list, its updates are never applied. The system fails closed by default.</p>"
                }
            ],
            "toolsOpenSource": [
                "TensorFlow Federated (TFF)",
                "Flower (Federated Learning Framework)",
                "PySyft (OpenMined)",
                "NVIDIA FLARE",
                "scikit-learn (for clustering/anomaly detection)",
                "PyTorch",
                "TensorFlow",
                "SPIFFE/SPIRE (for workload identity attestation and signed workload identities)"
            ],
            "toolsCommercial": [
                "Enterprise Federated Learning Platforms (Owkin, Substra Foundation, IBM)",
                "MLOps Platforms with Federated Learning capabilities (Amazon SageMaker)",
                "AI Security Platforms (Protect AI, HiddenLayer)"
            ],
            "defendsAgainst": [
                {
                    "framework": "MITRE ATLAS",
                    "items": [
                        "AML.T0020 Poison Training Data",
                        "AML.T0019 Publish Poisoned Datasets"
                    ]
                },
                {
                    "framework": "MAESTRO",
                    "items": [
                        "Data Poisoning (L2)",
                        "Attacks on Decentralized Learning (Cross-Layer)"
                    ]
                },
                {
                    "framework": "OWASP LLM Top 10 2025",
                    "items": [
                        "LLM04:2025 Data and Model Poisoning"
                    ]
                },
                {
                    "framework": "OWASP ML Top 10 2023",
                    "items": [
                        "ML02:2023 Data Poisoning Attack",
                        "ML10:2023 Model Poisoning"
                    ]
                }
            ]
        },
        {
            "id": "AID-I-007",
            "name": "Client-Side AI Execution Isolation",
            "pillar": ["app"],
            "phase": ["operation"],
            "description": "This technique focuses on containing a compromised or malicious client-side model, preventing it from accessing sensitive data from other browser tabs, local application context, or the operating system. It addresses the security challenges of AI models that execute in untrusted environments like a user's web browser, Electron shell, hybrid mobile app, or native mobile runtime. This assumes the model or model runtime may already be tampered with or coerced (e.g. prompt-injected, modified weights, wrapped with hostile JS). The goal is not to \"fix\" the model but to strictly confine its blast radius using sandboxing, least capability, and controlled IPC.",
            "toolsOpenSource": [
                "WebAssembly runtimes (Wasmtime, Wasmer, browser WebAssembly runtime)",
                "TensorFlow.js, ONNX.js",
                "Web Workers (Browser API)",
                "Browser postMessage() channel (structured clone IPC for sandboxed components)",
                "Sandboxed iframes (HTML5 iframe with sandbox attribute)",
                "Content Security Policy (CSP) headers"
            ],
            "toolsCommercial": [
                "Mobile OS sandboxing (iOS App Sandbox, Android Application Sandbox)",
                "Enterprise Mobile Device Management (MDM) solutions with app sandboxing / clipboard control / data loss prevention"
            ],
            "defendsAgainst": [
                {
                    "framework": "MITRE ATLAS",
                    "items": [
                        "AML.T0025 Exfiltration via Cyber Means (from client device)",
                        "AML.T0037 Data from Local System (stealing browser/app state, session tokens, local storage)"
                    ]
                },
                {
                    "framework": "MAESTRO",
                    "items": [
                        "Data Exfiltration (L2, from the client)",
                        "Runtime Code Injection (L4, hostile JS/WASM in the browser process or hybrid app sandbox)"
                    ]
                },
                {
                    "framework": "OWASP LLM Top 10 2025",
                    "items": [
                        "LLM02:2025 Sensitive Information Disclosure (blocking direct access to other DOM state / tokens / org data)",
                        "LLM05:2025 Improper Output Handling (preventing model-produced HTML/JS from gaining privileged DOM execution)"
                    ]
                },
                {
                    "framework": "OWASP ML Top 10 2023",
                    "items": [
                        "ML06:2023 AI Supply Chain Attacks (containing a malicious downloaded client-side model/runtime so it cannot pivot)"
                    ]
                }
            ],
            "implementationStrategies": [
                {
                    "strategy": "Execute AI models in a dedicated Web Worker.",
                    "howTo": "<h5>Concept:</h5><p>A Web Worker runs JavaScript in a background thread with no direct access to the DOM, forms, cookies, or UI state. Moving the model into a worker prevents a compromised model from scraping sensitive data rendered in the main page or from directly modifying the UI. All interaction happens through a controlled message channel.</p><h5>Step 1: Worker script that owns the model</h5><pre><code>// File: worker.js\n\n// Load AI runtime libraries inside the worker, not in the main page context\nimportScripts('https://cdn.jsdelivr.net/npm/@tensorflow/tfjs');\n\nlet model = null;\n\nself.onmessage = async (event) => {\n  const msg = event.data;\n\n  // Basic allowlist on message types to avoid unexpected commands\n  if (!['load', 'predict'].includes(msg.type)) {\n    self.postMessage({ status: 'error', error: 'Unsupported message type' });\n    return;\n  }\n\n  try {\n    if (msg.type === 'load') {\n      model = await tf.loadLayersModel(msg.modelPath);\n      self.postMessage({ status: 'loaded' });\n    } else if (msg.type === 'predict' && model) {\n      const inputTensor = tf.tensor(msg.input);\n      const outputTensor = model.predict(inputTensor);\n      const prediction = await outputTensor.data();\n      self.postMessage({ status: 'complete', prediction });\n    }\n  } catch (err) {\n    self.postMessage({ status: 'error', error: String(err) });\n  }\n};\n</code></pre><h5>Step 2: Main thread talks to worker via postMessage</h5><pre><code>// File: main.js\n\nconst inferenceWorker = new Worker('worker.js');\n\ninferenceWorker.onmessage = (event) => {\n  const msg = event.data;\n  if (msg.status === 'loaded') {\n    console.log('Model loaded in isolated worker.');\n    inferenceWorker.postMessage({ type: 'predict', input: [1, 2, 3, 4] });\n  } else if (msg.status === 'complete') {\n    console.log('Prediction from worker:', msg.prediction);\n  } else if (msg.status === 'error') {\n    console.error('Worker error:', msg.error);\n  }\n};\n\n// Trigger model load in the isolated worker (not in window scope)\ninferenceWorker.postMessage({ type: 'load', modelPath: './model/model.json' });\n</code></pre><p><strong>Action:</strong> Always run model load + inference inside a Web Worker. Do not hand the model direct handles to DOM, cookies, storage, or other tabs. Use a minimal, validated message schema for communication via <code>postMessage</code>.</p>"
                },
                {
                    "strategy": "Run untrusted models or their UI components in a sandboxed iframe.",
                    "howTo": "<h5>Concept:</h5><p>An <code>&lt;iframe&gt;</code> with a restrictive <code>sandbox</code> attribute creates a separate browsing context with sharply reduced privileges. You can render untrusted model-driven UI (chat box, visualization, explanation panel) in that iframe. The iframe cannot read the parent DOM, cannot reach cookies in the parent origin (unless you explicitly weaken it), and can be denied network, top-level navigation, form submission, etc. This prevents a hostile or poisoned model output from executing arbitrary script in the privileged parent page.</p><h5>Example: sandboxed iframe with minimal capabilities</h5><pre><code>&lt;!-- File: index.html --&gt;\n&lt;h2&gt;Main Application Content&lt;/h2&gt;\n&lt;p&gt;This main page holds sensitive app state, tokens, etc. The untrusted AI widget will NOT run here.&lt;/p&gt;\n\n&lt;iframe\n  id=\"ai-widget-frame\"\n  src=\"/ai-widget/widget.html\"\n  sandbox=\"allow-scripts\"\n  style=\"width:400px; height:300px; border:1px solid #ccc;\"\n&gt;&lt;/iframe&gt;\n\n&lt;script&gt;\n  const iframe = document.getElementById('ai-widget-frame');\n\n  // Send non-sensitive input to the sandboxed AI widget.\n  iframe.contentWindow.postMessage({\n    type: 'inferenceRequest',\n    prompt: 'Summarize this public text:'\n  }, '*');\n\n  // Receive responses from the sandboxed widget.\n  window.addEventListener('message', (event) =&gt; {\n    // Optional: enforce origin allowlist in production\n    const data = event.data;\n    if (data &amp;&amp; data.type === 'inferenceResult') {\n      console.log('AI iframe result:', data.output);\n    }\n  });\n&lt;/script&gt;\n</code></pre><p><strong>Key rule:</strong> Do <em>not</em> directly inject model-produced HTML/JS into your privileged DOM via <code>innerHTML</code>. Instead, render that untrusted UI inside the sandboxed iframe. Allow only the minimum needed sandbox flags (e.g. <code>allow-scripts</code>) and avoid <code>allow-same-origin</code> unless you fully understand the privacy and cookie implications.</p><p><strong>Action:</strong> Treat the iframe as the only place untrusted model output is allowed to render and execute. Communicate with it only via <code>postMessage</code> using a strict, typed message contract.</p>"
                },
                {
                    "strategy": "Leverage WebAssembly (WASM) runtimes for a capabilities-based sandbox.",
                    "howTo": "<h5>Concept:</h5><p>WebAssembly executes in a sandboxed virtual machine. By default, it cannot access network, filesystem, clipboard, camera, or DOM unless you explicitly hand it those capabilities via the import object. That import boundary becomes your policy enforcement point: you can decide exactly what privileged actions (if any) the model logic can perform, log them, throttle them, or deny them entirely.</p><h5>Run a WASM module with no capabilities granted</h5><pre><code>// File: run_wasm_in_browser.js\n\nasync function runSandboxedWasm() {\n  const wasmBytes = await fetch('./model_inference.wasm').then(res =&gt; res.arrayBuffer());\n\n  // Empty importObject = no host capabilities. The model code can only do math.\n  const importObject = {};\n\n  const { instance } = await WebAssembly.instantiate(wasmBytes, importObject);\n\n  // Call an exported pure function. It cannot touch DOM, cookies, etc.\n  const result = instance.exports.run_inference(/* your args here */);\n  console.log('Inference from WASM sandbox:', result);\n}\n\nrunSandboxedWasm();\n</code></pre><p><strong>Hardening tip:</strong> If you <em>must</em> allow I/O (for example, limited network fetches), expose a tiny, audited function in <code>importObject</code> (like <code>sendRedactedTelemetry()</code>) instead of giving raw fetch or filesystem access. That small shim becomes the choke point where you can redact secrets, rate-limit, and log usage for forensics.</p><p><strong>Action:</strong> Ship the model (or sensitive model kernels) as WASM. Instantiate it with an intentionally minimal <code>importObject</code>. Treat that import boundary as your enforcement layer: nothing outside that allowlist exists to the model.</p>"
                },
                {
                    "strategy": "Utilize Content Security Policy (CSP) to restrict model data exfiltration and script execution.",
                    "howTo": "<h5>Concept:</h5><p>Content Security Policy (CSP) lets you define which network endpoints scripts in this page are allowed to talk to (<code>connect-src</code>), which scripts can run (<code>script-src</code>), and more. A strict CSP makes it much harder for a compromised in-browser model to exfiltrate sensitive data to an attacker-controlled domain, or to inject arbitrary remote scripts.</p><h5>Example CSP header</h5><pre><code>Content-Security-Policy: \n  default-src 'self'; \n  script-src 'self' https://cdn.jsdelivr.net; \n  connect-src 'self' https://api.my-trusted-domain.com; \n  img-src 'self' data:; \n  frame-ancestors 'none';\n</code></pre><p>This policy says:\n<ul>\n<li>Only load scripts from self and a known CDN.</li>\n<li>Only allow outbound fetch/WebSocket/XHR to <code>self</code> and your trusted API endpoint.</li>\n<li>Disallow being iframed elsewhere (<code>frame-ancestors 'none'</code>), which helps protect your privileged parent app from clickjacking or hostile embedding.</li>\n</ul><h5>Meta tag fallback (if you cannot set headers)</h5><pre><code>&lt;head&gt;\n  &lt;meta http-equiv=\"Content-Security-Policy\"\n        content=\"default-src 'self'; connect-src 'self' https://api.my-trusted-domain.com; script-src 'self' https://cdn.jsdelivr.net;\"&gt;\n&lt;/head&gt;\n</code></pre><p><strong>Detection tip:</strong> CSP can also emit violation reports (via <code>report-to</code> / <code>report-uri</code>). Treat repeated CSP violations from a given session as a potential sign of a compromised model trying to leak data.</p><p><strong>Action:</strong> Enforce CSP on any page that hosts a client-side model. Use <code>connect-src</code> as an allowlist of outbound egress targets. Monitor CSP violation reports to detect attempted exfiltration or unexpected script loads.</p>"
                },
                {
                    "strategy": "Enforce a minimal, allowlisted native bridge between the AI runtime and device/system capabilities (mobile, Electron, hybrid apps).",
                    "howTo": "<h5>Concept:</h5><p>On mobile and hybrid desktop apps (React Native, Capacitor, Electron, etc.), the AI model often runs in a JS or WASM sandbox but can still reach powerful native APIs through a bridge layer (filesystem, microphone, corporate tokens, VPN configs, clipboard). A malicious or hijacked model could try to call those bridge APIs to exfiltrate data or escalate privileges. The defense is to force all privileged actions through a single, allowlisted gateway module and refuse anything else by default.</p><h5>Example: restricted bridge in an Electron-style preload</h5><pre><code>// File: preload.js (runs in isolated context)\nconst { contextBridge, ipcRenderer } = require('electron');\n\n// Only expose a very small, audited surface to the AI runtime.\ncontextBridge.exposeInMainWorld('secureBridge', {\n  getRedactedScreenshot: async () => {\n    // Ask main process for a redacted screenshot instead of raw screen pixels.\n    return ipcRenderer.invoke('get-redacted-screenshot');\n  },\n  sendTelemetry: async (msg) => {\n    // Allowlisted telemetry path, can be rate-limited and logged in main.\n    if (typeof msg !== 'string' || msg.length > 2000) {\n      throw new Error('Telemetry message rejected');\n    }\n    return ipcRenderer.invoke('send-telemetry', msg);\n  }\n});\n</code></pre><h5>Usage in untrusted AI code</h5><pre><code>// File: ai_runtime.js (untrusted / model-controlled layer)\n\nasync function tryExfiltrateSensitiveData() {\n  // The model CANNOT just read files or system secrets directly.\n  // It only sees window.secureBridge, which is minimal and audited.\n  const screenshot = await window.secureBridge.getRedactedScreenshot();\n  await window.secureBridge.sendTelemetry('[summary only]\\n' + screenshot.summary);\n}\n</code></pre><p><strong>Action:</strong> In mobile / Electron / hybrid apps, do NOT let the AI runtime call arbitrary native APIs. Instead, expose a narrowly scoped, audited bridge object with an allowlist of safe functions. Enforce input validation, redaction, and rate limiting at that bridge boundary. Treat anything not on the allowlist as denied by default (fail-closed).</p>"
                }
            ]
        }
    ]
};