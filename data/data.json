{
  "version": {
    "schemaVersion": "2.0",
    "dataVersion": "2026.02.03",
    "generatedAt": "2026-02-03T05:38:58.726Z",
    "source": "bundled",
    "keywordStructure": "categorized"
  },
  "tactics": [
    {
      "id": "model",
      "name": "Model",
      "description": "The \"Model\" tactic, in the context of AI security, focuses on developing a comprehensive understanding and detailed mapping of all AI/ML assets, their configurations, data flows, operational behaviors, and interdependencies. This foundational knowledge is crucial for informing and enabling all subsequent defensive actions. It involves knowing precisely what AI systems exist within the organization, how they are architected, what data they ingest and produce, their critical dependencies (both internal and external), and their expected operational parameters and potential emergent behaviors.",
      "techniques": [
        {
          "id": "AID-M-001",
          "name": "AI Asset Inventory & Mapping",
          "description": "Systematically catalog and map all AI/ML assets, including models (categorized by type, version, deployment location, and ownership), datasets (training, validation, testing, and operational), data pipelines, and APIs. This process includes mapping their configurations, data flows (sources, transformations, destinations), and interdependencies (e.g., reliance on third-party APIs, upstream data providers, or specific libraries). The goal is to achieve comprehensive visibility into all components that constitute the AI ecosystem and require protection. This technique is foundational as it underpins the ability to apply targeted security controls and assess risk accurately.",
          "pillar": "infra",
          "phase": "scoping",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0007 Discover AI Artifacts",
                "AML.T0002 Acquire Public ML Artifacts",
                "AML.T0035 AI Artifact Collection"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Foundational for assessing risks across all layers",
                "Agent Ecosystem (L7)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "Indirectly LLM03:2025 Supply Chain"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "Indirectly ML06:2023 AI Supply Chain Attacks"
              ]
            }
          ],
          "implementationStrategies": [],
          "toolsOpenSource": [],
          "toolsCommercial": [],
          "subTechniques": [
            {
              "id": "AID-M-001.001",
              "name": "AI Component & Infrastructure Inventory",
              "description": "Systematically catalogs all AI/ML assets, including models (categorized by type, version, and ownership), datasets, software components, and the specialized hardware they run on (e.g., GPUs, TPUs). This technique focuses on creating a dynamic, up-to-date inventory to provide comprehensive visibility into all components that constitute the AI ecosystem, which is a prerequisite for accurate risk assessment and the application of targeted security controls.",
              "pillar": "infra",
              "phase": "scoping",
              "implementationStrategies": [
                "Establish and maintain a dynamic, up-to-date inventory of all AI models, datasets, software components, and associated infrastructure.",
                "Include specialized AI accelerators (GPUs, TPUs, NPUs, FPGAs) and their firmware versions in the AI asset inventory.",
                "Assign clear ownership and accountability for each inventoried AI asset.",
                "Integrate AI asset inventory with broader IT asset management and configuration management databases (CMDBs)."
              ],
              "toolsOpenSource": [
                "MLflow, Kubeflow (for model/experiment inventory)",
                "DVC (for dataset inventory)",
                "Great Expectations (for data asset profiling)",
                "Cloud provider CLIs (AWS CLI, gcloud, Azure CLI)",
                "General IT asset management tools (Snipe-IT)"
              ],
              "toolsCommercial": [
                "AI Security Posture Management (AI-SPM) platforms (Wiz AI-SPM, Microsoft Defender for Cloud, Prisma Cloud)",
                "MLOps platforms (Amazon SageMaker Model Registry, Google Vertex AI Model Registry, Databricks Unity Catalog)",
                "Data catalog and governance platforms (Alation, Collibra)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0007 Discover AI Artifacts"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Foundational for securing L4: Deployment & Infrastructure",
                    "Agent Supply Chain (L7) understanding"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM03:2025 Supply Chain"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML06:2023 ML Supply Chain Attacks"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "unauthorized access",
                  "asset discovery",
                  "shadow ai",
                  "assets",
                  "risk",
                  "reconnaissance",
                  "misconfiguration",
                  "exposure"
                ],
                "defense": [
                  "risk assessment",
                  "asset inventory",
                  "asset management",
                  "configuration management",
                  "model registry",
                  "component",
                  "infrastructure",
                  "inventory",
                  "visibility",
                  "assessment",
                  "accountability",
                  "inventoried"
                ]
              }
            },
            {
              "id": "AID-M-001.002",
              "name": "AI System Dependency Mapping",
              "description": "Systematically identifies and documents all components and services that an AI system depends on to function correctly. This includes direct software libraries, transitive dependencies, external data sources, third-party APIs, and other internal AI models or microservices. This dependency map is crucial for understanding the complete supply chain attack surface and for performing comprehensive security assessments.",
              "pillar": "infra",
              "phase": "building",
              "implementationStrategies": [
                "Pin and document all software library dependencies to exact versions.",
                "Document all external service and third-party API dependencies in a configuration manifest.",
                "Generate and maintain a Software Bill of Materials (SBOM) for every AI application build (i.e. AIBOM).",
                "Visualize the full dependency graph to understand complex relationships."
              ],
              "toolsOpenSource": [
                "pip-tools, pip-audit (for Python dependencies)",
                "Syft, Grype, Trivy (for SBOM generation and SCA)",
                "OWASP Dependency-Check",
                "pipdeptree (for dependency visualization)"
              ],
              "toolsCommercial": [
                "Snyk, Mend (formerly WhiteSource), JFrog Xray (for SCA and SBOM management)",
                "API Security platforms (Noname Security, Salt Security) for API discovery",
                "Data Governance platforms (Alation, Collibra)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0010.001 AI Supply Chain Compromise: AI Software",
                    "AML.T0011.001 User Execution: Malicious Package"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Supply Chain Attacks (Cross-Layer)",
                    "Compromised Framework Components (L3)",
                    "Agent Supply Chain (L7)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM03:2025 Supply Chain"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML06:2023 AI Supply Chain Attacks"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "supply chain attack",
                  "lateral movement",
                  "attack surface",
                  "dependency confusion",
                  "vulnerable dependency",
                  "attack",
                  "reconnaissance",
                  "exposure",
                  "compromised"
                ],
                "defense": [
                  "dependency mapping",
                  "system",
                  "dependency",
                  "mapping",
                  "document",
                  "sbom",
                  "inventory",
                  "visibility",
                  "documentation",
                  "baseline",
                  "catalog",
                  "discovery",
                  "assessment",
                  "governance"
                ]
              }
            }
          ],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "data pipeline",
              "unauthorized access",
              "asset discovery",
              "shadow ai",
              "lateral movement",
              "attack surface",
              "malicious input",
              "invalid input",
              "assets",
              "risk",
              "reconnaissance",
              "exposure",
              "injection",
              "bypass",
              "exploit"
            ],
            "defense": [
              "asset inventory",
              "asset",
              "inventory",
              "mapping",
              "catalog",
              "validation",
              "visibility",
              "protection",
              "assess",
              "documentation",
              "baseline",
              "discovery",
              "assessment",
              "governance"
            ]
          }
        },
        {
          "id": "AID-M-002",
          "name": "Data Provenance & Lineage Tracking",
          "description": "Establish and maintain verifiable records of the origin, history, and transformations of data used in AI systems, particularly training and fine-tuning data. This includes tracking model updates and their associated data versions. The objective is to ensure the trustworthiness and integrity of data and models by knowing their complete lifecycle, from source to deployment, and to facilitate auditing and incident investigation. This often involves cryptographic methods like signing or checksumming datasets and subunits and models at critical stages.",
          "pillar": "data",
          "phase": "building",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0020 Poison Training Data",
                "AML.T0010 AI Supply Chain Compromise",
                "AML.T0010.002 AI Supply Chain Compromise: Data",
                "AML.T0018 Manipulate AI Model",
                "AML.T0019 Publish Poisoned Datasets",
                "AML.T0059 Erode Dataset Integrity"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Data Poisoning (L2)",
                "Compromised RAG Pipelines (L2)",
                "Model Skewing (L2)",
                "Supply Chain Attacks (Cross-Layer)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM04:2025 Data and Model Poisoning",
                "LLM03:2025 Supply Chain"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML02:2023 Data Poisoning Attack",
                "ML10:2023 Model Poisoning",
                "ML07:2023 Transfer Learning Attack",
                "ML06:2023 ML Supply Chain Attacks"
              ]
            }
          ],
          "implementationStrategies": [],
          "toolsOpenSource": [],
          "toolsCommercial": [],
          "subTechniques": [
            {
              "id": "AID-M-002.001",
              "name": "Data & Artifact Versioning",
              "description": "Implements systems and processes to version control datasets and model artifacts, treating them with the same rigor as source code. By tracking every version of a data file and linking it to specific code commits, this technique ensures perfect reproducibility, provides an auditable history of changes, and enables rapid rollbacks to a known-good state, which is critical for recovering from data corruption or poisoning incidents.",
              "pillar": "data",
              "phase": "building",
              "implementationStrategies": [
                "Use a dedicated data version control system to track large files alongside Git.",
                "Define the data processing pipeline in a version-controlled manifest to map data flow.",
                "Link specific data versions to training runs in an MLOps platform."
              ],
              "toolsOpenSource": [
                "DVC (Data Version Control)",
                "Git-LFS",
                "LakeFS",
                "Pachyderm",
                "MLflow"
              ],
              "toolsCommercial": [
                "Databricks (with Delta Lake Time Travel)",
                "Amazon S3 Object Versioning",
                "Azure Blob Storage versioning"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0059 Erode Dataset Integrity"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Data Tampering (L2)",
                    "Supply Chain Attacks (Cross-Layer)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM03:2025 Supply Chain"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML06:2023 AI Supply Chain Attacks"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "rollback attack",
                  "unauthorized modification",
                  "forensic evasion",
                  "malicious update",
                  "artifact",
                  "artifacts",
                  "state",
                  "corruption",
                  "poisoning",
                  "tampering",
                  "repudiation",
                  "unauthorized"
                ],
                "defense": [
                  "version control",
                  "data",
                  "artifact",
                  "versioning",
                  "control",
                  "tracking",
                  "recovering",
                  "track",
                  "inventory",
                  "visibility",
                  "mapping",
                  "documentation",
                  "baseline",
                  "catalog",
                  "discovery"
                ]
              }
            },
            {
              "id": "AID-M-002.002",
              "name": "Cryptographic Integrity Verification",
              "description": "Employs cryptographic hashing and digital signatures to create and verify a tamper-evident chain of custody for macro-scale AI artifacts throughout their lifecycle. Focuses on whole-artifact integrity for datasets, model weights, container images, and manifests to ensure you deploy exactly what you built. This technique provides artifact lifecycle integrity from creation through storage to deployment, with provenance verification to prove authenticity and origin. For fine-grained chunk-level integrity in RAG pipelines, see AID-H-021.001.",
              "pillar": "data",
              "phase": "building",
              "implementationStrategies": [
                "Generate and verify whole-artifact checksums (e.g., SHA-256) for datasets, models, and container images at critical pipeline stages.",
                "Digitally sign critical artifacts to prove authenticity and origin."
              ],
              "toolsOpenSource": [
                "sha256sum (Linux utility)",
                "GnuPG (GPG)",
                "Sigstore / Cosign",
                "pyca/cryptography (Python library)",
                "MLflow (for storing hashes/signatures as tags)"
              ],
              "toolsCommercial": [
                "Cloud Provider KMS (AWS KMS, Azure Key Vault, Google Cloud KMS) for signing operations",
                "Code Signing services (DigiCert, GlobalSign)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0010.002 AI Supply Chain Compromise: Data",
                    "AML.T0010.003 AI Supply Chain Compromise: Model",
                    "AML.T0076 Corrupt AI Model"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Data Tampering (L2)",
                    "Model Tampering (L1)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM03:2025 Supply Chain"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML06:2023 AI Supply Chain Attacks"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "model weights",
                  "rag pipeline",
                  "data poisoning",
                  "supply chain attack",
                  "untrusted data",
                  "unauthorized modification",
                  "man in the middle",
                  "artifacts",
                  "weights",
                  "artifact",
                  "tampering",
                  "corruption",
                  "manipulation",
                  "forgery",
                  "interception"
                ],
                "defense": [
                  "digital signature",
                  "integrity verification",
                  "cryptographic",
                  "integrity",
                  "verification",
                  "hashing",
                  "verify",
                  "container",
                  "provenance",
                  "sign"
                ]
              }
            },
            {
              "id": "AID-M-002.003",
              "name": "Third-Party Data Vetting",
              "description": "Implements a formal, security-focused process for onboarding any external or third-party datasets. This technique involves a combination of procedural checks (source reputation, licensing) and technical scans (PII detection, integrity verification, statistical profiling) to identify and mitigate risks before untrusted data is introduced into the organization's AI ecosystem.",
              "pillar": "data",
              "phase": "scoping",
              "implementationStrategies": [
                "Establish a formal checklist and review process for onboarding all external datasets.",
                "Automatically scan all incoming datasets for Personally Identifiable Information (PII) and other sensitive secrets.",
                "Profile all new datasets to check for statistical anomalies or unexpected distributions before use."
              ],
              "toolsOpenSource": [
                "Microsoft Presidio",
                "TruffleHog",
                "ydata-profiling (formerly Pandas Profiling)",
                "Great Expectations",
                "DVC"
              ],
              "toolsCommercial": [
                "Google Cloud Data Loss Prevention (DLP) API",
                "Amazon Macie",
                "Azure Purview",
                "Data governance platforms (Alation, Collibra)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0010.002 AI Supply Chain Compromise: Data",
                    "AML.T0019 Publish Poisoned Datasets"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Data Poisoning (L2)",
                    "Supply Chain Attacks (Cross-Layer)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM03:2025 Supply Chain (Untrusted Datasets)",
                    "LLM04:2025 Data and Model Poisoning"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML02:2023 Data Poisoning Attack",
                    "ML06:2023 AI Supply Chain Attacks"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "untrusted data",
                  "unauthorized modification",
                  "supply chain attack",
                  "risks",
                  "untrusted",
                  "secrets",
                  "tampering",
                  "corruption",
                  "manipulation",
                  "forgery",
                  "malicious",
                  "compromised",
                  "backdoor",
                  "evasion",
                  "bypass"
                ],
                "defense": [
                  "integrity verification",
                  "review process",
                  "third-party",
                  "data",
                  "vetting",
                  "detection",
                  "integrity",
                  "verification",
                  "mitigate",
                  "scan",
                  "secrets"
                ]
              }
            },
            {
              "id": "AID-M-002.004",
              "name": "Trust-Tiered Memory/KB (Knowledge Base) Write-Gate",
              "description": "Place a policy-enforced write-gate in front of agent memory/KB/vector stores. Route writes into trust-tiered namespaces (trusted, probation, quarantined) based on evidence presence, validator score, and policy decisions. Retrieval prefers trusted; probation requires re-verification; quarantined is excluded.",
              "pillar": "data",
              "phase": "building",
              "implementationStrategies": [
                "FastAPI write-gate + Pinecone namespaces + OPA routing"
              ],
              "toolsOpenSource": [
                "SPIFFE/SPIRE (workload identity)",
                "Envoy/Nginx (write-gate proxy)",
                "OPA/Kyverno (write policy)",
                "Milvus/Weaviate (namespaces/collections)",
                "Sigstore/cosign (signing/verification)"
              ],
              "toolsCommercial": [
                "Pinecone (separate indexes/namespaces)",
                "Databricks Unity Catalog (data lineage/access)",
                "JFrog Artifactory/XRay (artifact policy concepts for KB artifacts)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0010 AI Supply Chain Compromise (Data)",
                    "AML.T0020 Poison Training Data"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Data Poisoning (Training Phase) (L1)",
                    "Data Poisoning (L2)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM01:2025 Prompt Injection (via poisoned context)",
                    "LLM02:2025 Sensitive Information Disclosure (write-path controls)"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML02:2023 Data Poisoning",
                    "ML09:2023 Output Integrity Attack"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "vector store",
                  "knowledge base",
                  "shadow ai",
                  "unauthorized access",
                  "memory",
                  "vector",
                  "reconnaissance",
                  "misconfiguration",
                  "exposure"
                ],
                "defense": [
                  "trust-tiered",
                  "memory",
                  "knowledge",
                  "base",
                  "write-gate",
                  "trusted",
                  "quarantined",
                  "policy",
                  "inventory",
                  "visibility",
                  "mapping",
                  "documentation",
                  "baseline",
                  "catalog",
                  "discovery"
                ]
              }
            }
          ],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "data poisoning",
              "supply chain attack",
              "untrusted data",
              "unauthorized modification",
              "man in the middle",
              "forensic evasion",
              "tampering",
              "manipulation",
              "corruption",
              "forgery",
              "interception",
              "repudiation",
              "unauthorized"
            ],
            "defense": [
              "lineage tracking",
              "data",
              "provenance",
              "lineage",
              "tracking",
              "integrity",
              "auditing",
              "cryptographic",
              "signing",
              "inventory",
              "visibility",
              "mapping",
              "documentation",
              "baseline",
              "catalog"
            ]
          }
        },
        {
          "id": "AID-M-003",
          "name": "Model Behavior Baseline & Documentation",
          "description": "Establish, document, and maintain a comprehensive baseline of expected AI model behavior. This includes defining its intended purpose, architectural details, training data characteristics, operational assumptions, limitations, and key performance metrics (e.g., accuracy, precision, recall, output distributions, latency, confidence scores) under normal conditions. This documentation, often in the form of model cards, and the established behavioral baseline serve as a reference to detect anomalies, drift, or unexpected outputs that might indicate an attack or system degradation, and to inform risk assessments and incident response.",
          "pillar": "model",
          "phase": "building",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0063 Discover AI Model Outputs",
                "AML.T0013 Discover AI Model Ontology",
                "AML.T0014 Discover AI Model Family",
                "AML.T0015 Evade AI Model",
                "AML.T0054 LLM Jailbreak",
                "AML.T0031 Erode AI Model Integrity"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Evasion of Security AI Agents (L6)",
                "Unpredictable agent behavior / Performance Degradation (L5)",
                "Inaccurate Agent Capability Description (L7)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM01:2025 Prompt Injection (jailbreaking aspect)",
                "LLM09:2025 Misinformation"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML01:2023 Input Manipulation Attack",
                "ML08:2023 Model Skewing"
              ]
            }
          ],
          "implementationStrategies": [],
          "toolsOpenSource": [],
          "toolsCommercial": [],
          "subTechniques": [
            {
              "id": "AID-M-003.001",
              "name": "Model Card & Datasheet Generation",
              "description": "A systematic process of creating and maintaining standardized documentation for AI models (Model Cards) and datasets (Datasheets). This documentation captures crucial metadata, including the model's intended use cases, limitations, performance metrics, fairness evaluations, ethical considerations, and details about the data's provenance and characteristics. This ensures transparency, enables responsible governance, and provides a foundational reference for security audits and risk assessments.",
              "pillar": "model",
              "phase": "building",
              "implementationStrategies": [
                "Use a standardized toolkit to programmatically generate model cards.",
                "Create and maintain 'Datasheets for Datasets' to document data provenance, composition, and collection processes.",
                "Integrate documentation generation and validation into a CI/CD pipeline.",
                "Store and version control documentation in a centralized, accessible repository or model registry."
              ],
              "toolsOpenSource": [
                "Google's Model Card Toolkit",
                "Hugging Face Hub (for hosting models with cards)",
                "DVC (Data Version Control)",
                "MLflow, Kubeflow (for artifact logging)",
                "Sphinx, MkDocs (for building documentation sites)"
              ],
              "toolsCommercial": [
                "Google Vertex AI Model Registry",
                "Amazon SageMaker Model Registry",
                "Databricks Unity Catalog",
                "AI Governance Platforms (IBM Watson OpenScale, Fiddler AI, Arize AI)",
                "Data Cataloging Platforms (Alation, Collibra)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0013 Discover AI Model Ontology",
                    "AML.T0014 Discover AI Model Family",
                    "AML.T0063 Discover AI Model Outputs"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Inaccurate Agent Capability Description (L7)",
                    "Foundational for assessing risks across all layers"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM03:2025 Supply Chain",
                    "LLM09:2025 Misinformation"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML06:2023 AI Supply Chain Attacks",
                    "ML08:2023 Model Skewing"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "data poisoning",
                  "supply chain attack",
                  "untrusted data",
                  "compliance violation",
                  "malicious input",
                  "invalid input",
                  "apt",
                  "risk",
                  "tampering",
                  "misconfiguration",
                  "misuse",
                  "unauthorized",
                  "injection",
                  "bypass",
                  "exploit"
                ],
                "defense": [
                  "risk assessment",
                  "security audit",
                  "model registry",
                  "version control",
                  "model",
                  "card",
                  "datasheet",
                  "generation",
                  "documentation",
                  "provenance",
                  "governance",
                  "document",
                  "validation",
                  "control",
                  "repository"
                ]
              }
            },
            {
              "id": "AID-M-003.002",
              "name": "Performance & Operational Metric Baselining",
              "description": "Establishes a quantitative, empirical baseline of a model's expected behavior under normal conditions. This involves calculating and recording two types of metrics: 1) key performance indicators (e.g., accuracy, precision, F1-score) on a trusted, 'golden' dataset, and 2) operational metrics (e.g., inference latency, confidence scores, output distributions) derived from simulated or live traffic. This documented baseline serves as the ground truth for drift detection, anomaly detection, and ongoing performance monitoring.",
              "pillar": "model",
              "phase": "validation",
              "implementationStrategies": [
                "Calculate and store key performance metrics on a trusted validation dataset.",
                "Establish baselines for operational metrics like latency and throughput via load testing.",
                "Baseline the model's output distribution on normal data.",
                "Link performance and operational baselines to specific model versions in a central model registry."
              ],
              "toolsOpenSource": [
                "scikit-learn (for performance metrics)",
                "MLflow, DVC (for versioning baselines with models)",
                "Evidently AI, NannyML, Alibi Detect (for drift detection using baselines)",
                "Locust, k6, Apache JMeter (for load testing and operational baselining)",
                "Prometheus, Grafana (for storing and visualizing time-series metrics)"
              ],
              "toolsCommercial": [
                "AI Observability Platforms (Arize AI, Fiddler, WhyLabs)",
                "Cloud Provider Monitoring (Amazon SageMaker Model Monitor, Google Vertex AI Model Monitoring, Azure Model Monitor)",
                "Application Performance Monitoring (APM) tools (Datadog, New Relic)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0031 Erode AI Model Integrity",
                    "AML.T0015 Evade AI Model",
                    "AML.T0063 Discover AI Model Outputs"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Unpredictable agent behavior / Performance Degradation (L5)",
                    "Model Skewing (L2)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM01:2025 Prompt Injection (detecting anomalous outputs)",
                    "LLM09:2025 Misinformation (detecting distributional drift)"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML08:2023 Model Skewing",
                    "ML01:2023 Input Manipulation Attack",
                    "ML02:2023 Data Poisoning Attack"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "anomalous behavior",
                  "malicious input",
                  "invalid input",
                  "stealth attack",
                  "persistent threat",
                  "key",
                  "drift",
                  "deviation",
                  "degradation",
                  "manipulation",
                  "injection",
                  "bypass",
                  "exploit",
                  "evasion",
                  "undetected"
                ],
                "defense": [
                  "anomaly detection",
                  "performance monitoring",
                  "drift detection",
                  "model registry",
                  "performance",
                  "operational",
                  "metric",
                  "baselining",
                  "baseline",
                  "key",
                  "trusted",
                  "documented",
                  "detection",
                  "monitoring",
                  "validation"
                ]
              }
            },
            {
              "id": "AID-M-003.003",
              "name": "Explainability (XAI) Output Baselining",
              "description": "Establishes a baseline of normal or expected outputs from eXplainable AI (XAI) methods for a given AI model. By generating and documenting typical explanations (e.g., feature attributions, decision rules) for a diverse set of known, benign inputs, this technique creates a reference point to detect future anomalies. A significant deviation from this baseline can indicate that an attacker is attempting to manipulate or mislead the explanation method itself to conceal malicious activity, as investigated by AID-D-006.",
              "pillar": "model",
              "phase": "validation",
              "implementationStrategies": [
                "Generate and store baseline feature attributions for different prediction classes.",
                "Create qualitative documentation of expected explanatory behavior in model cards.",
                "Baseline the stability of explanations under minor input perturbations.",
                "Version control XAI baselines and link them to specific model versions in a registry."
              ],
              "toolsOpenSource": [
                "SHAP, LIME, Captum, Alibi Explain, InterpretML (XAI libraries)",
                "scikit-learn, PyTorch, TensorFlow (for model interaction)",
                "MLflow, DVC (for versioning and storing baselines)",
                "Google's Model Card Toolkit, MkDocs (for documentation)"
              ],
              "toolsCommercial": [
                "AI Observability Platforms (Fiddler, Arize AI, WhyLabs)",
                "Cloud Provider XAI tools (Google Vertex AI Explainable AI, Amazon SageMaker Clarify, Azure Machine Learning Interpretability)",
                "AI Governance Platforms (IBM Watson OpenScale)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.TA0007 Defense Evasion"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Evasion of Auditing/Compliance (L6)",
                    "Manipulation of Evaluation Metrics (L5)",
                    "Lack of Explainability in Security AI Agents (L6)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "Supports investigation of LLM01: Prompt Injection"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "Supports investigation of ML08: Model Skewing and ML10: Model Poisoning"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "anomalous behavior",
                  "compliance violation",
                  "bias injection",
                  "hidden behavior",
                  "attacker",
                  "manipulate",
                  "malicious",
                  "drift",
                  "deviation",
                  "degradation",
                  "manipulation",
                  "misconfiguration",
                  "misuse",
                  "unauthorized",
                  "deception"
                ],
                "defense": [
                  "version control",
                  "explainability",
                  "xai",
                  "output",
                  "baselining",
                  "baseline",
                  "documenting",
                  "rules",
                  "detect",
                  "documentation",
                  "control",
                  "registry"
                ]
              }
            },
            {
              "id": "AID-M-003.004",
              "name": "Agent Goal & Mission Baselining",
              "description": "Specifically for autonomous or agentic AI, this technique involves formally defining, documenting, and cryptographically signing the agent's core mission, objectives, operational constraints, and goal hierarchy. This signed 'mission directive' serves as a trusted, immutable baseline. It is a critical prerequisite for runtime monitoring systems (like AID-D-010) to detect goal manipulation, unauthorized deviations, or emergent behaviors that contradict the agent's intended purpose.",
              "pillar": "app",
              "phase": "scoping",
              "implementationStrategies": [
                "Define the agent's mission, goals, and constraints in a structured, machine-readable format.",
                "Cryptographically sign the goal document to create a tamper-evident, verifiable baseline.",
                "Embed mission metadata into the model/agent card so reviewers see the intended purpose.",
                "Implement a secure mechanism for the agent and monitoring systems to fetch and verify the signed goal at runtime."
              ],
              "toolsOpenSource": [
                "GnuPG (GPG), pyca/cryptography (for signing and verification)",
                "HashiCorp Vault (can act as a signing authority)",
                "Agentic frameworks (LangChain, AutoGen, CrewAI)",
                "Documentation generators (MkDocs, Sphinx)"
              ],
              "toolsCommercial": [
                "Cloud Provider KMS (AWS KMS, Azure Key Vault, Google Cloud KMS)",
                "Code Signing Services (DigiCert, GlobalSign)",
                "AI Safety & Governance Platforms (Lasso Security, Protect AI Guardian, CalypsoAI Validator)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0051 LLM Prompt Injection",
                    "AML.T0018 Manipulate AI Model"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Agent Goal Manipulation (L7)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM06:2025 Excessive Agency",
                    "LLM01:2025 Prompt Injection"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML08:2023 Model Skewing (by detecting deviation from intended purpose)"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "man in the middle",
                  "anomalous behavior",
                  "stealth attack",
                  "persistent threat",
                  "manipulation",
                  "unauthorized",
                  "tampering",
                  "forgery",
                  "interception",
                  "drift",
                  "deviation",
                  "degradation",
                  "evasion",
                  "undetected"
                ],
                "defense": [
                  "agent",
                  "goal",
                  "mission",
                  "baselining",
                  "documenting",
                  "signing",
                  "constraints",
                  "signed",
                  "trusted",
                  "baseline",
                  "monitoring",
                  "detect",
                  "sign",
                  "document",
                  "secure"
                ]
              }
            },
            {
              "id": "AID-M-003.005",
              "name": "Generative Model Inversion for Anomaly Pre-screening",
              "description": "Utilizes a generative model (e.g., a Generative Adversarial Network - GAN) to establish a baseline of 'normal' data characteristics. An input, such as an image, is projected into the model's latent space to find a vector that best reconstructs the input. A high reconstruction error suggests the input is anomalous, out-of-distribution, or potentially a synthetic deepfake not created by a similar generative process. This technique models the expected data fidelity to pre-screen inputs for potential threats.",
              "pillar": "model",
              "phase": "validation",
              "implementationStrategies": [
                "Establish a reconstruction error baseline using a trusted, clean dataset.",
                "Implement a real-time anomaly detection check at the API ingress based on the error baseline.",
                "Utilize latent space clustering to identify anomalous groups of inputs.",
                "Periodically retrain the inversion model and update baselines to adapt to data drift."
              ],
              "toolsOpenSource": [
                "PyTorch, TensorFlow, Keras (for building GANs and inversion models)",
                "OpenCV, Pillow (for image processing and calculating reconstruction error)",
                "scikit-learn (for clustering algorithms like DBSCAN)",
                "Public research repositories on GitHub for specific GAN inversion algorithms",
                "MLOps workflow orchestrators (Kubeflow Pipelines, Airflow)"
              ],
              "toolsCommercial": [
                "AI security platforms with deepfake detection capabilities (Sensity, Hive AI, Clarifai)",
                "Cloud-based computer vision services (Amazon Rekognition, Google Cloud Vision AI, Azure Cognitive Services)",
                "AI observability platforms that monitor for data drift and anomalies (Arize AI, Fiddler, WhyLabs)",
                "Protect AI, HiddenLayer (platforms for model security)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0043 Craft Adversarial Data",
                    "AML.T0048.002 External Harms: Societal Harm"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Adversarial Examples (L1)",
                    "Data Poisoning (L2)",
                    "Input Validation Attacks (L3)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM01:2025 Prompt Injection (in a multimodal context, by pre-screening images)",
                    "LLM09:2025 Misinformation (by identifying synthetic/fake images)"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML01:2023 Input Manipulation Attack",
                    "ML09:2023 Output Integrity Attack"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "model inversion",
                  "anomalous behavior",
                  "apt",
                  "adversarial",
                  "vector",
                  "anomalous",
                  "drift",
                  "deviation",
                  "degradation",
                  "manipulation",
                  "evasion",
                  "bypass",
                  "stealth",
                  "obfuscation",
                  "hiding"
                ],
                "defense": [
                  "anomaly detection",
                  "generative",
                  "model",
                  "inversion",
                  "anomaly",
                  "pre-screening",
                  "baseline",
                  "trusted",
                  "detection",
                  "update"
                ]
              }
            },
            {
              "id": "AID-M-003.006",
              "name": "Graph Energy Analysis for GNN Robustness",
              "description": "Utilizes metrics derived from a graph's adjacency matrix, such as graph subspace energy, as a quantifiable indicator of a Graph Neural Network's (GNN) robustness to adversarial topology perturbations. By modeling and baselining these structural properties, this technique can guide the development of more inherently resilient GNNs, for instance, by enhancing adversarial training to generate perturbations that are not only effective but also structurally significant according to the energy metric.",
              "pillar": "model",
              "phase": "validation",
              "implementationStrategies": [
                "Compute graph energy metrics as a baseline to quantify structural robustness.",
                "Correlate graph energy metrics with model performance under attack to validate the metric's utility.",
                "Use the graph energy metric as a regularization term during adversarial training to generate more challenging perturbations.",
                "Monitor the graph energy of dynamic graphs over time to detect significant structural changes or potential coordinated attacks."
              ],
              "toolsOpenSource": [
                "PyTorch Geometric, Deep Graph Library (DGL) (for GNN implementation)",
                "NetworkX (for graph creation and manipulation)",
                "NumPy, SciPy (for linear algebra operations, e.g., eigenvalue computation)",
                "MLflow (for experiment tracking and model baselining)"
              ],
              "toolsCommercial": [
                "Graph databases with analytics features (Neo4j, TigerGraph)",
                "ML platforms supporting GNNs (Amazon SageMaker, Google Vertex AI)",
                "AI Observability platforms (Arize AI, Fiddler, WhyLabs) if extended to graph metrics"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0020 Poison Training Data",
                    "AML.T0043 Craft Adversarial Data"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Data Poisoning (L2)",
                    "Data Tampering (L2)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "Not directly applicable"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML01:2023 Input Manipulation Attack",
                    "ML02:2023 Data Poisoning Attack"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "coordinated attack",
                  "anomalous behavior",
                  "adversarial",
                  "attack",
                  "drift",
                  "deviation",
                  "degradation",
                  "manipulation"
                ],
                "defense": [
                  "adversarial training",
                  "graph",
                  "energy",
                  "analysis",
                  "gnn",
                  "robustness",
                  "baselining",
                  "resilient",
                  "baseline",
                  "validate",
                  "monitor",
                  "detect"
                ]
              }
            },
            {
              "id": "AID-M-003.007",
              "name": "GNN Structural Baselining & Discrepancy Profiling",
              "description": "Employs self-supervised learning during the validation phase to generate baseline artifacts for Graph Neural Network (GNN) backdoor defense. Trains an auxiliary GNN model that learns intrinsic semantic information and attribute importance of nodes without using potentially poisoned labels, producing clean embedding distributions, drift profiles, and discrepancy statistics. These baseline artifacts are persisted for use by downstream detection techniques (see AID-D-012.001). This technique does not perform alerting; it generates and stores the trusted reference state. Outputs: baselines/clean_node_embeddings.npy, baselines/node_semantic_drift.npy, baselines/primary_embeddings.npy (NumPy arrays).",
              "pillar": "model",
              "phase": "validation",
              "implementationStrategies": [
                "Train an auxiliary GNN model using a self-supervised task to learn clean node representations.",
                "Extract clean baseline embeddings and attribute importance from the auxiliary model.",
                "Train the primary (potentially compromised) model using standard supervised learning.",
                "Compute and persist discrepancy metrics between the primary and auxiliary models as baseline artifacts."
              ],
              "toolsOpenSource": [
                "PyTorch Geometric, Deep Graph Library (DGL) (for GNN implementation)",
                "NetworkX (for graph analysis and manipulation)",
                "NumPy, scikit-learn (for vector operations and clustering)",
                "XAI libraries for GNNs (GNNExplainer, Captum) for calculating attribute importance"
              ],
              "toolsCommercial": [
                "ML platforms supporting GNNs (Amazon SageMaker, Google Vertex AI, Azure Machine Learning)",
                "Graph database platforms (Neo4j, TigerGraph, Memgraph)",
                "AI Observability and Security platforms (Arize AI, Fiddler, Protect AI)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0018 Manipulate AI Model",
                    "AML.T0020 Poison Training Data"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Backdoor Attacks (L1)",
                    "Data Poisoning (L2)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "Not directly applicable"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML02:2023 Data Poisoning Attack",
                    "ML10:2023 Model Poisoning"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "anomalous behavior",
                  "artifacts",
                  "backdoor",
                  "poisoned",
                  "embedding",
                  "state",
                  "embeddings",
                  "representations",
                  "extract",
                  "compromised",
                  "drift",
                  "deviation",
                  "degradation",
                  "manipulation",
                  "injection"
                ],
                "defense": [
                  "gnn",
                  "structural",
                  "baselining",
                  "discrepancy",
                  "profiling",
                  "validation",
                  "baseline",
                  "defense",
                  "detection",
                  "alerting",
                  "trusted"
                ]
              }
            }
          ],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "anomalous behavior",
              "compliance violation",
              "key",
              "attack",
              "degradation",
              "risk",
              "drift",
              "deviation",
              "manipulation",
              "misconfiguration",
              "misuse",
              "unauthorized"
            ],
            "defense": [
              "incident response",
              "risk assessment",
              "model",
              "behavior",
              "baseline",
              "documentation",
              "document",
              "key",
              "detect",
              "inventory",
              "visibility",
              "mapping",
              "catalog",
              "discovery",
              "assessment"
            ]
          }
        },
        {
          "id": "AID-M-004",
          "name": "AI Threat Modeling & Risk Assessment",
          "description": "Systematically identify, analyze, and prioritize potential AI-specific threats and vulnerabilities for each AI component (e.g., data, models, algorithms, pipelines, agentic capabilities, APIs) throughout its lifecycle. This process involves understanding how an adversary might attack the AI system and assessing the potential impact of such attacks. The outcomes guide the design of appropriate defensive measures and inform risk management strategies. This proactive approach is essential for building resilient AI systems.",
          "pillar": "data",
          "phase": "scoping",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "Proactively addresses all relevant tactics (Reconnaissance, Resource Development, Initial Access, ML Model Access, Execution, Impact, etc.)"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Systematically addresses threats across all 7 Layers and Cross-Layer threats"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "Enables proactive consideration for all 10 risks (LLM01-LLM10)"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "Enables proactive consideration for all 10 risks (ML01-ML10)"
              ]
            }
          ],
          "implementationStrategies": [
            "Utilize established threat modeling methodologies (STRIDE, PASTA, OCTAVE) adapted for AI.",
            "Leverage AI-specific threat frameworks (ATLAS, MAESTRO, OWASP).",
            "For agentic AI, consider tool misuse, memory tampering, goal manipulation, etc.",
            "Explicitly include the model training process, environment, and MLOps pipeline components in threat modeling exercises, considering threats of training data manipulation, training code compromise, and environment exploitation (relevant to defenses like AID-H-007).",
            "For systems employing federated learning, specifically model threats related to malicious client participation, insecure aggregation protocols, and potential inference attacks against client data, and evaluate countermeasures like AID-H-008.",
            "Explicitly model threats related to AI hardware security, including side-channel attacks, fault injection, and physical tampering against AI accelerators (addressed by AID-H-009).",
            "Involve a multi-disciplinary team.",
            "Prioritize risks based on likelihood and impact.",
            "Document threat models and integrate into MLOps.",
            "Regularly review and update threat models."
          ],
          "toolsOpenSource": [
            "MITRE ATLAS Navigator",
            "MAESTRO framework documentation",
            "OWASP Top 10 checklists",
            "OWASP Threat Dragon, Microsoft Threat Modeling Tool",
            "Academic frameworks (ATM for LLMs, ATFAA)",
            "NIST AI RMF and Playbook"
          ],
          "toolsCommercial": [
            "AI security consulting services",
            "AI governance and risk management platforms (OneTrust AI Governance, FlowForma)",
            "Some AI red teaming platforms"
          ],
          "subTechniques": [],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "inference attack",
              "tool misuse",
              "apt",
              "threat",
              "risk",
              "vulnerabilities",
              "adversary",
              "attack",
              "misuse",
              "memory",
              "tampering",
              "manipulation",
              "compromise",
              "exploitation",
              "malicious"
            ],
            "defense": [
              "threat modeling",
              "risk assessment",
              "risk management",
              "federated learning",
              "secure aggregation",
              "threat",
              "modeling",
              "risk",
              "assessment",
              "analyze",
              "assessing",
              "resilient",
              "evaluate",
              "document",
              "update"
            ]
          }
        },
        {
          "id": "AID-M-005",
          "name": "AI Configuration Benchmarking & Secure Baselines",
          "description": "Establish, document, maintain, and regularly audit secure configurations for all components of AI systems. This includes the underlying infrastructure (cloud instances, GPU clusters, networks), ML libraries and frameworks, agent runtimes, MLOps pipelines, and specific settings within AI platform APIs (e.g., LLM function access). Configurations are benchmarked against industry standards (e.g., CIS Benchmarks, NIST SSDF), vendor guidance, and internal security policies to identify and remediate misconfigurations that could be exploited by attackers.",
          "pillar": "infra",
          "phase": "scoping",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.TA0004 Initial Access (misconfigurations)",
                "AML.TA0005 Execution (insecure settings)"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Misconfigurations in L4: Deployment & Infrastructure",
                "Insecure default settings in L3: Agent Frameworks or L1: Foundation Models"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM03:2025 Supply Chain",
                "Indirectly LLM06:2025 Excessive Agency"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML06:2023 AI Supply Chain Attacks (misconfigured components)"
              ]
            }
          ],
          "implementationStrategies": [],
          "toolsOpenSource": [],
          "toolsCommercial": [],
          "subTechniques": [
            {
              "id": "AID-M-005.001",
              "name": "Design - Secure Configuration Baseline Development",
              "description": "Covers the 'design' phase of creating and documenting secure, hardened templates and configurations for all AI system components, based on industry benchmarks. This proactive technique involves defining 'golden standard' configurations for infrastructure, containers, and AI platforms to ensure that systems are secure by default, systematically reducing the attack surface by eliminating common misconfigurations before deployment.",
              "pillar": "infra",
              "phase": "scoping",
              "implementationStrategies": [
                "Develop and enforce secure baseline configurations using Infrastructure as Code (IaC).",
                "Create and use hardened, minimal-footprint base container images for AI workloads.",
                "Utilize security benchmarks (CIS, NIST SSDF) to seed concrete baseline controls and enforce them in the cluster.",
                "Harden default settings of common AI development tools (e.g., Jupyter) and distribute a pre-secured image."
              ],
              "toolsOpenSource": [
                "Terraform, Ansible, CloudFormation, Pulumi (for IaC)",
                "Docker, Podman (for containerization)",
                "CIS Benchmarks, NIST Secure Software Development Framework (SSDF) (guidance documents)",
                "OpenSCAP (compliance checking)"
              ],
              "toolsCommercial": [
                "Configuration management platforms (Ansible Tower, Puppet Enterprise)",
                "Cloud provider guidance (AWS Well-Architected Framework, Azure Security Center)",
                "HashiCorp Terraform Enterprise"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.TA0004 Initial Access",
                    "AML.TA0005 Execution"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Misconfigurations (L4)",
                    "Compromised Container Images (L4)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM03:2025 Supply Chain"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML06:2023 AI Supply Chain Attacks"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "anomalous behavior",
                  "shadow ai",
                  "unauthorized access",
                  "attack",
                  "drift",
                  "deviation",
                  "degradation",
                  "manipulation",
                  "reconnaissance",
                  "misconfiguration",
                  "exposure"
                ],
                "defense": [
                  "secure configuration",
                  "secure baseline",
                  "configuration baseline",
                  "design",
                  "secure",
                  "configuration",
                  "baseline",
                  "development",
                  "documenting",
                  "hardened",
                  "containers",
                  "enforce",
                  "container",
                  "harden"
                ]
              }
            },
            {
              "id": "AID-M-005.002",
              "name": "Configuration Baseline Definition & Posture SLOs (Service Level Objectives)",
              "description": "During build and validation, define security configuration baselines for AI infrastructure and services as policy-as-code, and establish measurable posture SLO/SLI and release gates. This technique focuses on producing versioned, signed baselines and scoring criteria as the single source of truth for subsequent deployments and audits; it does not include runtime CSPM or continuous monitoring (those belong under Detect).",
              "pillar": "infra",
              "phase": "building",
              "implementationStrategies": [
                "Author security baselines as policy-as-code and wire them into CI gates.",
                "Define posture SLO/SLI and scoring rules; document release gates.",
                "Produce and sign a Baseline Manifest to ensure versioning and immutability."
              ],
              "toolsOpenSource": [
                "Checkov, Terrascan, tfsec, KICS (IaC security scanners)",
                "TruffleHog, gitleaks, git-secrets (for secrets scanning)",
                "Open Policy Agent (OPA) (for writing custom policies)"
              ],
              "toolsCommercial": [
                "Bridgecrew (by Palo Alto Networks)",
                "Snyk IaC",
                "Prisma Cloud (by Palo Alto Networks)",
                "Wiz",
                "Tenable.cs"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.TA0004 Initial Access"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Misconfigurations (L4)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM03:2025 Supply Chain"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML06:2023 AI Supply Chain Attacks",
                    "ML05:2023 Model Theft (by preventing insecure storage configurations)"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "rollback attack",
                  "unauthorized modification",
                  "anomalous behavior",
                  "malicious input",
                  "invalid input",
                  "tampering",
                  "corruption",
                  "drift",
                  "deviation",
                  "degradation",
                  "manipulation",
                  "injection",
                  "bypass",
                  "exploit",
                  "evasion"
                ],
                "defense": [
                  "continuous monitoring",
                  "configuration baseline",
                  "configuration",
                  "baseline",
                  "definition",
                  "posture",
                  "slos",
                  "level",
                  "objectives",
                  "validation",
                  "versioned",
                  "signed",
                  "monitoring",
                  "detect",
                  "rules"
                ]
              }
            }
          ],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "anomalous behavior",
              "forensic evasion",
              "exploited",
              "drift",
              "deviation",
              "degradation",
              "manipulation",
              "repudiation",
              "tampering",
              "unauthorized"
            ],
            "defense": [
              "secure configuration",
              "secure baseline",
              "configuration",
              "benchmarking",
              "secure",
              "baselines",
              "document",
              "audit",
              "benchmarked",
              "policies",
              "remediate"
            ]
          }
        },
        {
          "id": "AID-M-006",
          "name": "Human-in-the-Loop (HITL) Control Point Mapping",
          "description": "Systematically identify, document, map, and validate all designed human intervention, oversight, and control points within AI systems. This is especially critical for agentic AI and systems capable of high-impact autonomous decision-making. The process includes defining the triggers, procedures, required operator training, and authority levels for human review, override, or emergency system halt. The goal is to ensure that human control can be effectively, safely, and reliably exercised when automated defenses fail, novel threats emerge, or ethical boundaries are approached.",
          "pillar": "app",
          "phase": "scoping",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "Indirectly mitigates AML.T0048 External Harms (by enabling human intervention to prevent or reduce harm from autonomous AI decisions)",
                "AML.TA0005 Execution (if human oversight can interrupt or redirect harmful execution paths initiated by compromised AI)."
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Runaway Agent Behavior (L7: Agent Ecosystem)",
                "Agent Goal Manipulation (L7: Agent Ecosystem) by providing an override mechanism",
                "Unpredictable agent behavior / Performance Degradation (L5: Evaluation & Observability) by allowing human assessment and control",
                "Failure of Safety Interlocks (L6: Security & Compliance)."
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM06:2025 Excessive Agency (by providing a defined mechanism for human control over agent actions and decisions, acting as a crucial backstop)."
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "Contributes to overall system safety and robustness, helping to manage the impact of various attacks by ensuring human oversight can be asserted."
              ]
            }
          ],
          "implementationStrategies": [],
          "toolsOpenSource": [],
          "toolsCommercial": [],
          "subTechniques": [
            {
              "id": "AID-M-006.001",
              "name": "HITL Checkpoint Design & Documentation",
              "description": "This sub-technique covers the initial development phase of implementing Human-in-the-Loop controls. It involves formally defining the specific triggers that require human intervention in code and configuration, implementing the technical hooks for the AI agent to pause and await a decision, and creating the clear Standard Operating Procedures (SOPs) that operators will follow when an intervention is required.",
              "pillar": "app",
              "phase": "scoping",
              "implementationStrategies": [
                "Integrate HITL checkpoint design into the AI SDLC with enforceable configs and production-ready hooks.",
                "Create clear SOPs for every HITL checkpoint and link them directly from alerts."
              ],
              "toolsOpenSource": [
                "YAML, JSON (for configuration files)",
                "Python (for implementing agent logic)",
                "Agentic frameworks (LangChain, AutoGen, CrewAI, Semantic Kernel)",
                "Documentation platforms (MkDocs, Sphinx)",
                "BPMN tools (Camunda Modeler)"
              ],
              "toolsCommercial": [
                "SOAR platforms (Palo Alto XSOAR, Splunk SOAR)",
                "Incident Management platforms (PagerDuty)",
                "Business Process Management (BPM) software (ServiceNow, Pega)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0048 External Harms",
                    "AML.TA0005 Execution"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Failure of Safety Interlocks (L6)",
                    "Runaway Agent Behavior (L7)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM06:2025 Excessive Agency"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "General safety control to manage impact of various attacks."
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "compliance violation",
                  "shadow ai",
                  "unauthorized access",
                  "misconfiguration",
                  "misuse",
                  "unauthorized",
                  "reconnaissance",
                  "exposure"
                ],
                "defense": [
                  "hitl",
                  "checkpoint",
                  "design",
                  "documentation",
                  "human-in-the-loop",
                  "inventory",
                  "visibility",
                  "mapping",
                  "baseline",
                  "catalog",
                  "discovery",
                  "assessment",
                  "governance"
                ]
              }
            },
            {
              "id": "AID-M-006.002",
              "name": "HITL Operator Training & Readiness Testing",
              "description": "Covers the human and procedural readiness aspects of a Human-in-the-Loop (HITL) system. This technique involves developing comprehensive training programs and running simulated emergency scenarios ('fire drills') for human operators. It also includes regularly auditing and testing the technical HITL mechanisms to ensure both operator preparedness and end-to-end functionality, confirming that human control can be asserted effectively and reliably when needed.",
              "pillar": "app",
              "phase": "validation",
              "implementationStrategies": [
                "Develop comprehensive operator training with realistic simulations and measurable outcomes.",
                "Automate regular HITL fire drills to validate end-to-end readiness."
              ],
              "toolsOpenSource": [
                "Python (for custom simulators)",
                "Workflow Orchestrators (Apache Airflow, Prefect, Kubeflow Pipelines)",
                "Grafana, Kibana (for operator performance dashboards)",
                "Oncall (by Grafana Labs), go-incident (open-source incident management)"
              ],
              "toolsCommercial": [
                "Incident Management Platforms (PagerDuty, xMatters)",
                "Cybersecurity training platforms (Immersive Labs, RangeForce)",
                "SOAR platforms (Palo Alto XSOAR, Splunk SOAR)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0048 External Harms"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Failure of Safety Interlocks (L6)",
                    "Runaway Agent Behavior (L7)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM06:2025 Excessive Agency"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "General safety control to manage impact of attacks."
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "forensic evasion",
                  "shadow ai",
                  "unauthorized access",
                  "repudiation",
                  "tampering",
                  "unauthorized",
                  "reconnaissance",
                  "misconfiguration",
                  "exposure"
                ],
                "defense": [
                  "hitl",
                  "operator",
                  "training",
                  "readiness",
                  "testing",
                  "human-in-the-loop",
                  "auditing",
                  "control",
                  "validate",
                  "inventory",
                  "visibility",
                  "mapping",
                  "documentation",
                  "baseline",
                  "catalog"
                ]
              }
            },
            {
              "id": "AID-M-006.003",
              "name": "HITL Escalation & Activity Monitoring",
              "description": "Covers the live operational and security aspects of a Human-in-the-Loop (HITL) system. This technique involves defining and implementing the technical escalation paths for undecided or unhandled intervention requests and ensuring that all HITL activations, operator decisions, and system responses are securely logged. This provides a comprehensive audit trail for forensic analysis and real-time monitoring to detect anomalous operator behavior or high-frequency intervention events.",
              "pillar": "app",
              "phase": "operation",
              "implementationStrategies": [
                "Define, codify, and test clear escalation paths for human intervention.",
                "Implement structured logging and SIEM analytics for all HITL activations and decisions."
              ],
              "toolsOpenSource": [
                "ELK Stack (Elasticsearch, Logstash, Kibana), OpenSearch, Grafana Loki (for logging)",
                "Prometheus, Grafana (for dashboards and metrics)",
                "Sigma (for defining SIEM rules in a standard format)",
                "Oncall (by Grafana Labs)"
              ],
              "toolsCommercial": [
                "Incident Management Platforms (PagerDuty)",
                "SIEM/Log Analytics Platforms (Splunk, Datadog, Google Chronicle, Microsoft Sentinel)",
                "SOAR Platforms (Palo Alto XSOAR, Splunk SOAR)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0048 External Harms"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Evaluation & Observability (L5)",
                    "Repudiation (L7)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM06:2025 Excessive Agency"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "General security control for auditing and investigating incidents."
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "stealth attack",
                  "persistent threat",
                  "forensic evasion",
                  "cover tracks",
                  "log injection",
                  "anomalous",
                  "evasion",
                  "undetected",
                  "repudiation",
                  "tampering",
                  "unauthorized"
                ],
                "defense": [
                  "real-time monitoring",
                  "audit trail",
                  "hitl",
                  "escalation",
                  "activity",
                  "monitoring",
                  "human-in-the-loop",
                  "logged",
                  "audit",
                  "analysis",
                  "detect",
                  "logging",
                  "siem"
                ]
              }
            }
          ],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "lateral movement",
              "attack surface",
              "shadow ai",
              "unauthorized access",
              "reconnaissance",
              "exposure",
              "misconfiguration"
            ],
            "defense": [
              "human review",
              "human-in-the-loop",
              "hitl",
              "control",
              "point",
              "mapping",
              "document",
              "validate",
              "inventory",
              "visibility",
              "documentation",
              "baseline",
              "catalog",
              "discovery",
              "assessment"
            ]
          }
        },
        {
          "id": "AID-M-007",
          "name": "AI Use Case & Safety Boundary Modeling",
          "description": "This technique involves the formal, technical documentation and validation of an AI system's intended purpose, operational boundaries, and ethical guardrails. It translates abstract governance policies into concrete, machine-readable artifacts and automated tests that model the system's safety posture. The goal is to proactively define and enforce the AI's scope of acceptable use, assess it for fairness and bias, and analyze its potential for misuse, creating a verifiable record for security, compliance, and responsible AI assurance, integrated as CI/CD gates and policy-as-code.",
          "pillar": "app",
          "phase": "scoping",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0048 External Harms (by defining and testing against misuse that leads to societal, reputational, or user harm)"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Misuse for Malicious Purposes (Cross-Layer)",
                "Evasion of Auditing/Compliance (L6, by creating the auditable artifacts)",
                "Unpredictable agent behavior / Performance Degradation (L5, by defining clear boundaries)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM01:2025 Prompt Injection (by codifying forbidden intents/topics and enforcing refusal tests)",
                "LLM02:2025 Sensitive Information Disclosure (by defining disallowed data categories and output rules)",
                "LLM06:2025 Excessive Agency (by strict operational boundaries and forbidden actions)",
                "LLM09:2025 Misinformation (by defining forbidden topics and content categories)"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML04:2023 Data Leakage (by specifying disallowed data domains and output redaction rules)",
                "ML08:2023 Model Skewing (by providing a framework for fairness and bias assessment)"
              ]
            }
          ],
          "implementationStrategies": [
            "Codify intended use cases and explicit restrictions in a machine-readable policy file.",
            "Implement automated bias and fairness testing in the CI/CD pipeline.",
            "Generate and maintain auditable Model Cards that include safety and ethical considerations.",
            "Develop and run 'red teaming' test suites that probe for misuse and dual-use potential.",
            "Validate prompts and outputs against the safety policy in CI and staging (policy conformance tests)."
          ],
          "toolsOpenSource": [
            "Fairness toolkits (Fairlearn, IBM AI Fairness 360, Themis-ML)",
            "Bias/Explainability tools (Google's What-If Tool, InterpretML)",
            "Model Card Toolkit (Google)",
            "Documentation and versioning (Git, MkDocs, Sphinx)",
            "Policy-as-code engines (Open Policy Agent - OPA)",
            "Testing frameworks (pytest)"
          ],
          "toolsCommercial": [
            "AI Governance Platforms (Credo AI, OneTrust AI Governance, IBM Watson OpenScale)",
            "Bias detection & mitigation tools (Fiddler AI, Arize AI, Arthur)",
            "GRC (Governance, Risk, and Compliance) platforms"
          ],
          "subTechniques": [],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "compliance violation",
              "malicious input",
              "invalid input",
              "forensic evasion",
              "artifacts",
              "misuse",
              "prompts",
              "misconfiguration",
              "unauthorized",
              "injection",
              "bypass",
              "exploit",
              "repudiation",
              "tampering",
              "jailbreak"
            ],
            "defense": [
              "use",
              "case",
              "safety",
              "boundary",
              "modeling",
              "guardrail",
              "guardrails",
              "documentation",
              "validation",
              "governance",
              "policies",
              "enforce",
              "scope",
              "assess",
              "analyze"
            ]
          }
        },
        {
          "id": "AID-M-008",
          "name": "Automated Agentic Security Benchmarking",
          "description": "Integrate standardized security benchmark suites (such as AgentHarm, ToolEmu, or R-Judge) into the CI/CD pipeline to quantitatively measure an AI agent's resistance to adversarial attacks, safety policy compliance, and tool misuse risks. This ensures that any changes to the agent's prompts, models, or tools do not degrade its security posture before deployment, moving security testing from ad-hoc red teaming toward continuous regression testing.",
          "pillar": "model",
          "phase": "validation",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0048 External Harms"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Evaluation & Observability (L5 in general)",
                "Framework Evasion (L3)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM06:2025 Excessive Agency",
                "LLM01:2025 Prompt Injection"
              ]
            }
          ],
          "implementationStrategies": [
            "Integrate agentic security test suites (for example garak promptinject probes) into CI/CD as a blocking gate."
          ],
          "toolsOpenSource": [
            "garak (Generative AI Red-teaming & Assessment Kit)",
            "AgentHarm Dataset",
            "ToolEmu",
            "promptfoo"
          ],
          "toolsCommercial": [
            "Robust Intelligence",
            "Lakera Red Teaming",
            "Credo AI"
          ],
          "subTechniques": [],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "adversarial attack",
              "tool misuse",
              "shadow ai",
              "unauthorized access",
              "adversarial",
              "misuse",
              "risks",
              "prompts",
              "degrade",
              "reconnaissance",
              "misconfiguration",
              "exposure"
            ],
            "defense": [
              "automated",
              "agentic",
              "security",
              "benchmarking",
              "benchmark",
              "safety",
              "policy",
              "compliance",
              "blocking",
              "inventory",
              "visibility",
              "mapping",
              "documentation",
              "baseline",
              "catalog"
            ]
          }
        },
        {
          "id": "AID-M-009",
          "name": "Agent Autonomy Level Governance",
          "description": "Establish a formal governance framework that categorizes AI agents into discrete 'Autonomy Levels' (for example L0 to L4) based on their capabilities and associated risks. Each level mandates a specific bundle of technical controls (for example HITL requirements, logging depth, and tool restrictions). This ensures that high-autonomy agents (such as those that can autonomously write code or spend money) are deployed with correspondingly rigorous safeguards, preventing 'Excessive Agency' by design.",
          "pillar": "app",
          "phase": "scoping",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0048 External Harms"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Integration Risks (L7)",
                "Agent Goal Manipulation (L7)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM06:2025 Excessive Agency"
              ]
            }
          ],
          "implementationStrategies": [
            "Define Autonomy Levels and enforce control bundles using OPA."
          ],
          "toolsOpenSource": [
            "Open Policy Agent (OPA)",
            "Kyverno (for Kubernetes policy enforcement)",
            "Rego (policy language)"
          ],
          "toolsCommercial": [
            "Styra DAS",
            "OneTrust AI Governance"
          ],
          "subTechniques": [],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "cover tracks",
              "log injection",
              "shadow ai",
              "unauthorized access",
              "risks",
              "repudiation",
              "tampering",
              "reconnaissance",
              "misconfiguration",
              "exposure"
            ],
            "defense": [
              "policy enforcement",
              "agent",
              "autonomy",
              "level",
              "governance",
              "logging",
              "safeguards",
              "preventing",
              "enforce",
              "control"
            ]
          }
        }
      ]
    },
    {
      "id": "harden",
      "name": "Harden",
      "description": "The \"Harden\" tactic encompasses proactive measures taken to reinforce AI systems and reduce their attack surface before an attack occurs. These techniques aim to make AI models, the data they rely on, and the infrastructure they inhabit more resilient to compromise. This involves building security into the design and development phases and applying preventative controls to make successful attacks more difficult, costly, and less impactful for adversaries.",
      "techniques": [
        {
          "id": "AID-H-001",
          "name": "Adversarial Robustness Training",
          "description": "A set of techniques that proactively improve a model's resilience to adversarial inputs by training it with examples specifically crafted to try and fool it. This process 'vaccinates' the model against various forms of attackfrom subtle, full-image perturbations to localized, high-visibility adversarial patchesby directly incorporating adversarial defense into the training loop, forcing the model to learn more robust and generalizable features.",
          "pillar": "model",
          "phase": "building",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0015: Evade ML Model",
                "AML.T0043: Craft Adversarial Data",
                "AML.T0018: Manipulate AI Model (via robust training)",
                "AML.T0020: Poison Training Data (via robust training)"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Adversarial Examples (L1)",
                "Evasion of Security AI Agents (L6)",
                "Data Poisoning (L2)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM01:2025 Prompt Injection",
                "LLM04:2025 Data and Model Poisoning"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML01:2023 Input Manipulation Attack",
                "ML02:2023 Data Poisoning Attack",
                "ML10:2023 Model Poisoning"
              ]
            }
          ],
          "implementationStrategies": [
            "Implement PGD-based adversarial training to defend against evasion attacks.",
            "Use 'Friendly' adversarial training to balance robust and clean accuracy.",
            "Employ efficient methods like 'Free' Adversarial Training to reduce computational cost.",
            "For Reinforcement Learning, use a two-player, zero-sum game setup (RARL).",
            "Implement patch-based adversarial training for vision models.",
            "Employ masking or occlusion-based training as data augmentation."
          ],
          "toolsOpenSource": [
            "Adversarial Robustness Toolbox (ART), CleverHans, Foolbox, Torchattacks",
            "RL Libraries (Stable-Baselines3, RLlib, PettingZoo)",
            "PyTorch, TensorFlow",
            "Albumentations, torchvision.transforms (for data augmentation)",
            "MLflow, Weights & Biases (for experiment tracking)"
          ],
          "toolsCommercial": [
            "AI security platforms (Robust Intelligence, HiddenLayer, Protect AI, Adversa.AI)",
            "MLOps platforms (Amazon SageMaker, Google Vertex AI, Databricks, Azure ML)",
            "RL Platforms (Microsoft Bonsai, AnyLogic)"
          ],
          "subTechniques": [],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "evasion attack",
              "adversarial input",
              "attack surface",
              "adversarial",
              "attack",
              "evasion",
              "exploitation",
              "vulnerability",
              "misconfiguration",
              "bypass"
            ],
            "defense": [
              "adversarial training",
              "adversarial robustness",
              "adversarial",
              "robustness",
              "training",
              "resilience",
              "defense",
              "robust",
              "defend",
              "patch"
            ]
          }
        },
        {
          "id": "AID-H-002",
          "name": "AI-Contextualized Data Sanitization & Input Validation",
          "description": "Implement rigorous validation, sanitization, and filtering mechanisms for all data fed into AI systems. This applies to training data, fine-tuning data, and live operational inputs (including user prompts for LLMs). The goal is to detect and remove or neutralize malicious content, anomalous data, out-of-distribution samples, or inputs structured to exploit vulnerabilities like prompt injection or data poisoning before they can adversely affect the model or downstream systems. For LLMs, this involves specific techniques like stripping or encoding control tokens and filtering for known injection patterns or harmful content. For multimodal systems, this includes validating and sanitizing inputs across all modalities (e.g., text, image, audio, video) and ensuring that inputs in one modality cannot be readily used to trigger vulnerabilities, bypass controls, or inject malicious content into another modality processing pathway.",
          "pillar": "data",
          "phase": "building",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0020 Poison Training Data",
                "AML.T0051 LLM Prompt Injection",
                "AML.T0070 RAG Poisoning",
                "AML.T0054 LLM Jailbreak",
                "AML.T0059 Erode Dataset Integrity",
                "AML.T0049 Exploit Public-Facing Application",
                "AML.T0061 LLM Prompt Self-Replication",
                "AML.T0068 LLM Prompt Obfuscation",
                "AML.T0071 False RAG Entry Injection",
                "AML.T0065 LLM Prompt Crafting",
                "AML.T0056 Extract LLM System Prompt"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Data Poisoning (L2)",
                "Input Validation Attacks (L3)",
                "Compromised RAG Pipelines (L2)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM01:2025 Prompt Injection",
                "LLM04:2025 Data and Model Poisoning",
                "LLM08:2025 Vector and Embedding Weaknesses"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML01:2023 Input Manipulation Attack",
                "ML02:2023 Data Poisoning Attack"
              ]
            }
          ],
          "implementationStrategies": [],
          "toolsOpenSource": [],
          "toolsCommercial": [],
          "subTechniques": [
            {
              "id": "AID-H-002.001",
              "name": "Training & Fine-Tuning Data Sanitization",
              "description": "Focuses on detecting and removing poisoned samples, unwanted biases, or sensitive data from datasets before they are used for model training or fine-tuning. This pre-processing step is critical for preventing the model from learning vulnerabilities or undesirable behaviors from the outset.",
              "pillar": "data",
              "phase": "building",
              "implementationStrategies": [
                "Perform EDA to understand distributions and flag outliers for human review.",
                "Validate data against schema and constraints with policy-as-code.",
                "Detect anomalies with an unsupervised model and quarantine high-error samples.",
                "Scan and anonymize PII or sensitive data prior to training.",
                "Verify integrity and provenance of external datasets with strong hashing."
              ],
              "toolsOpenSource": [
                "Great Expectations (for data validation and quality checks)",
                "TensorFlow Data Validation (TFDV)",
                "ydata-profiling (for EDA and outlier detection)",
                "Microsoft Presidio (for PII detection and anonymization)",
                "cleanlab (for finding and cleaning label errors)",
                "Alibi Detect (for outlier detection)"
              ],
              "toolsCommercial": [
                "Databricks (with Delta Lake and Data Quality Monitoring)",
                "Alation, Collibra (for data governance and quality)",
                "Gretel.ai, Tonic.ai (for PII removal and synthetic data generation)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0020: Poison Training Data",
                    "AML.T0018: Manipulate AI Model",
                    "AML.T0057: LLM Data Leakage (by removing PII)",
                    "AML.T0059 Erode Dataset Integrity"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Data Poisoning (L2)",
                    "Model Skewing (L2)",
                    "Compromised RAG Pipelines (L2)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM04:2025 Data and Model Poisoning",
                    "LLM02:2025 Sensitive Information Disclosure"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML02:2023 Data Poisoning Attack",
                    "ML10:2023 Model Poisoning",
                    "ML08:2023 Model Skewing",
                    "ML04:2023 Membership Inference Attack (by removing sensitive records)"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "data poisoning",
                  "supply chain attack",
                  "untrusted data",
                  "unauthorized modification",
                  "malicious input",
                  "poisoned",
                  "vulnerabilities",
                  "tampering",
                  "corruption",
                  "manipulation",
                  "forgery",
                  "injection",
                  "poisoning",
                  "exploit",
                  "xss"
                ],
                "defense": [
                  "human review",
                  "data validation",
                  "training",
                  "fine-tuning",
                  "data",
                  "sanitization",
                  "detecting",
                  "preventing",
                  "validate",
                  "constraints",
                  "detect",
                  "quarantine",
                  "scan",
                  "verify",
                  "integrity"
                ]
              }
            },
            {
              "id": "AID-H-002.002",
              "name": "Inference-Time Prompt & Input Validation",
              "description": "Focuses on real-time defense against malicious inputs at the point of inference, such as prompt injection, jailbreaking attempts, or other input-based evasions. This technique acts as a guardrail for the live, operational model.",
              "pillar": "app",
              "phase": "operation",
              "implementationStrategies": [
                "Apply strict schema validation and bounds checking for all user inputs.",
                "Canonicalize text (Unicode normalization, strip control chars) to defeat obfuscation.",
                "Sanitize prompts: strip/escape control tokens and known injection markers.",
                "Moderate with a guardrail model or API before invoking the primary LLM.",
                "Synchronous Gate: rule-pack driven allow/deny with deterministic short-circuit",
                "Synchronous Gate: safe regex execution with timeouts / RE2 semantics",
                "Synchronous Gate: Aho-Corasick / trie keyword scanning",
                "Synchronous Gate: deterministic pipeline ordering at the API edge",
                "Synchronous Gate: rule-pack lifecycle (signing, hot-reload, and unit tests)",
                "Safe prompt templating to separate trusted instructions from untrusted user input.",
                "Validate and sanitize external/RAG sources before embedding or retrieval."
              ],
              "toolsOpenSource": [
                "NVIDIA NeMo Guardrails",
                "Rebuff",
                "LangChain Guardrails",
                "Llama Guard (Meta)",
                "Pydantic (for structured input validation)"
              ],
              "toolsCommercial": [
                "OpenAI Moderation API",
                "Google Perspective API",
                "Lakera Guard",
                "Protect AI Guardian",
                "CalypsoAI Validator",
                "Securiti LLM Firewall"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0051: LLM Prompt Injection",
                    "AML.T0071 False RAG Entry Injection",
                    "AML.T0054: LLM Jailbreak",
                    "AML.T0068: LLM Prompt Obfuscation"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Input Validation Attacks (L3)",
                    "Reprogramming Attacks (L1)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM01:2025 Prompt Injection",
                    "LLM08:2025 Vector and Embedding Weaknesses"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML01:2023 Input Manipulation Attack"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "prompt injection",
                  "malicious input",
                  "user input",
                  "prompt",
                  "malicious",
                  "injection",
                  "jailbreaking",
                  "prompts",
                  "tokens",
                  "untrusted",
                  "embedding"
                ],
                "defense": [
                  "input validation",
                  "inference-time",
                  "prompt",
                  "input",
                  "validation",
                  "guardrail",
                  "defense",
                  "control",
                  "sanitize",
                  "tokens",
                  "deny",
                  "safe",
                  "scanning",
                  "signing",
                  "trusted"
                ]
              }
            },
            {
              "id": "AID-H-002.003",
              "name": "Multimodal Input Sanitization",
              "description": "Focuses on the unique challenges of validating and sanitizing non-textual inputs like images, audio, and video before they are processed by a model. This includes implementing defensive transformations to remove adversarial perturbations, stripping potentially malicious metadata, and ensuring consistency across modalities to prevent cross-modal attacks.",
              "pillar": "data",
              "phase": "building",
              "implementationStrategies": [
                "Rebuild images from pixels and strip metadata; optionally re-encode to disrupt noise.",
                "Purify images via a denoising diffusion step for high-security use cases.",
                "Re-encode audio to standardized lossy or PCM format to remove hidden commands.",
                "Cross-modal consistency checks to detect mismatched or stego-driven prompts.",
                "File-type and content safety gates for all uploads."
              ],
              "toolsOpenSource": [
                "Pillow (PIL Fork), OpenCV (for image processing)",
                "pydub, Librosa (for audio processing)",
                "Hugging Face Diffusers (for diffusion models)",
                "Hugging Face Transformers (for captioning and similarity models)",
                "sentence-transformers"
              ],
              "toolsCommercial": [
                "AI security platforms (Protect AI, HiddenLayer, Adversa.AI)",
                "Content moderation services (Hive AI, Clarifai)",
                "Cloud Provider AI Services (Amazon Rekognition, Google Cloud Vision AI)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0043: Craft Adversarial Data",
                    "AML.T0051: LLM Prompt Injection (via non-text modalities)"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Cross-Modal Manipulation Attacks (L1)",
                    "Input Validation Attacks (L3)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM01:2025 Prompt Injection"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML01:2023 Input Manipulation Attack"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "adversarial perturbation",
                  "malicious input",
                  "adversarial",
                  "malicious",
                  "prompts",
                  "injection",
                  "poisoning",
                  "exploit",
                  "xss",
                  "sqli"
                ],
                "defense": [
                  "input sanitization",
                  "multimodal",
                  "input",
                  "sanitization",
                  "validating",
                  "sanitizing",
                  "prevent",
                  "detect",
                  "safety",
                  "hardening",
                  "protection",
                  "prevention",
                  "secure",
                  "robust",
                  "resilient"
                ]
              }
            }
          ],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "prompt injection",
              "data poisoning",
              "prompts",
              "malicious",
              "anomalous",
              "exploit",
              "vulnerabilities",
              "prompt",
              "injection",
              "poisoning",
              "tokens",
              "harmful",
              "bypass",
              "inject"
            ],
            "defense": [
              "input validation",
              "ai-contextualized",
              "data",
              "sanitization",
              "input",
              "validation",
              "filtering",
              "detect",
              "control",
              "tokens",
              "validating",
              "sanitizing"
            ]
          }
        },
        {
          "id": "AID-H-003",
          "name": "Secure ML Supply Chain Management",
          "description": "Apply rigorous software supply chain security principles throughout the AI/ML development and operational lifecycle. This involves verifying the integrity, authenticity, and security of all components, including source code, pre-trained models, datasets, ML libraries, development tools, and deployment infrastructure. The aim is to prevent the introduction of vulnerabilities, backdoors, malicious code (e.g., via compromised dependencies), or tampered artifacts into the AI system. This is critical as AI systems often rely on a complex ecosystem of third-party elements.",
          "pillar": "infra",
          "phase": "building",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0010.000: AI Supply Chain Compromise: Hardware",
                "AML.T0010.001: AI Supply Chain Compromise: AI Software",
                "AML.T0010.002: AI Supply Chain Compromise: Data",
                "AML.T0010.003: AI Supply Chain Compromise: Model",
                "AML.T0010.004: AI Supply Chain Compromise: Container Registry",
                "AML.T0011.001: User Execution: Malicious Package",
                "AML.T0019: Publish Poisoned Datasets",
                "AML.T0058: Publish Poisoned Models",
                "AML.T0059: Erode Dataset Integrity",
                "AML.T0076: Corrupt AI Model",
                "AML.T0073: Impersonation",
                "AML.T0074: Masquerading"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Compromised Framework Components (L3)",
                "Compromised Container Images (L4)",
                "Supply Chain Attacks (Cross-Layer)",
                "Model Tampering (L1)",
                "Backdoor Attacks (L1)",
                "Data Poisoning (L2)",
                "Compromised RAG Pipelines (L2)",
                "Physical Tampering (L4)",
                "Side-Channel Attacks (L4)",
                "Compromised Hardware Accelerators (L4)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM03:2025 Supply Chain",
                "LLM04:2025 Data and Model Poisoning"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML02:2023 Data Poisoning Attack",
                "ML06:2023 AI Supply Chain Attacks",
                "ML07:2023 Transfer Learning Attack",
                "ML10:2023 Model Poisoning"
              ]
            }
          ],
          "implementationStrategies": [],
          "toolsOpenSource": [],
          "toolsCommercial": [],
          "subTechniques": [
            {
              "id": "AID-H-003.001",
              "name": "Software Dependency & Package Security",
              "description": "Ensure integrity of all third-party code and libraries (Python packages, containers, build tools) used to develop and serve AI workloads.",
              "pillar": "infra",
              "phase": "building",
              "implementationStrategies": [
                "Run SCA scanners (Syft/Grype, Trivy) on every build pipeline.",
                "Pin exact versions & hashes in requirements/lock files; block implicit upgrades.",
                "Sign artifacts with Sigstore cosign + in-toto link metadata.",
                "Fail the build if a dependency is yanked or contains critical CVEs."
              ],
              "toolsOpenSource": [
                "Trivy",
                "Syft",
                "Grype",
                "Sigstore",
                "in-toto",
                "OWASP Dependency-Check",
                "pip-audit"
              ],
              "toolsCommercial": [
                "Snyk",
                "Mend (formerly WhiteSource)",
                "JFrog Xray",
                "Veracode SCA",
                "Checkmarx SCA"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0010.001: AI Supply Chain Compromise: AI Software",
                    "AML.T0010.004: AI Supply Chain Compromise: Container Registry",
                    "AML.T0011.001: User Execution: Malicious Package"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Compromised Framework Components (L3)",
                    "Compromised Container Images (L4)",
                    "Supply Chain Attacks (Cross-Layer)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM03:2025 Supply Chain (Traditional Third-party Package Vulnerabilities)"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML06:2023 AI Supply Chain Attacks"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "supply chain attack",
                  "dependency confusion",
                  "vulnerable dependency",
                  "unauthorized modification",
                  "artifacts",
                  "compromised",
                  "tampering",
                  "corruption",
                  "manipulation",
                  "forgery"
                ],
                "defense": [
                  "software",
                  "dependency",
                  "package",
                  "security",
                  "integrity",
                  "containers",
                  "sca",
                  "block",
                  "sign",
                  "hardening",
                  "protection",
                  "prevention",
                  "secure",
                  "robust",
                  "resilient"
                ]
              }
            },
            {
              "id": "AID-H-003.002",
              "name": "CI/CD Release Gating, Model Artifact Signing & Secure Distribution",
              "description": "Mandatory, automated acceptance criteria applied to a model artifact before promotion to production. Acts as a final security gate to ensure only trusted, verified, and safely configured models are deployed by validating their origin, integrity, provenance, and operational policies. This control assumes production never trusts public names directly; only immutable, attested bytes from an internal mirror are allowed.",
              "pillar": "infra",
              "phase": "building",
              "implementationStrategies": [
                "Store in internal registry; require signatures and digest pinning for promotion.",
                "Use reproducible, safe packaging and verify on deploy.",
                "Serve over mTLS and enforce content-hash pinning at the inference layer.",
                "Enforce a mandatory acceptance criteria checklist for production promotion."
              ],
              "toolsOpenSource": [
                "MLflow Model Registry",
                "Sigstore/cosign",
                "in-toto / SLSA",
                "safetensors",
                "Kyverno",
                "Sigstore Policy Controller",
                "Open Policy Agent (OPA)"
              ],
              "toolsCommercial": [
                "Protect AI Platform (ModelScan)",
                "Databricks Model Registry",
                "Amazon SageMaker Model Registry",
                "Google Vertex AI Model Registry",
                "JFrog Artifactory (OCI/metadata)",
                "Snyk Container"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0010.003: AI Supply Chain Compromise: Model",
                    "AML.T0010.004: AI Supply Chain Compromise: Container Registry",
                    "AML.T0058: Publish Poisoned Models",
                    "AML.T0076: Corrupt AI Model",
                    "AML.T0074: Masquerading"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Model Tampering (L1)",
                    "Backdoor Attacks (L1)",
                    "Supply Chain Attacks (Cross-Layer)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM03:2025 Supply Chain (Vulnerable Pre-Trained Model)",
                    "LLM04:2025 Data and Model Poisoning"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML06:2023 AI Supply Chain Attacks",
                    "ML10:2023 Model Poisoning"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "data poisoning",
                  "supply chain attack",
                  "untrusted data",
                  "unauthorized modification",
                  "artifact",
                  "tampering",
                  "corruption",
                  "manipulation",
                  "forgery"
                ],
                "defense": [
                  "model registry",
                  "release",
                  "gating",
                  "model",
                  "artifact",
                  "signing",
                  "secure",
                  "distribution",
                  "trusted",
                  "verified",
                  "validating",
                  "integrity",
                  "provenance",
                  "policies",
                  "control"
                ]
              }
            },
            {
              "id": "AID-H-003.003",
              "name": "Dataset Supply Chain Validation",
              "description": "Authenticate, checksum and license-check every external dataset (training, fine-tuning, RAG).",
              "pillar": "data",
              "phase": "building",
              "implementationStrategies": [
                "Maintain per-file hashes in DVC/LakeFS; block pipeline if hash drift.",
                "Run license & PII scanners (Gretel, Presidio) before datasets enter feature store.",
                "Embed signed provenance metadata (datasheets for datasets) for auditing."
              ],
              "toolsOpenSource": [
                "DVC (Data Version Control)",
                "LakeFS",
                "Great Expectations (for data validation)",
                "Microsoft Presidio (for PII scanning)"
              ],
              "toolsCommercial": [
                "Gretel.ai",
                "Databricks Unity Catalog",
                "Alation, Collibra (for data governance and lineage)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0010.002: AI Supply Chain Compromise: Data",
                    "AML.T0019: Publish Poisoned Datasets"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Data Poisoning (L2)",
                    "Compromised RAG Pipelines (L2)",
                    "Supply Chain Attacks (Cross-Layer)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM03:2025 Supply Chain (Outdated or Deprecated Models/Datasets)",
                    "LLM04:2025 Data and Model Poisoning"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML02:2023 Data Poisoning Attack",
                    "ML07:2023 Transfer Learning Attack"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "data poisoning",
                  "supply chain attack",
                  "untrusted data",
                  "malicious input",
                  "invalid input",
                  "forensic evasion",
                  "tampering",
                  "injection",
                  "bypass",
                  "exploit",
                  "repudiation",
                  "unauthorized"
                ],
                "defense": [
                  "data validation",
                  "dataset",
                  "supply",
                  "chain",
                  "validation",
                  "authenticate",
                  "block",
                  "hash",
                  "signed",
                  "provenance",
                  "auditing"
                ]
              }
            },
            {
              "id": "AID-H-003.004",
              "name": "Hardware & Firmware Integrity Assurance",
              "description": "Verify accelerator cards, firmware and BIOS/UEFI images are genuine and un-modified before joining an AI cluster.",
              "pillar": "infra",
              "phase": "building",
              "implementationStrategies": [
                "Secure-boot GPUs/TPUs; attestation via TPM/CCA or NVIDIA Confidential Computing.",
                "Continuously monitor firmware versions and revoke out-of-policy images.",
                "Run side-channel/fault-injection self-tests during maintenance windows."
              ],
              "toolsOpenSource": [
                "Open-source secure boot implementations (e.g., U-Boot)",
                "Firmware analysis tools (e.g., binwalk)",
                "Intel SGX SDK, Open Enclave SDK"
              ],
              "toolsCommercial": [
                "NVIDIA Confidential Computing",
                "Azure Confidential Computing",
                "Google Cloud Confidential Computing",
                "Hardware Security Modules (HSMs)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0010.000: AI Supply Chain Compromise: Hardware"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Physical Tampering (L4)",
                    "Side-Channel Attacks (L4)",
                    "Compromised Hardware Accelerators (L4)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM03:2025 Supply Chain (LLM Model on Device supply-chain vulnerabilities)"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML06:2023 AI Supply Chain Attacks"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "unauthorized modification",
                  "attack surface",
                  "tampering",
                  "corruption",
                  "manipulation",
                  "forgery",
                  "exploitation",
                  "vulnerability",
                  "misconfiguration",
                  "bypass"
                ],
                "defense": [
                  "hardware",
                  "firmware",
                  "integrity",
                  "assurance",
                  "verify",
                  "attestation",
                  "confidential",
                  "monitor",
                  "hardening",
                  "protection",
                  "prevention",
                  "secure",
                  "robust",
                  "resilient",
                  "mitigation"
                ]
              }
            },
            {
              "id": "AID-H-003.005",
              "name": "Infrastructure as Code (IaC) Security Scanning for AI Systems",
              "description": "Covers the 'pre-deployment' phase of automatically scanning Infrastructure as Code (IaC) files (e.g., Terraform, CloudFormation, Kubernetes YAML) in the CI/CD pipeline. This 'shift-left' security practice aims to detect and block security misconfigurations, such as insecure network paths that could undermine model supply chain security, before infrastructure is provisioned.",
              "pillar": "infra",
              "phase": "validation",
              "implementationStrategies": [
                "Integrate IaC security scanners into the CI/CD pipeline to act as a merge blocker.",
                "Scan for hardcoded secrets within IaC and configuration files.",
                "Develop and enforce custom policies to prevent insecure network paths for AI workloads."
              ],
              "toolsOpenSource": [
                "Checkov",
                "Terrascan",
                "tfsec",
                "KICS",
                "TruffleHog",
                "gitleaks",
                "Open Policy Agent (OPA)"
              ],
              "toolsCommercial": [
                "Snyk IaC",
                "Prisma Cloud",
                "Wiz",
                "Tenable.cs"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.TA0004: Initial Access"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Misconfigurations (L4)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM03:2025 Supply Chain"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML06:2023 AI Supply Chain Attacks",
                    "ML05:2023 Model Theft (by preventing insecure storage configurations)"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "attack surface",
                  "insecure",
                  "undermine",
                  "secrets",
                  "exploitation",
                  "vulnerability",
                  "misconfiguration",
                  "bypass"
                ],
                "defense": [
                  "infrastructure",
                  "code",
                  "iac",
                  "security",
                  "scanning",
                  "systems",
                  "detect",
                  "block",
                  "scan",
                  "secrets",
                  "enforce",
                  "policies",
                  "prevent"
                ]
              }
            },
            {
              "id": "AID-H-003.006",
              "name": "Model SBOM & Provenance Attestation",
              "description": "Produce a model-centric SBOM that inventories model bytes (files, hashes), tokenizer, config, format, and loader code commit; bind it to the exact artifact digest via in-toto/SLSA attestation signed with Sigstore. Verify the attestation and digest binding at admission and re-verify hashes at runtime before loading. This creates a tamper-evident content contract for the model, closing name/namespace trust gaps.",
              "pillar": "infra",
              "phase": "building",
              "implementationStrategies": [
                "Create deterministic model package and compute digest",
                "Generate Model SBOM (JSON) describing bytes and critical metadata",
                "Attach SBOM via in-toto/SLSA attestation and sign with Sigstore",
                "Publish as OCI artifact with digest addressing (optional but recommended)",
                "Admission policy: verify attestation issuer/subject, Rekor inclusion, and digest binding",
                "Runtime guard: re-hash before load and block on mismatch",
                "Tombstone & revocation handling for upstream namespace/owner changes",
                "Format allow-list and unsafe deserialization ban"
              ],
              "toolsOpenSource": [
                "Sigstore/cosign",
                "in-toto / SLSA",
                "oras (OCI artifacts)",
                "jq, sha256sum",
                "safetensors"
              ],
              "toolsCommercial": [
                "JFrog Artifactory (OCI + metadata)",
                "Databricks/SageMaker/Vertex registries",
                "Protect AI (model scanning)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0010.003: AI Supply Chain Compromise: Model",
                    "AML.T0074: Masquerading",
                    "AML.T0076: Corrupt AI Model",
                    "AML.T0058: Publish Poisoned Models"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Model Tampering (L1)",
                    "Supply Chain Attacks (Cross-Layer)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM03:2025 Supply Chain",
                    "LLM04:2025 Data and Model Poisoning"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "data poisoning",
                  "supply chain attack",
                  "untrusted data",
                  "attack surface",
                  "artifact",
                  "gaps",
                  "tampering",
                  "exploitation",
                  "vulnerability",
                  "misconfiguration",
                  "bypass"
                ],
                "defense": [
                  "model",
                  "sbom",
                  "provenance",
                  "attestation",
                  "signed",
                  "verify",
                  "trust",
                  "sign",
                  "policy",
                  "block"
                ]
              }
            }
          ],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "unauthorized modification",
              "vulnerabilities",
              "malicious",
              "compromised",
              "tampered",
              "artifacts",
              "tampering",
              "corruption",
              "manipulation",
              "forgery"
            ],
            "defense": [
              "secure",
              "supply",
              "chain",
              "management",
              "verifying",
              "integrity",
              "prevent",
              "hardening",
              "protection",
              "prevention",
              "robust",
              "resilient",
              "mitigation",
              "defense",
              "control"
            ]
          }
        },
        {
          "id": "AID-H-004",
          "name": "Identity & Access Management (IAM) for AI Systems",
          "description": "Implement and enforce comprehensive Identity and Access Management (IAM) controls for all AI resources, including models, APIs, data stores, agentic tools, and administrative interfaces. This involves applying the principle of least privilege, strong authentication, and robust authorization to limit who and what can interact with, modify, or manage AI systems.",
          "pillar": "infra",
          "phase": "building",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0012: Valid Accounts",
                "AML.T0040: AI Model Inference API Access",
                "AML.T0036 Data from Information Repositories",
                "AML.T0037 Data from Local System",
                "AML.T0044 Full AI Model Access",
                "AML.T0053 AI Agent Tool Invocation",
                "AML.T0073 Impersonation",
                "AML.T0055 Unsecured Credentials"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Agent Identity Attack (L7)",
                "Compromised Agent Registry (L7)",
                "Compromised Agents (L7)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM02:2025 Sensitive Information Disclosure",
                "LLM03:2025 Supply Chain",
                "LLM06:2025 Excessive Agency"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML05:2023 Model Theft"
              ]
            }
          ],
          "implementationStrategies": [],
          "toolsOpenSource": [],
          "toolsCommercial": [],
          "subTechniques": [
            {
              "id": "AID-H-004.001",
              "name": "User & Privileged Access Management",
              "description": "Focuses on securing access for human users, such as developers, data scientists, and system administrators, who manage and interact with AI systems. The goal is to enforce strong authentication and granular permissions for human identities.",
              "pillar": "infra",
              "phase": "building",
              "implementationStrategies": [
                "Enforce Multi-Factor Authentication (MFA) for all users accessing sensitive AI environments.",
                "Implement Role-Based Access Control (RBAC) to grant permissions based on job function.",
                "Utilize Privileged Access Management (PAM) solutions for administrators to control and audit high-risk actions.",
                "Conduct regular access reviews and promptly de-provision inactive accounts."
              ],
              "toolsOpenSource": [
                "Keycloak",
                "FreeIPA",
                "OpenUnison",
                "HashiCorp Boundary"
              ],
              "toolsCommercial": [
                "Okta, Ping Identity, Auth0 (IDaaS)",
                "CyberArk, Delinea, BeyondTrust (PAM)",
                "Cloud Provider IAM (AWS IAM, Azure AD, Google Cloud IAM)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0012: Valid Accounts",
                    "AML.T0036 Data from Information Repositories",
                    "AML.T0037 Data from Local System"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM02:2025 Sensitive Information Disclosure"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML05:2023 Model Theft"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "unauthorized access",
                  "privilege escalation",
                  "lateral movement",
                  "credential theft",
                  "forensic evasion",
                  "impersonation",
                  "spoofing",
                  "repudiation",
                  "tampering",
                  "unauthorized"
                ],
                "defense": [
                  "access control",
                  "access management",
                  "multi-factor authentication",
                  "role-based access",
                  "user",
                  "privileged",
                  "access",
                  "management",
                  "authentication",
                  "securing",
                  "enforce",
                  "permissions",
                  "mfa",
                  "control",
                  "rbac"
                ]
              }
            },
            {
              "id": "AID-H-004.002",
              "name": "Service & API Authentication",
              "description": "Focuses on securing machine-to-machine communication for AI services. This includes authenticating service accounts, applications, and other services that need to interact with AI model APIs, data stores, or MLOps pipelines.",
              "pillar": "infra",
              "phase": "building",
              "implementationStrategies": [
                "Use OAuth 2.0 client credentials flow for service-to-service authentication.",
                "Implement short-lived, securely managed API keys for external service access.",
                "Enforce mutual TLS (mTLS) for all internal API traffic to ensure both client and server are authenticated.",
                "Use cloud provider IAM roles (e.g., AWS IAM Roles for Service Accounts) for workloads running in the cloud."
              ],
              "toolsOpenSource": [
                "OAuth2-Proxy",
                "SPIFFE/SPIRE (for service identity)",
                "Istio, Linkerd (for mTLS)"
              ],
              "toolsCommercial": [
                "API Gateways (Kong, Apigee, MuleSoft)",
                "Cloud Provider Secret Managers (AWS Secrets Manager, Azure Key Vault)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0040: AI Model Inference API Access",
                    "AML.T0024 Exfiltration via AI Inference API"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM03:2025 Supply Chain",
                    "LLM02:2025 Sensitive Information Disclosure"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML05:2023 Model Theft"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "credential theft",
                  "unauthorized access",
                  "attack surface",
                  "credentials",
                  "keys",
                  "impersonation",
                  "spoofing",
                  "exploitation",
                  "vulnerability",
                  "misconfiguration",
                  "bypass"
                ],
                "defense": [
                  "secret management",
                  "api",
                  "authentication",
                  "securing",
                  "authenticating",
                  "oauth",
                  "credentials",
                  "keys",
                  "enforce",
                  "tls",
                  "authenticated",
                  "iam",
                  "roles"
                ]
              }
            },
            {
              "id": "AID-H-004.003",
              "name": "Secure Agent-to-Agent Communication",
              "description": "Focuses on the unique challenge of securing communications within multi-agent systems. This ensures that autonomous agents can trust each other, and that their messages cannot be spoofed, tampered with, or replayed by an adversary.",
              "pillar": "app",
              "phase": "building",
              "implementationStrategies": [
                "Issue unique, cryptographically verifiable identities to each AI agent (e.g., using SPIFFE/SPIRE).",
                "Encrypt all agent-to-agent communication channels (prefer mTLS).",
                "Digitally sign every inter-agent message to ensure end-to-end integrity and non-repudiation (payload-level signing).",
                "Include sequence numbers or timestamps in messages to prevent replay attacks.",
                "Implement identity-preserving protocol translation (anti-scrubbing) for adapters/gateways.",
                "Protocol pinning & anti-downgrade strict schema validation at L7."
              ],
              "toolsOpenSource": [
                "SPIFFE/SPIRE",
                "Libraries for JWT or PASETO for message signing",
                "gRPC with TLS authentication",
                "JSON Schema (strict contract validation)",
                "Envoy Proxy (mTLS + metadata propagation)",
                "MCP SDK (when applicable)"
              ],
              "toolsCommercial": [
                "Enterprise Service Mesh solutions",
                "API Gateway / Service Gateway products (policy + schema validation + mTLS)",
                "Cloudflare Zero Trust",
                "Kong Gateway",
                "Apigee AI Gateway",
                "Entitle AI (emerging market for agent governance)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0073 Impersonation",
                    "AML.T0055 Unsecured Credentials"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Agent Identity Attack (L7)",
                    "Compromised Agent Registry (L7)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM06:2025 Excessive Agency"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "replay attack",
                  "unauthorized modification",
                  "man in the middle",
                  "malicious input",
                  "apt",
                  "spoofed",
                  "tampered",
                  "adversary",
                  "tampering",
                  "corruption",
                  "manipulation",
                  "forgery",
                  "interception",
                  "injection",
                  "bypass"
                ],
                "defense": [
                  "zero trust",
                  "secure",
                  "agent-to-agent",
                  "communication",
                  "securing",
                  "trust",
                  "encrypt",
                  "sign",
                  "integrity",
                  "signing",
                  "prevent",
                  "gateways",
                  "validation"
                ]
              }
            },
            {
              "id": "AID-H-004.004",
              "name": "ANS-Backed Agent Identity, Verified Discovery & Provenance",
              "description": "Utilizes the Agent Name Service (ANS) as a decentralized, verifiable directory for AI agents. It enforces a secure resolution protocol where all discovery responses are digitally signed by the registry. By implementing a full RA/CA lifecycle, TTL-based revalidation, and mandatory revocation checks (CRL/OCSP), this defense prevents registry poisoning, identity impersonation, and the use of hijacked or stale agent endpoints.",
              "pillar": "infra",
              "phase": "building",
              "implementationStrategies": [
                "Secure ANS Resolution Verification & TTL Management",
                "Mandatory Revocation Check (CRL) for Agent Identities",
                "Cryptographic Capability Pinning (Manifest Hash Binding)"
              ],
              "toolsOpenSource": [
                "SPIFFE/SPIRE",
                "pyspiffe",
                "cryptography.io",
                "Step-ca"
              ],
              "toolsCommercial": [
                "Venafi",
                "HashiCorp Vault",
                "AWS Private CA",
                "GlobalSign"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0012 Valid Accounts",
                    "AML.T0073 Impersonation"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Agent Impersonation (L7)",
                    "Compromised Agent Registry (L7)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM03:2025 Supply Chain"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "data poisoning",
                  "supply chain attack",
                  "untrusted data",
                  "man in the middle",
                  "malicious input",
                  "invalid input",
                  "poisoning",
                  "impersonation",
                  "hijacked",
                  "tampering",
                  "forgery",
                  "interception",
                  "injection",
                  "bypass",
                  "exploit"
                ],
                "defense": [
                  "ans-backed",
                  "agent",
                  "identity",
                  "verified",
                  "discovery",
                  "provenance",
                  "secure",
                  "signed",
                  "registry",
                  "defense",
                  "verification",
                  "cryptographic",
                  "hash"
                ]
              }
            }
          ],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "credential theft",
              "unauthorized access",
              "privilege escalation",
              "attack surface",
              "impersonation",
              "spoofing",
              "bypass",
              "exploitation",
              "vulnerability",
              "misconfiguration"
            ],
            "defense": [
              "access management",
              "least privilege",
              "identity",
              "access",
              "management",
              "iam",
              "systems",
              "authentication",
              "authorization",
              "enforce",
              "privilege",
              "robust",
              "limit"
            ]
          }
        },
        {
          "id": "AID-H-005",
          "name": "Privacy-Preserving Machine Learning (PPML) Techniques",
          "description": "Employ a range of advanced cryptographic and statistical techniques during AI model training, fine-tuning, and inference to protect the privacy of sensitive information within datasets. These methods aim to prevent the leakage of individual data records, membership inference, or the reconstruction of sensitive inputs from model outputs.",
          "pillar": "data",
          "phase": "building",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0024.000: Exfiltration via AI Inference API: Infer Training Data Membership",
                "AML.T0024.001: Exfiltration via AI Inference API: Invert AI Model",
                "AML.T0025 Exfiltration via Cyber Means",
                "AML.T0057: LLM Data Leakage"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Attacks on Decentralized Learning (Cross-Layer)",
                "Data Exfiltration (L2)",
                "Membership Inference Attacks (L1)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM02:2025 Sensitive Information Disclosure"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML03:2023 Model Inversion Attack",
                "ML04:2023 Membership Inference Attack",
                "ML07:2023 Transfer Learning Attack"
              ]
            }
          ],
          "implementationStrategies": [],
          "toolsOpenSource": [],
          "toolsCommercial": [],
          "subTechniques": [
            {
              "id": "AID-H-005.001",
              "name": "Differential Privacy for AI",
              "description": "Implements differential privacy mechanisms to add calibrated noise to model training, outputs, or data queries, ensuring that individual data points cannot be identified while maintaining overall utility.",
              "pillar": "data",
              "phase": "building",
              "implementationStrategies": [
                "Implement Differentially Private Stochastic Gradient Descent (DP-SGD) for model training.",
                "Use gradient clipping and noise addition to bound sensitivity of updates.",
                "Apply output perturbation techniques adding calibrated noise to model predictions.",
                "Manage privacy budget (epsilon, delta) across multiple queries or training epochs.",
                "Implement local differential privacy for distributed data collection scenarios."
              ],
              "toolsOpenSource": [
                "PyTorch Opacus",
                "TensorFlow Privacy",
                "Google DP Library",
                "OpenDP"
              ],
              "toolsCommercial": [
                "Gretel.ai",
                "Immuta",
                "Sarus"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0024.000: Exfiltration via AI Inference API: Infer Training Data Membership"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM02:2025 Sensitive Information Disclosure"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML04:2023 Membership Inference Attack",
                    "ML03:2023 Model Inversion Attack"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "membership inference",
                  "data extraction",
                  "privacy attack",
                  "attack surface",
                  "gradient",
                  "exploitation",
                  "vulnerability",
                  "misconfiguration",
                  "bypass"
                ],
                "defense": [
                  "differential privacy",
                  "differential",
                  "privacy",
                  "private",
                  "budget",
                  "hardening",
                  "protection",
                  "prevention",
                  "secure",
                  "robust",
                  "resilient",
                  "mitigation",
                  "defense",
                  "control"
                ]
              }
            },
            {
              "id": "AID-H-005.002",
              "name": "Homomorphic Encryption for AI",
              "description": "Enables computation on encrypted data, allowing models to train or perform inference without ever decrypting sensitive information, providing strong cryptographic guarantees.",
              "pillar": "data",
              "phase": "building",
              "implementationStrategies": [
                "Implement fully homomorphic encryption (FHE) schemes for simple model architectures.",
                "Use partially homomorphic encryption for specific operations (addition, multiplication).",
                "Apply leveled homomorphic encryption for known-depth computations.",
                "Optimize encryption parameters for the specific ML workload to balance security and performance.",
                "Implement hybrid approaches combining HE with secure enclaves for complex operations."
              ],
              "toolsOpenSource": [
                "Microsoft SEAL",
                "PALISADE",
                "HElib",
                "OpenFHE"
              ],
              "toolsCommercial": [
                "Duality Technologies",
                "Enveil",
                "Zama.ai"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0025: Exfiltration via Cyber Means"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Data Exfiltration (L2)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM02:2025 Sensitive Information Disclosure"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML03:2023 Model Inversion Attack"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "model architecture",
                  "man in the middle",
                  "data exfiltration",
                  "parameters",
                  "tampering",
                  "forgery",
                  "interception",
                  "eavesdropping"
                ],
                "defense": [
                  "homomorphic encryption",
                  "homomorphic",
                  "encryption",
                  "encrypted",
                  "decrypting",
                  "cryptographic",
                  "secure",
                  "enclaves",
                  "hardening",
                  "protection",
                  "prevention",
                  "robust",
                  "resilient",
                  "mitigation",
                  "defense"
                ]
              }
            },
            {
              "id": "AID-H-005.003",
              "name": "Adaptive Data Augmentation for Membership Inference Defense",
              "description": "Employs adaptive data augmentation techniques, such as 'mixup', during the model training process to harden it against membership inference attacks (MIAs).  Mixup creates new training samples by linearly interpolating between existing samples and their labels.  The 'adaptive' component involves dynamically adjusting the mixup strategy during training, which enhances the model's generalization and makes it more difficult for an attacker to determine if a specific data point was part of the training set. ",
              "pillar": "model",
              "phase": "building",
              "implementationStrategies": [
                "Implement the core 'mixup' data augmentation function.",
                "Apply an adaptive schedule to the mixup coefficient (lambda) to decay its strength over time.",
                "Integrate the full adaptive mixup process into the model training pipeline.",
                "Evaluate MIA resistance by training an 'attack model' to test the final model."
              ],
              "toolsOpenSource": [
                "PyTorch, TensorFlow (for implementing custom data augmentation and training loops)",
                "Albumentations, torchvision.transforms (as a base for data augmentation)",
                "Adversarial Robustness Toolbox (ART) (contains specific modules for MIA attacks and defenses)",
                "MLflow (for tracking experiments and model performance)"
              ],
              "toolsCommercial": [
                "Privacy-enhancing technology platforms (Gretel.ai, Tonic.ai, SarUS, Immuta)",
                "AI Security platforms (Protect AI, HiddenLayer, Robust Intelligence, Weights & Biases)",
                "MLOps Platforms for custom training (Amazon SageMaker, Google Vertex AI, Databricks, Azure ML)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0024.000 Exfiltration via AI Inference API: Infer Training Data Membership"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Membership Inference Attacks (L1)",
                    "Data Exfiltration (L2)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM02:2025 Sensitive Information Disclosure"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML04:2023 Membership Inference Attack"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "membership inference",
                  "inference attack",
                  "training pipeline",
                  "attack surface",
                  "apt",
                  "attacker",
                  "attack",
                  "exploitation",
                  "vulnerability",
                  "misconfiguration",
                  "bypass"
                ],
                "defense": [
                  "adaptive",
                  "data",
                  "augmentation",
                  "membership",
                  "inference",
                  "defense",
                  "harden",
                  "evaluate",
                  "hardening",
                  "protection",
                  "prevention",
                  "secure",
                  "robust",
                  "resilient",
                  "mitigation"
                ]
              }
            },
            {
              "id": "AID-H-005.004",
              "name": "LLM Training Data Deduplication",
              "description": "A data-centric hardening technique that involves systematically removing duplicate or near-duplicate sequences from the training datasets for Large Language Models (LLMs). This pre-processing step directly mitigates the risk of unintended memorization, where LLMs are prone to learn and regenerate specific training examples verbatim, which can lead to the leakage of sensitive or copyrighted information. ",
              "pillar": "data",
              "phase": "building",
              "implementationStrategies": [
                "Implement exact sequence deduplication using hashing.",
                "Use MinHash LSH to detect and remove near-duplicate documents.",
                "Integrate deduplication into a large-scale data processing pipeline using frameworks like Apache Spark.",
                "Evaluate the effectiveness of deduplication by testing the final model for memorization of specific canary phrases."
              ],
              "toolsOpenSource": [
                "Apache Spark, Dask (for distributed data processing)",
                "datasketch (for MinHash LSH implementation)",
                "Pandas, NumPy (for in-memory data manipulation)",
                "Hugging Face Datasets library (for data processing features)"
              ],
              "toolsCommercial": [
                "Databricks (for large-scale Spark jobs)",
                "Snowflake (for data processing at scale)",
                "Data quality and preparation platforms (Talend, Informatica)",
                "Privacy-enhancing technology platforms (Gretel.ai, Tonic.ai)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0057 LLM Data Leakage",
                    "AML.T0024.000 Exfiltration via AI Inference API: Infer Training Data Membership"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Data Exfiltration (L2)",
                    "Data Leakage through Observability (L5)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM02:2025 Sensitive Information Disclosure"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML03:2023 Model Inversion Attack",
                    "ML04:2023 Membership Inference Attack"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "attack surface",
                  "risk",
                  "leakage",
                  "exploitation",
                  "vulnerability",
                  "misconfiguration",
                  "bypass"
                ],
                "defense": [
                  "llm",
                  "training",
                  "data",
                  "deduplication",
                  "hardening",
                  "hashing",
                  "detect",
                  "evaluate",
                  "protection",
                  "prevention",
                  "secure",
                  "robust",
                  "resilient",
                  "mitigation",
                  "defense"
                ]
              }
            }
          ],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "membership inference",
              "man in the middle",
              "attack surface",
              "leakage",
              "tampering",
              "forgery",
              "interception",
              "exploitation",
              "vulnerability",
              "misconfiguration",
              "bypass"
            ],
            "defense": [
              "privacy-preserving",
              "machine",
              "learning",
              "ppml",
              "techniques",
              "cryptographic",
              "protect",
              "privacy",
              "prevent",
              "hardening",
              "protection",
              "prevention",
              "secure",
              "robust",
              "resilient"
            ]
          }
        },
        {
          "id": "AID-H-006",
          "name": "AI Output Hardening & Sanitization",
          "description": "Implement programmatic transformations, structuring, and sanitization on the raw output generated by an AI model before it is passed to a user or downstream system. This proactive control aims to enforce a safe, expected format, remove potentially exploitable content, and reduce the risk of the output itself becoming an attack vector against end-users or other system components. This is distinct from detective output monitoring; it is a preventative measure to harden the output stream itself.",
          "pillar": "app",
          "phase": "building",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0077 LLM Response Rendering",
                "AML.T0050 Command and Scripting Interpreter (via generated code)",
                "AML.T0048 External Harms (by cleaning malicious payloads)",
                "AML.T0052 Phishing (by sanitizing malicious links)",
                "AML.T0057 LLM Data Leakage (by redacting sensitive info)"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Misinformation Generation (L1/L7)",
                "Agent Tool Misuse (L7, by structuring/sanitizing tool calls)",
                "Data Exfiltration (L2, by redacting sensitive data from outputs)",
                "Runtime Code Injection (L4, by sanitizing generated code)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM05:2025 Improper Output Handling",
                "LLM09:2025 Misinformation",
                "LLM01:2025 Prompt Injection (mitigating the impact of successful injections)",
                "LLM02:2025 Sensitive Information Disclosure"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML09:2023 Output Integrity Attack",
                "ML03:2023 Model Inversion Attack (by redacting reconstructable sensitive data)"
              ]
            }
          ],
          "implementationStrategies": [],
          "toolsOpenSource": [],
          "toolsCommercial": [],
          "subTechniques": [
            {
              "id": "AID-H-006.001",
              "name": "Structured Output Enforcement",
              "description": "Forces LLMs to generate output that conforms to a strict, pre-defined schema (e.g., JSON, YAML) instead of free-form text. This ensures the output can be safely parsed and validated by downstream systems, preventing the generation of unintended or malicious scripts, formats, or commands.",
              "pillar": "app",
              "phase": "building",
              "implementationStrategies": [
                "Use libraries like Instructor to force LLM output into a Pydantic model.",
                "Leverage native tool-use or function-calling APIs provided by LLM vendors.",
                "High-impact JSON Schema + Pydantic enforcement profile"
              ],
              "toolsOpenSource": [
                "Instructor",
                "Pydantic",
                "Guardrails AI",
                "NVIDIA NeMo Guardrails",
                "JSONformer",
                "Outlines",
                "TypeChat",
                "LangChain (Output Parsers)",
                "Native tool-use/function-calling capabilities of open-source models (Llama, Mistral, etc.)",
                "Microsoft Semantic Kernel"
              ],
              "toolsCommercial": [
                "OpenAI API (Function Calling & Tool Use)",
                "Google Vertex AI (Function Calling)",
                "Anthropic API (Tool Use)"
              ],
              "defendsAgainst": [
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM05:2025 Improper Output Handling",
                    "LLM06:2025 Excessive Agency"
                  ]
                },
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0053: AI Agent Tool Invocation",
                    "AML.T0050: Command and Scripting Interpreter"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Agent Tool Misuse (L7)",
                    "Reprogramming Attacks (L1)"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML09:2023 Output Integrity Attack"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "attack surface",
                  "malicious",
                  "exploitation",
                  "vulnerability",
                  "misconfiguration",
                  "bypass"
                ],
                "defense": [
                  "structured",
                  "output",
                  "enforcement",
                  "validated",
                  "preventing",
                  "hardening",
                  "protection",
                  "prevention",
                  "secure",
                  "robust",
                  "resilient",
                  "mitigation",
                  "defense",
                  "control"
                ]
              }
            },
            {
              "id": "AID-H-006.002",
              "name": "Output Content Sanitization & Validation",
              "description": "Applies security checks and sanitization to the content generated by an AI model before it is displayed to a user or passed to a downstream system. This involves escaping output to prevent injection attacks (e.g., Cross-Site Scripting, Shell Injection) and validating specific content types, such as URLs, against blocklists or safety APIs to prevent users from being directed to malicious websites.",
              "pillar": "app",
              "phase": "building",
              "implementationStrategies": [
                "Escape model outputs to prevent injection attacks into downstream systems.",
                "Validate all URLs in model outputs against safe Browse APIs and blocklists.",
                "Use a Web Application Firewall (WAF) with AI-specific rulesets to filter malicious outputs."
              ],
              "toolsOpenSource": [
                "Python's `html` and `shlex` libraries",
                "bleach (for HTML sanitization)",
                "sqlparse (for SQL validation)",
                "Python `ast` module (for code analysis)",
                "OWASP Java Encoder Project",
                "DOMPurify (for client-side HTML sanitization)",
                "Requests (for API calls)",
                "ModSecurity (open-source WAF)"
              ],
              "toolsCommercial": [
                "Google Safe Browse API",
                "Azure Content Safety",
                "Google Cloud DLP API",
                "Cloud WAFs (AWS WAF, Azure WAF, Cloudflare WAF, Akamai)",
                "API Security platforms (Noname Security, Salt Security)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0052: Phishing",
                    "AML.T0050: Command and Scripting Interpreter",
                    "AML.T0048: External Harms"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Misinformation Generation (Cross-Layer)",
                    "Input Validation Attacks (L3)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM05:2025 Indirect Prompt Injection"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML09:2023 Output Integrity Attack"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "malicious input",
                  "invalid input",
                  "injection",
                  "malicious",
                  "poisoning",
                  "exploit",
                  "xss",
                  "sqli",
                  "bypass"
                ],
                "defense": [
                  "output",
                  "content",
                  "sanitization",
                  "validation",
                  "prevent",
                  "validating",
                  "safety",
                  "validate",
                  "safe",
                  "firewall",
                  "waf",
                  "filter"
                ]
              }
            },
            {
              "id": "AID-H-006.003",
              "name": "Passive AI Output Obfuscation",
              "description": "A hardening technique that intentionally reduces the precision or fidelity of an AI model's output before it is returned to an end-user or downstream system. This proactive control aims to significantly increase the difficulty of model extraction, inversion, and membership inference attacks, which often rely on precise output values (like confidence scores or logits) to reverse-engineer the model or its training data. By returning less informationsuch as binned confidence scores, rounded numerical predictions, or only the top predicted classthe utility for legitimate users is maintained while the value of the output for an attacker is drastically reduced.",
              "pillar": "model",
              "phase": "building",
              "implementationStrategies": [
                "Quantize classifier outputs by binning confidence scores.",
                "Round or bucket numerical outputs from regression models.",
                "Add calibrated noise to output logits before final prediction."
              ],
              "toolsOpenSource": [
                "NumPy, SciPy (for numerical manipulation and noise generation)",
                "PyTorch, TensorFlow (for adding noise at the logit level)",
                "Custom logic within API frameworks (FastAPI, Flask, etc.)"
              ],
              "toolsCommercial": [
                "API Gateways with response transformation capabilities (Kong, Apigee, MuleSoft)",
                "AI Security Firewalls (Protect AI Guardian, Lakera Guard, CalypsoAI Validator)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0024.002 Invert AI Model / AML.T0048.004 External Harms: AI Intellectual Property Theft",
                    "AML.T0024.000 Exfiltration via AI Inference API: Infer Training Data Membership"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Model Stealing (L1)",
                    "Membership Inference Attacks (L1)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM02:2025 Sensitive Information Disclosure"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML05:2023 Model Theft",
                    "ML03:2023 Model Inversion Attack",
                    "ML04:2023 Membership Inference Attack"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "model extraction",
                  "membership inference",
                  "inference attack",
                  "attack surface",
                  "extraction",
                  "attacker",
                  "exploitation",
                  "vulnerability",
                  "misconfiguration"
                ],
                "defense": [
                  "passive",
                  "output",
                  "obfuscation",
                  "hardening",
                  "control",
                  "protection",
                  "prevention",
                  "secure",
                  "robust",
                  "resilient",
                  "mitigation",
                  "defense"
                ]
              }
            }
          ],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "malicious input",
              "stealth attack",
              "persistent threat",
              "risk",
              "attack",
              "vector",
              "end",
              "injection",
              "poisoning",
              "exploit",
              "xss",
              "sqli",
              "evasion",
              "undetected",
              "exploitation"
            ],
            "defense": [
              "output",
              "hardening",
              "sanitization",
              "control",
              "enforce",
              "safe",
              "monitoring",
              "harden",
              "protection",
              "prevention",
              "secure",
              "robust",
              "resilient",
              "mitigation",
              "defense"
            ]
          }
        },
        {
          "id": "AID-H-007",
          "name": "Secure & Resilient Training Process Hardening",
          "description": "Implement robust security measures to protect the integrity, confidentiality, and stability of the AI model training process itself. This involves securing the training environment (infrastructure, code, data access), continuously monitoring training jobs for anomalous behavior (e.g., unexpected resource consumption, convergence failures, unusual metric fluctuations that could indicate subtle data poisoning effects not caught by pre-filtering or direct manipulation of training code), and ensuring training reproducibility and auditability.",
          "pillar": "infra",
          "phase": "building",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0018.000: Manipulate AI Model: Poison AI Model",
                "AML.T0031: Erode AI Model Integrity",
                "AML.T0020: Poison Training Data (by detecting anomalous training dynamics caused by subtle poisoning)",
                "AML.T0019: Publish Poisoned Datasets (if poisoning occurs through manipulation of the training process, code, or environment rather than just static model parameters)",
                "AML.T0010: AI Supply Chain Compromise (if a compromised development tool or library specifically targets and manipulates the training loop)."
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Data Poisoning (L2: Data Operations, by monitoring its impact during training)",
                "Compromised Training Environment (L4: Deployment & Infrastructure)",
                "Resource Hijacking (L4: Deployment & Infrastructure, if training resources are targeted by malware or unauthorized processes)",
                "Training Algorithm Manipulation (L1: Foundation Models or L3: Agent Frameworks)."
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM04:2025 Data and Model Poisoning (by providing an additional layer to detect sophisticated poisoning attempts that manifest during the training process itself)."
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML02:2023 Data Poisoning Attack (detecting subtle or run-time effects)",
                "ML10:2023 Model Poisoning (if poisoning involves altering training code or runtime)."
              ]
            }
          ],
          "implementationStrategies": [],
          "toolsOpenSource": [],
          "toolsCommercial": [],
          "subTechniques": [
            {
              "id": "AID-H-007.001",
              "name": "Secure Training Environment Provisioning",
              "description": "This sub-technique focuses on the infrastructure layer of AI security. It covers the creation of dedicated, isolated, and hardened environments for training jobs using Infrastructure as Code (IaC), least-privilege IAM roles, and, where necessary, confidential computing. The goal is to build a secure foundation for the training process, protecting it from both internal and external threats, and ensuring the confidentiality and integrity of the data and model being processed.",
              "pillar": "infra",
              "phase": "building",
              "implementationStrategies": [
                "Utilize dedicated, isolated, and hardened network environments for model training activities.",
                "Apply the principle of least privilege for training jobs, granting only necessary access to data and resources.",
                "Employ confidential computing (e.g., secure enclaves) for training on highly sensitive data."
              ],
              "toolsOpenSource": [
                "Terraform, Ansible, CloudFormation, Pulumi (for IaC)",
                "Intel SGX SDK, Open Enclave SDK (for confidential computing applications)",
                "AWS CLI, gcloud, Azure CLI (for scripting infrastructure setup)"
              ],
              "toolsCommercial": [
                "Cloud Provider Confidential Computing (AWS Nitro Enclaves, Google Cloud Confidential Computing, Azure Confidential Computing)",
                "HashiCorp Terraform Enterprise, Ansible Tower (for IaC management)",
                "Cloud Security Posture Management (CSPM) tools for auditing configurations"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0025: Exfiltration via Cyber Means",
                    "AML.T0010: AI Supply Chain Compromise",
                    "AML.T0020: Poison Training Data"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Compromised Training Environment (L4)",
                    "Resource Hijacking (L4)",
                    "Lateral Movement (Cross-Layer)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM03:2025 Supply Chain",
                    "LLM02:2025 Sensitive Information Disclosure"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML06:2023 AI Supply Chain Attacks",
                    "ML05:2023 Model Theft"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "external threat",
                  "unauthorized modification",
                  "attack surface",
                  "tampering",
                  "corruption",
                  "manipulation",
                  "forgery",
                  "exploitation",
                  "vulnerability",
                  "misconfiguration",
                  "bypass"
                ],
                "defense": [
                  "least privilege",
                  "secure",
                  "training",
                  "environment",
                  "provisioning",
                  "isolated",
                  "hardened",
                  "iam",
                  "roles",
                  "confidential",
                  "protecting",
                  "confidentiality",
                  "integrity",
                  "privilege",
                  "enclaves"
                ]
              }
            },
            {
              "id": "AID-H-007.002",
              "name": "Runtime Training Job Monitoring & Auditing",
              "description": "Focuses on instrumenting the training script itself to continuously monitor for behavioral anomalies and to create a detailed, immutable audit log. This involves real-time tracking of key training metrics (e.g., loss, gradient norms) to detect signs of instability or poisoning, and systematically logging all parameters, code versions, and data versions to ensure any training run is fully auditable and reproducible.",
              "pillar": "infra",
              "phase": "operation",
              "implementationStrategies": [
                "Monitor training metrics in real time to detect anomalies that suggest poisoning, backdoor insertion, or destabilization.",
                "Enforce runtime stability checks (loss sanity, gradient clipping) and kill the job if it becomes unstable.",
                "Attach a mandatory audit trail to every training run: code version, dataset version, container image, and hyperparameters."
              ],
              "toolsOpenSource": [
                "MLflow, Kubeflow Pipelines, ClearML (for experiment tracking and logging)",
                "Prometheus, Grafana (for visualizing real-time metrics)",
                "PyTorch, TensorFlow"
              ],
              "toolsCommercial": [
                "MLOps platforms (Amazon SageMaker, Google Vertex AI Experiments, Databricks, Azure Machine Learning)",
                "AI Observability Platforms (Arize AI, Fiddler, WhyLabs)",
                "Weights & Biases"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0020 Poison Training Data",
                    "AML.T0018 Manipulate AI Model"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Evaluation & Observability (L5)",
                    "Data Poisoning (L2)",
                    "Training Algorithm Manipulation"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM04:2025 Data and Model Poisoning"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML02:2023 Data Poisoning Attack",
                    "ML10:2023 Model Poisoning"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "stealth attack",
                  "persistent threat",
                  "forensic evasion",
                  "cover tracks",
                  "hyperparameters",
                  "key",
                  "gradient",
                  "poisoning",
                  "parameters",
                  "backdoor",
                  "evasion",
                  "undetected",
                  "repudiation",
                  "tampering",
                  "unauthorized"
                ],
                "defense": [
                  "audit trail",
                  "runtime",
                  "training",
                  "job",
                  "monitoring",
                  "auditing",
                  "monitor",
                  "audit",
                  "log",
                  "tracking",
                  "key",
                  "detect",
                  "logging",
                  "enforce",
                  "container"
                ]
              }
            },
            {
              "id": "AID-H-007.003",
              "name": "Training Process Reproducibility",
              "description": "This sub-technique focuses on the governance and versioning aspect of securing the training process. It covers the strict version control of all inputs to a training jobincluding source code, configuration files, dependencies, the dataset, and the container imageto ensure any run can be perfectly and verifiably reproduced. This is critical for auditing, debugging incidents, and ensuring the integrity of the model's entire lifecycle.",
              "pillar": "infra",
              "phase": "building",
              "implementationStrategies": [
                "Strictly version control all training code, configurations, and dependencies using a single Git commit.",
                "Use Data Version Control (DVC) to version the dataset and link it to the code commit.",
                "Version control the training environment using a uniquely tagged container image.",
                "Link all versioned artifacts together in an MLOps platform for a complete audit trail."
              ],
              "toolsOpenSource": [
                "Git (for code, configs)",
                "DVC (Data Version Control), Git-LFS (for data, models)",
                "Docker, Podman (for environment containerization)",
                "Harbor (for container registries)",
                "MLflow, Kubeflow Pipelines (for orchestrating and logging)"
              ],
              "toolsCommercial": [
                "GitHub, GitLab, Bitbucket (for source control)",
                "Amazon ECR, Google Artifact Registry, Azure Container Registry (for container registries)",
                "MLOps Platforms (Databricks, Amazon SageMaker, Google Vertex AI)",
                "Docker Hub"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0010: AI Supply Chain Compromise",
                    "AML.T0031: Erode AI Model Integrity"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Evaluation & Observability (L5)",
                    "Compromised Training Environment (L4)",
                    "Supply Chain Attacks (Cross-Layer)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM03:2025 Supply Chain"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML06:2023 AI Supply Chain Attacks"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "rollback attack",
                  "unauthorized modification",
                  "forensic evasion",
                  "artifacts",
                  "tampering",
                  "corruption",
                  "manipulation",
                  "forgery",
                  "repudiation",
                  "unauthorized"
                ],
                "defense": [
                  "audit trail",
                  "version control",
                  "artifact registry",
                  "training",
                  "process",
                  "reproducibility",
                  "governance",
                  "versioning",
                  "securing",
                  "control",
                  "container",
                  "auditing",
                  "integrity",
                  "versioned",
                  "audit"
                ]
              }
            }
          ],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "data poisoning",
              "anomalous behavior",
              "unauthorized modification",
              "malicious content",
              "harmful output",
              "data exfiltration",
              "stealth attack",
              "anomalous",
              "poisoning",
              "manipulation",
              "tampering",
              "corruption",
              "forgery",
              "injection",
              "evasion"
            ],
            "defense": [
              "secure",
              "resilient",
              "training",
              "process",
              "hardening",
              "robust",
              "protect",
              "integrity",
              "confidentiality",
              "securing",
              "monitoring"
            ]
          }
        },
        {
          "id": "AID-H-008",
          "name": "Robust Federated Learning Aggregation",
          "description": "Implement and enforce secure aggregation protocols and defenses against malicious or unreliable client updates within Federated Learning (FL) architectures. This technique aims to prevent attackers controlling a subset of participating clients from disproportionately influencing, poisoning, or degrading the global model, or inferring information about other clients' data.",
          "pillar": "model",
          "phase": "building",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0020 Poison Training Data (specifically in the context of federated learning where malicious clients submit poisoned updates)",
                "AML.T0019 Publish Poisoned Datasets (where the global model is poisoned via aggregation of malicious client models)."
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Data Poisoning (L2: Data Operations, within FL setups)",
                "Model Skewing (L2: Data Operations, in FL)",
                "Attacks on Decentralized Learning (Cross-Layer)",
                "Inference Attacks against FL participants (if secure aggregation also provides confidentiality)."
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM04:2025 Data and Model Poisoning (especially relevant for distributed or federated fine-tuning/training of LLMs)."
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML02:2023 Data Poisoning Attack (specifically in FL)",
                "ML10:2023 Model Poisoning (via compromised clients in FL)."
              ]
            }
          ],
          "implementationStrategies": [],
          "toolsOpenSource": [],
          "toolsCommercial": [],
          "subTechniques": [
            {
              "id": "AID-H-008.001",
              "name": "Secure Aggregation Protocols for Federated Learning",
              "description": "Employs cryptographic methods in Federated Learning (FL) to protect the privacy of individual client contributions, such as model updates or gradients.  These protocols are designed so the central server can compute the aggregate (sum or average) of all client updates but cannot inspect or reverse-engineer any individual contribution.  This hardens the FL process against inference attacks by the server and preserves user privacy in collaborative learning environments. ",
              "pillar": "model",
              "phase": "building",
              "implementationStrategies": [
                "Use Additively Homomorphic Encryption (HE) to encrypt client updates before aggregation.",
                "Implement a Secure Multi-Party Computation (SMC) protocol for masked aggregation.",
                "Design the aggregation protocol to tolerate client dropouts without breaking privacy."
              ],
              "toolsOpenSource": [
                "TensorFlow Federated (TFF)",
                "Flower (Federated Learning Framework)",
                "PySyft (OpenMined)",
                "Microsoft SEAL, OpenFHE (for homomorphic encryption)",
                "TF-Encrypted (for secure multi-party computation)"
              ],
              "toolsCommercial": [
                "Enterprise Federated Learning platforms (Owkin, Substra (enterprise FL offering / support from Owkin/Substra), IBM)",
                "Confidential Computing platforms (AWS Nitro Enclaves, Google Cloud Confidential Computing)",
                "Privacy-enhancing technology vendors (Duality Technologies, Enveil, Zama.ai)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0025 Exfiltration via Cyber Means (protects raw client training data and gradients from being exposed to an untrusted aggregator in FL)"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Attacks on Decentralized Learning (Cross-Layer)",
                    "Data Exfiltration (L2)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM02:2025 Sensitive Information Disclosure"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML03:2023 Model Inversion Attack",
                    "ML04:2023 Membership Inference Attack"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "inference attack",
                  "man in the middle",
                  "data exfiltration",
                  "model corruption",
                  "byzantine attack",
                  "malicious participant",
                  "gradients",
                  "tampering",
                  "forgery",
                  "interception",
                  "eavesdropping",
                  "poisoning"
                ],
                "defense": [
                  "federated learning",
                  "secure aggregation",
                  "homomorphic encryption",
                  "secure",
                  "aggregation",
                  "protocols",
                  "federated",
                  "learning",
                  "cryptographic",
                  "protect",
                  "privacy",
                  "inspect",
                  "encryption",
                  "encrypt"
                ]
              }
            },
            {
              "id": "AID-H-008.002",
              "name": "Byzantine-Robust Aggregation Rules",
              "description": "A class of statistical, non-cryptographic aggregation methods designed to protect the integrity of the global model in Federated Learning. These rules identify and mitigate the impact of outlier or malicious model updates from compromised clients (Byzantine actors) by using functions like median, trimmed mean, or distance-based scoring (e.g., Krum) to filter out or down-weight anomalous contributions before they can corrupt the final aggregated model.",
              "pillar": "model",
              "phase": "building",
              "implementationStrategies": [
                "Replace standard Federated Averaging with robust statistical aggregators like Krum, Multi-Krum, or Trimmed Mean.",
                "Monitor the statistical properties of each client's updates over time to detect consistently anomalous actors.",
                "Implement client reputation / trust scoring and use it to weight or exclude updates."
              ],
              "toolsOpenSource": [
                "TensorFlow Federated (TFF)",
                "Flower (Federated Learning Framework)",
                "PySyft (OpenMined)",
                "PyTorch, TensorFlow (for implementing custom aggregation logic)",
                "NumPy, SciPy (for statistical calculations)"
              ],
              "toolsCommercial": [
                "Enterprise Federated Learning platforms (Owkin, Substra (enterprise FL offering / support from Owkin/Substra), IBM)",
                "MLOps platforms with FL capabilities (Amazon SageMaker, Google Vertex AI)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0020 Poison Training Data",
                    "AML.T0019 Publish Poisoned Datasets"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Data Poisoning (L2)",
                    "Model Skewing (L2)",
                    "Attacks on Decentralized Learning (Cross-Layer)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM04:2025 Data and Model Poisoning"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML02:2023 Data Poisoning Attack",
                    "ML10:2023 Model Poisoning"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "unauthorized modification",
                  "man in the middle",
                  "model corruption",
                  "byzantine attack",
                  "malicious",
                  "compromised",
                  "anomalous",
                  "corrupt",
                  "weight",
                  "tampering",
                  "corruption",
                  "manipulation",
                  "forgery",
                  "interception",
                  "poisoning"
                ],
                "defense": [
                  "federated learning",
                  "byzantine-robust",
                  "aggregation",
                  "rules",
                  "protect",
                  "integrity",
                  "mitigate",
                  "filter",
                  "robust",
                  "monitor",
                  "detect",
                  "trust"
                ]
              }
            }
          ],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "model corruption",
              "byzantine attack",
              "malicious participant",
              "attack surface",
              "malicious",
              "poisoning",
              "degrading",
              "exploitation",
              "vulnerability",
              "misconfiguration",
              "bypass"
            ],
            "defense": [
              "federated learning",
              "secure aggregation",
              "robust",
              "federated",
              "learning",
              "aggregation",
              "enforce",
              "secure",
              "prevent",
              "controlling"
            ]
          }
        },
        {
          "id": "AID-H-009",
          "name": "AI Accelerator & Hardware Integrity",
          "description": "Implement measures to protect the physical integrity and operational security of specialized AI hardware (GPUs, TPUs, NPUs, FPGAs) and the platforms hosting them against physical tampering, side-channel attacks (power, timing, EM), fault injection, and hardware Trojans. It also includes leakage across co-located tenants or jobs on shared accelerators (e.g. shared GPUs in inference clusters). This aims to ensure the confidentiality and integrity of AI computations and model parameters processed by the hardware.",
          "pillar": "infra",
          "phase": "building",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0010.000 AI Supply Chain Compromise: Hardware",
                "AML.T0024.002 Invert AI Model (if extraction relies on side-channel attacks against hardware)",
                "AML.T0025 Exfiltration via Cyber Means (if side-channels are the means). (Potentially new ATLAS technique: \"Exploit AI Hardware Vulnerability\" or \"AI Hardware Side-Channel Attack\")."
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Physical Tampering (L4: Deployment & Infrastructure)",
                "Side-Channel Attacks (L4: Deployment & Infrastructure / L1: Foundation Models if model parameters are leaked)",
                "Compromised Hardware Accelerators (L4)."
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM03:2025 Supply Chain (by ensuring integrity of underlying hardware components)",
                "LLM02:2025 Sensitive Information Disclosure (if disclosure is via hardware side-channels)."
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML05:2023 Model Theft (if theft is facilitated by hardware-level attacks)",
                "ML06:2023 AI Supply Chain Attacks (specifically hardware components)."
              ]
            }
          ],
          "implementationStrategies": [],
          "toolsOpenSource": [],
          "toolsCommercial": [],
          "subTechniques": [
            {
              "id": "AID-H-009.001",
              "name": "Hardware Root of Trust & Secure Boot",
              "description": "This sub-technique focuses on ensuring that the hardware and its boot-level software start in a known, trusted state. It covers the implementation and verification of Secure Boot chains for servers equipped with AI accelerators. This process establishes a chain of trust from an immutable hardware root, ensuring that every piece of software loaded during startupfrom the UEFI firmware to the bootloader and operating system kernelis cryptographically signed and verified, preventing boot-level malware.",
              "pillar": "infra",
              "phase": "building",
              "implementationStrategies": [
                "Enable Secure Boot in the server's UEFI/BIOS settings for on-premises hardware.",
                "Utilize cloud provider capabilities for shielded and trusted VMs.",
                "Implement remote attestation to cryptographically verify the integrity of a running host.",
                "Enforce the use of cryptographically signed drivers for all AI accelerators."
              ],
              "toolsOpenSource": [
                "tianocore/edk2 (open-source UEFI implementation)",
                "GRUB2, shim (for Linux Secure Boot)",
                "Keylime (a CNCF project for TPM-based remote attestation)",
                "tpm2-tools (for interacting with a TPM)"
              ],
              "toolsCommercial": [
                "Cloud Provider Services (Google Cloud Shielded VMs, Azure Trusted Launch, AWS Nitro System)",
                "Hardware Vendor Technologies (Intel Boot Guard, AMD Secure Processor)",
                "Microsoft Defender for Endpoint (can leverage hardware security features)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0017 Persistence (via bootkits/rootkits)",
                    "AML.TA0005 Execution"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Compromised Training Environment (L4)",
                    "OS/Hypervisor Level Attacks (L4)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM03:2025 Supply Chain"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML06:2023 AI Supply Chain Attacks"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "unauthorized modification",
                  "man in the middle",
                  "state",
                  "malware",
                  "tampering",
                  "corruption",
                  "manipulation",
                  "forgery",
                  "interception"
                ],
                "defense": [
                  "hardware",
                  "root",
                  "trust",
                  "secure",
                  "boot",
                  "trusted",
                  "verification",
                  "signed",
                  "verified",
                  "preventing",
                  "attestation",
                  "verify",
                  "integrity",
                  "enforce"
                ]
              }
            },
            {
              "id": "AID-H-009.002",
              "name": "Accelerator Firmware & Driver Patch Management",
              "description": "This sub-technique covers the operational lifecycle management for the software that runs directly on the AI hardware. It includes processes for monitoring for vulnerabilities in firmware and drivers for GPUs, TPUs, and other accelerators, and applying security patches in a timely, controlled manner to prevent exploitation of known vulnerabilities.",
              "pillar": "infra",
              "phase": "building",
              "implementationStrategies": [
                "Subscribe to and monitor security bulletins from hardware vendors.",
                "Regularly check for and apply updated drivers for AI accelerators in a controlled manner.",
                "Establish a secure, out-of-band process for updating hardware firmware.",
                "Maintain current drivers/firmware via automated audits against vendor bulletins.",
                "Automate vulnerability scanning for host systems to detect outdated drivers and firmware."
              ],
              "toolsOpenSource": [
                "nvidia-smi (NVIDIA System Management Interface)",
                "rocm-smi (for AMD ROCm)",
                "ipmitool (for out-of-band management)",
                "System package managers (apt, yum, dnf)",
                "OpenVAS (open-source vulnerability scanner)"
              ],
              "toolsCommercial": [
                "Vulnerability Management Platforms (Tenable Nessus, Qualys VMDR, Rapid7 InsightVM)",
                "Enterprise Patch Management (Microsoft Endpoint Configuration Manager, Ivanti)",
                "Hardware Vendor Support Portals (NVIDIA, AMD, Intel)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0010.001: AI Supply Chain Compromise: AI Software"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "OS/Hypervisor Level Attacks (L4)",
                    "Compromised Training Environment (L4)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM03:2025 Supply Chain (securing underlying host)"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML06:2023 AI Supply Chain Attacks"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "stealth attack",
                  "persistent threat",
                  "forensic evasion",
                  "vulnerabilities",
                  "exploitation",
                  "vulnerability",
                  "evasion",
                  "undetected",
                  "repudiation",
                  "tampering",
                  "unauthorized"
                ],
                "defense": [
                  "vulnerability scanning",
                  "patch management",
                  "accelerator",
                  "firmware",
                  "driver",
                  "patch",
                  "management",
                  "monitoring",
                  "controlled",
                  "prevent",
                  "monitor",
                  "updated",
                  "secure",
                  "updating",
                  "scanning"
                ]
              }
            },
            {
              "id": "AID-H-009.003",
              "name": "Hardware Supply Chain Security",
              "description": "This sub-technique focuses on the procurement and sourcing of AI hardware. It covers vetting suppliers, verifying the authenticity of components, and contractually requiring features like side-channel attack resistance. The goal is to mitigate the risk of acquiring counterfeit, tampered, or inherently vulnerable hardware components that could be used to compromise AI systems.",
              "pillar": "infra",
              "phase": "scoping",
              "implementationStrategies": [
                "Source hardware only from trusted, vetted suppliers and require a secure chain of custody.",
                "Implement a component verification process to detect counterfeit or tampered-with hardware upon receipt.",
                "Include specific security requirements, such as side-channel resistance, in procurement contracts."
              ],
              "toolsOpenSource": [
                "NIST SP 800-161 (Supply Chain Risk Management Framework)",
                "Hardware analysis tools (e.g., for examining firmware)",
                "Vendor-specific verification tools"
              ],
              "toolsCommercial": [
                "Vendor Risk Management (VRM) Platforms (SecurityScorecard, BitSight, UpGuard)",
                "Hardware Authenticity Services (provided by some major vendors)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0010.000: AI Supply Chain Compromise: Hardware"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Physical Tampering (L4)",
                    "Compromised Hardware Accelerators (L4)",
                    "Side-Channel Attacks (L4)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM03:2025 Supply Chain"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML06:2023 AI Supply Chain Attacks"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "supply chain attack",
                  "attack",
                  "risk",
                  "tampered",
                  "vulnerable",
                  "compromise",
                  "malicious",
                  "untrusted",
                  "compromised",
                  "backdoor"
                ],
                "defense": [
                  "hardware",
                  "supply",
                  "chain",
                  "security",
                  "verifying",
                  "mitigate",
                  "trusted",
                  "secure",
                  "verification",
                  "detect"
                ]
              }
            },
            {
              "id": "AID-H-009.004",
              "name": "Accelerator Isolation & VRAM/KV-Cache Hygiene",
              "description": "In shared compute environments, prevent data leakage across GPU jobs by clearing VRAM/KV-cache after tasks, partitioning accelerators, and staying patched against known issues (e.g., LeftoverLocals).",
              "pillar": "infra",
              "phase": "operation",
              "implementationStrategies": [
                "Actively zero VRAM/KV-cache between jobs; enforce GPU partitioning where available (e.g., MIG)."
              ],
              "toolsOpenSource": [
                "PyTorch",
                "CUDA Toolkit"
              ],
              "toolsCommercial": [
                "NVIDIA Multi-Instance GPU (MIG)",
                "NVIDIA Data Center GPU Manager (DCGM)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0025 Exfiltration via Cyber Means",
                    "AML.T0037 Data from Local System"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Data Exfiltration (L2)",
                    "Side-Channel Attacks (L4)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM02:2025 Sensitive Information Disclosure"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML05:2023 Model Theft"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "data leakage",
                  "lateral movement",
                  "privilege escalation",
                  "containment escape",
                  "attack surface",
                  "leakage",
                  "breakout",
                  "exploitation",
                  "vulnerability",
                  "misconfiguration",
                  "bypass"
                ],
                "defense": [
                  "accelerator",
                  "isolation",
                  "vram",
                  "kv-cache",
                  "hygiene",
                  "prevent",
                  "patched",
                  "enforce",
                  "hardening",
                  "protection",
                  "prevention",
                  "secure",
                  "robust",
                  "resilient",
                  "mitigation"
                ]
              }
            },
            {
              "id": "AID-H-009.005",
              "name": "Confidential Inference & Remote Attestation",
              "description": "Run inference in TEEs or confidential VMs to protect model weights, inputs, and KV-cache in encrypted memory. Verify enclave integrity via remote attestation before sending sensitive assets.",
              "pillar": "infra",
              "phase": "building",
              "implementationStrategies": [
                "Perform client-side attestation verification prior to provisioning models or data."
              ],
              "toolsOpenSource": [
                "Open Enclave SDK",
                "Intel SGX SDK",
                "Confidential Computing Consortium projects"
              ],
              "toolsCommercial": [
                "NVIDIA Confidential Computing",
                "Azure Attestation",
                "Google Cloud Confidential Computing"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0024.002 Invert AI Model",
                    "AML.T0025 Exfiltration via Cyber Means"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Model Stealing (L1)",
                    "Data Exfiltration (L2)",
                    "Side-Channel Attacks (L4)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM02:2025 Sensitive Information Disclosure",
                    "LLM03:2025 Supply Chain"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML05:2023 Model Theft"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "model weights",
                  "unauthorized modification",
                  "weights",
                  "memory",
                  "assets",
                  "tampering",
                  "corruption",
                  "manipulation",
                  "forgery"
                ],
                "defense": [
                  "confidential",
                  "inference",
                  "remote",
                  "attestation",
                  "protect",
                  "encrypted",
                  "verify",
                  "enclave",
                  "integrity",
                  "verification"
                ]
              }
            }
          ],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "model parameters",
              "unauthorized modification",
              "tampering",
              "injection",
              "leakage",
              "parameters",
              "corruption",
              "manipulation",
              "forgery"
            ],
            "defense": [
              "accelerator",
              "hardware",
              "integrity",
              "protect",
              "confidentiality",
              "hardening",
              "protection",
              "prevention",
              "secure",
              "robust",
              "resilient",
              "mitigation",
              "defense",
              "control"
            ]
          }
        },
        {
          "id": "AID-H-010",
          "name": "Transformer Architecture Defenses",
          "description": "Implement security measures specifically designed to mitigate vulnerabilities inherent in the Transformer architecture, such as attention mechanism manipulation, position embedding attacks, and risks associated with self-attention complexity. These defenses reduce an attacker's ability to steer model behavior by injecting or repositioning a small set of crafted tokens that dominate attention, cause targeted misclassification, or override safety behavior. They aim to protect against attacks that exploit how Transformers process and prioritize information.",
          "pillar": "model",
          "phase": "building",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0015 Evade AI Model",
                "AML.T0043 Craft Adversarial Data"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Adversarial Examples (L1)",
                "Reprogramming Attacks (L1)",
                "Input Validation Attacks (L3)",
                "Framework Evasion (L3)"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML01:2023 Input Manipulation Attack"
              ]
            }
          ],
          "implementationStrategies": [
            "Introduce robustness into the attention mechanism (e.g. noisy attention) so attackers cannot reliably force the model to lock onto a single malicious token.",
            "Harden positional encoding to resist sequence-order manipulation and prompt steering (e.g. switch from naive absolute positions to relative / rotary position bias).",
            "Regularize attention distributions with an entropy term so the model cannot be easily forced into 'all attention on the malicious token'.",
            "Adopt gated or sparsified attention variants (e.g. GAU-style gated attention) to limit the blast radius of adversarial tokens."
          ],
          "toolsOpenSource": [
            "TextAttack (for generating adversarial examples against Transformers)",
            "Libraries for implementing custom attention mechanisms (PyTorch, TensorFlow)",
            "Research code from academic papers on Transformer security."
          ],
          "toolsCommercial": [
            "AI security platforms offering model-specific vulnerability scanning.",
            "Adversarial attack simulation tools with profiles for Transformer models."
          ],
          "subTechniques": [],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "vulnerabilities",
              "manipulation",
              "embedding",
              "risks",
              "attacker",
              "tokens",
              "exploit",
              "malicious",
              "token",
              "prompt",
              "adversarial"
            ],
            "defense": [
              "blast radius",
              "vulnerability scanning",
              "transformer",
              "architecture",
              "defenses",
              "mitigate",
              "tokens",
              "safety",
              "protect",
              "robustness",
              "token",
              "harden",
              "limit"
            ]
          }
        },
        {
          "id": "AID-H-011",
          "name": "Classifier-Free Guidance Hardening",
          "description": "A set of techniques focused on hardening the Classifier-Free Guidance (CFG) mechanism in diffusion models. CFG is a core component that steers image generation towards a text prompt, but adversaries can exploit high guidance scale values to force the model to generate harmful, unsafe, or out-of-distribution content. These hardening techniques aim to control the CFG scale and its influence, preventing its misuse while preserving the model's creative capabilities. High CFG scales can overpower built-in safety conditioning and negative prompts, effectively steering the diffusion model toward disallowed or high-risk generations. This hardening treats CFG as a runtime safety-critical control surface, not just a creative knob, and enforces guardrails on how guidance is applied at inference time.",
          "pillar": "model",
          "phase": "building",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0015: Evade ML Model",
                "AML.T0048: External Harms",
                "AML.T0054: LLM Jailbreak"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Adversarial Examples (L1)",
                "Input Validation Attacks (L3)",
                "Reprogramming Attacks (L1)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM01:2025 Prompt Injection"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML01:2023 Input Manipulation Attack",
                "ML09:2023 Output Integrity Attack"
              ]
            }
          ],
          "implementationStrategies": [
            "Implement strict server-side validation and clipping of the guidance scale parameter.",
            "Apply adaptive or per-prompt guidance scaling based on prompt risk analysis.",
            "Enforce non-removable safety conditioning / negative prompts in the guidance branch.",
            "Implement alternative guidance formulations that are inherently more robust (e.g. Dynamic CFG / decaying guidance)."
          ],
          "toolsOpenSource": [
            "Hugging Face Diffusers (for implementing custom pipelines)",
            "Pydantic (for API input validation)",
            "PyTorch, TensorFlow",
            "NVIDIA NeMo Guardrails"
          ],
          "toolsCommercial": [
            "AI security firewalls (Lakera Guard, Protect AI Guardian, CalypsoAI Validator)",
            "API Gateways with advanced validation (Kong, Apigee)",
            "AI Observability platforms (Arize AI, Fiddler, WhyLabs)"
          ],
          "subTechniques": [],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "malicious input",
              "invalid input",
              "apt",
              "prompt",
              "adversaries",
              "exploit",
              "harmful",
              "misuse",
              "prompts",
              "parameter",
              "risk",
              "injection",
              "bypass",
              "exploitation",
              "vulnerability"
            ],
            "defense": [
              "input validation",
              "classifier-free",
              "guidance",
              "hardening",
              "guardrail",
              "guardrails",
              "control",
              "preventing",
              "safety",
              "validation",
              "analysis",
              "enforce",
              "robust"
            ]
          }
        },
        {
          "id": "AID-H-012",
          "name": "Graph Neural Network (GNN) Poisoning Defense",
          "description": "Implement defenses to secure Graph Neural Networks (GNNs) against data poisoning attacks that manipulate the graph structure (nodes, edges) or node features. The goal is to ensure the integrity of the graph data and the robustness of the GNN's predictions against malicious alterations.",
          "pillar": "data",
          "phase": "building",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0020 Poison Training Data"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Data Poisoning (L2)",
                "Data Tampering (L2)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM04:2025 Data and Model Poisoning"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML02:2023 Data Poisoning Attack"
              ]
            }
          ],
          "implementationStrategies": [],
          "toolsOpenSource": [],
          "toolsCommercial": [],
          "subTechniques": [
            {
              "id": "AID-H-012.001",
              "name": "Graph Data Sanitization & Provenance",
              "description": "This sub-technique covers the data-centric defenses performed on a graph before training. It focuses on analyzing the graph's structure to identify and remove anomalous nodes or edges, and on incorporating provenance information (e.g., trust scores based on data sources) to down-weight the influence of less trusted parts of the graph during model training.",
              "pillar": "data",
              "phase": "building",
              "implementationStrategies": [
                "Apply graph structure filtering and anomaly detection to identify and remove suspicious nodes or edges before training.",
                "Analyze node and edge provenance to identify and down-weight untrusted data sources."
              ],
              "toolsOpenSource": [
                "NetworkX",
                "Pandas, NumPy, SciPy",
                "PyTorch Geometric, Deep Graph Library (DGL)"
              ],
              "toolsCommercial": [
                "Graph Database & Analytics Platforms (Neo4j, TigerGraph, Memgraph)",
                "Data Quality and Governance Platforms (Alation, Collibra)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0020: Poison Training Data",
                    "AML.T0059: Erode Dataset Integrity"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Data Poisoning (L2)",
                    "Data Tampering (L2)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM04:2025 Data and Model Poisoning"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML02:2023 Data Poisoning Attack"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "untrusted data",
                  "data poisoning",
                  "supply chain attack",
                  "malicious input",
                  "malicious content",
                  "harmful output",
                  "anomalous",
                  "suspicious",
                  "untrusted",
                  "tampering",
                  "injection",
                  "poisoning",
                  "exploit",
                  "xss",
                  "sqli"
                ],
                "defense": [
                  "anomaly detection",
                  "graph",
                  "data",
                  "sanitization",
                  "provenance",
                  "analyzing",
                  "trust",
                  "trusted",
                  "filtering",
                  "detection",
                  "analyze"
                ]
              }
            },
            {
              "id": "AID-H-012.002",
              "name": "Robust GNN Training & Architecture",
              "description": "This sub-technique covers the model-centric defenses against GNN poisoning. It focuses on modifying the GNN's architecture (e.g., using robust aggregation functions) and the training process (e.g., applying regularization) to make the model itself inherently more resilient to the effects of malicious data or structural perturbations.",
              "pillar": "model",
              "phase": "building",
              "implementationStrategies": [
                "Use robust aggregation functions in GNN layers that are less sensitive to outlier nodes or malicious neighbors.",
                "Regularize the model to prevent over-reliance on a small number of influential nodes or edges."
              ],
              "toolsOpenSource": [
                "PyTorch Geometric, Deep Graph Library (DGL)",
                "PyTorch, TensorFlow",
                "MLflow (for tracking experiments with different regularizers)"
              ],
              "toolsCommercial": [
                "ML Platforms supporting GNNs (Amazon SageMaker, Google Vertex AI)",
                "AI Observability Platforms (Arize AI, Fiddler, WhyLabs)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0020: Poison Training Data",
                    "AML.T0018: Manipulate AI Model"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Data Poisoning (L2)",
                    "Backdoor Attacks (L1)",
                    "Model Skewing (L2)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "Not directly applicable"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML02:2023 Data Poisoning Attack",
                    "ML10:2023 Model Poisoning"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "attack surface",
                  "poisoning",
                  "malicious",
                  "exploitation",
                  "vulnerability",
                  "misconfiguration",
                  "bypass"
                ],
                "defense": [
                  "robust",
                  "gnn",
                  "training",
                  "architecture",
                  "resilient",
                  "prevent",
                  "hardening",
                  "protection",
                  "prevention",
                  "secure",
                  "mitigation",
                  "defense",
                  "control"
                ]
              }
            },
            {
              "id": "AID-H-012.003",
              "name": "Certified GNN Robustness",
              "description": "This sub-technique covers the advanced, formal verification approach to defending Graph Neural Networks (GNNs). It provides a mathematical guarantee that a model's prediction for a specific node will remain unchanged even if an attacker adds or removes up to a certain number of edges in the graph. This 'certified radius' of robustness represents a distinct, high-assurance implementation path against structural poisoning attacks.",
              "pillar": "model",
              "phase": "building",
              "implementationStrategies": [
                "Train a GNN using a certification-friendly method like Randomized Smoothing.",
                "Implement a certification algorithm to calculate the 'robustness radius' for a given node's prediction."
              ],
              "toolsOpenSource": [
                "PyTorch Geometric, Deep Graph Library (DGL)",
                "PyTorch, TensorFlow",
                "Research libraries for certified robustness (e.g., code accompanying papers on Randomized Smoothing for GNNs, auto-LiRPA)"
              ],
              "toolsCommercial": [
                "AI Security validation platforms (some emerging startups in this space)",
                "Formal verification services"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0020: Poison Training Data",
                    "AML.T0031: Erode AI Model Integrity"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Data Poisoning (L2)",
                    "Data Tampering (L2)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "Not directly applicable"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML02:2023 Data Poisoning Attack",
                    "ML10:2023 Model Poisoning"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "attack surface",
                  "attacker",
                  "poisoning",
                  "exploitation",
                  "vulnerability",
                  "misconfiguration",
                  "bypass"
                ],
                "defense": [
                  "certified",
                  "gnn",
                  "robustness",
                  "verification",
                  "defending",
                  "certification",
                  "hardening",
                  "protection",
                  "prevention",
                  "secure",
                  "robust",
                  "resilient",
                  "mitigation",
                  "defense",
                  "control"
                ]
              }
            }
          ],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "data poisoning",
              "unauthorized modification",
              "poisoning",
              "manipulate",
              "malicious",
              "tampering",
              "corruption",
              "manipulation",
              "forgery"
            ],
            "defense": [
              "graph",
              "neural",
              "network",
              "gnn",
              "poisoning",
              "defense",
              "secure",
              "integrity",
              "robustness",
              "hardening",
              "protection",
              "prevention",
              "robust",
              "resilient",
              "mitigation"
            ]
          }
        },
        {
          "id": "AID-H-013",
          "name": "Reinforcement Learning (RL) Reward Hacking Prevention",
          "description": "Design and implement safeguards to prevent Reinforcement Learning (RL) agents from discovering and exploiting flaws in the reward function to achieve high rewards for unintended or harmful behaviors ('reward hacking'). This also includes protecting the reward signal itself from external manipulation and ensuring that policies are aligned with intended outcomes, not just numeric reward maximization.",
          "pillar": "model",
          "phase": "building",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0048 External Harms"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Agent Goal Manipulation (L7)",
                "Compromised Agents (L7)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM06:2025 Excessive Agency"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML08:2023 Model Skewing"
              ]
            }
          ],
          "implementationStrategies": [],
          "toolsOpenSource": [],
          "toolsCommercial": [],
          "subTechniques": [
            {
              "id": "AID-H-013.001",
              "name": "Robust Reward Function Engineering",
              "description": "Directly engineer the reward function to be less exploitable. Define multiple positive goals and explicit negative penalties so that the agent cannot get a high score by doing something misaligned, unsafe, or trivial. The goal is to encode real-world intent (what good means and what bad means) into the reward surface, instead of assuming a single simplistic metric will be enough.",
              "pillar": "model",
              "phase": "building",
              "implementationStrategies": [
                "Design complex, multi-objective reward functions that balance competing goals.",
                "Introduce constraints and penalties for undesirable behaviors or states ('guardrails')."
              ],
              "toolsOpenSource": [
                "RL libraries (Stable Baselines3, RLlib, Tianshou)",
                "Gymnasium / MuJoCo for rapid reward prototyping",
                "NumPy / Pandas for reward telemetry analysis"
              ],
              "toolsCommercial": [
                "Enterprise RL platforms (AnyLogic, Microsoft Bonsai)",
                "Simulation platforms for robotics and autonomous systems"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0048 External Harms"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Agent Goal Manipulation (L7)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM06:2025 Excessive Agency"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML08:2023 Model Skewing"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "harmful output",
                  "policy violation",
                  "attack surface",
                  "jailbreak",
                  "bypass",
                  "exploitation",
                  "vulnerability",
                  "misconfiguration"
                ],
                "defense": [
                  "robust",
                  "reward",
                  "function",
                  "engineering",
                  "guardrail",
                  "guardrails",
                  "constraints",
                  "hardening",
                  "protection",
                  "prevention",
                  "secure",
                  "resilient",
                  "mitigation",
                  "defense",
                  "control"
                ]
              }
            },
            {
              "id": "AID-H-013.002",
              "name": "Human-in-the-Loop Reward Learning",
              "description": "Learn the reward function from humans instead of hand-writing it. This includes preference-based learning (humans choose which trajectory looks better) and Inverse Reinforcement Learning (IRL), where the system infers what objective an expert was optimizing. This is critical for complex or high-impact tasks where 'good behavior' is intuitive to humans but hard to specify numerically.",
              "pillar": "model",
              "phase": "building",
              "implementationStrategies": [
                "Use preference-based learning to train a reward model from human comparisons.",
                "Use Inverse Reinforcement Learning (IRL) to learn a reward function from expert demonstrations."
              ],
              "toolsOpenSource": [
                "imitation (a library for imitation learning and inverse RL)",
                "RL libraries (Stable Baselines3, RLlib)",
                "PyTorch, TensorFlow",
                "Data labeling tools (Label Studio)"
              ],
              "toolsCommercial": [
                "Enterprise RL platforms (Microsoft Bonsai)",
                "Human data labeling services (Scale AI, Appen)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0048 External Harms"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Agent Goal Manipulation (L7)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM06:2025 Excessive Agency",
                    "LLM09:2025 Misinformation"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML08:2023 Model Skewing"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "attack surface",
                  "exploitation",
                  "vulnerability",
                  "misconfiguration",
                  "bypass"
                ],
                "defense": [
                  "human-in-the-loop",
                  "reward",
                  "learning",
                  "hardening",
                  "protection",
                  "prevention",
                  "secure",
                  "robust",
                  "resilient",
                  "mitigation",
                  "defense",
                  "control"
                ]
              }
            },
            {
              "id": "AID-H-013.003",
              "name": "Potential-Based Reward Shaping",
              "description": "Apply potential-based reward shaping (PBRS) to guide exploration without accidentally teaching the agent a hacked shortcut. PBRS adds dense 'shaping' reward at each step to help the agent learn faster, but does it in a mathematically safe way that does not change the optimal policy. This reduces the need for the agent to search for weird exploits just to get signal.",
              "pillar": "model",
              "phase": "building",
              "implementationStrategies": [
                "Define a potential function (s) that estimates the 'goodness' of any given state.",
                "Wrap the environment to add the shaping term F = (s')  (s) to the base reward at each step."
              ],
              "toolsOpenSource": [
                "RL libraries (Stable Baselines3, RLlib, Tianshou)",
                "Gymnasium",
                "PyTorch, TensorFlow",
                "NumPy"
              ],
              "toolsCommercial": [
                "Enterprise RL platforms (AnyLogic, Microsoft Bonsai)",
                "MATLAB Reinforcement Learning Toolbox"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0048 External Harms"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Unpredictable agent behavior / Performance Degradation (L5)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM06:2025 Excessive Agency"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML08:2023 Model Skewing"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "attack surface",
                  "state",
                  "exploitation",
                  "vulnerability",
                  "misconfiguration",
                  "bypass"
                ],
                "defense": [
                  "potential-based",
                  "reward",
                  "shaping",
                  "safe",
                  "policy",
                  "hardening",
                  "protection",
                  "prevention",
                  "secure",
                  "robust",
                  "resilient",
                  "mitigation",
                  "defense",
                  "control"
                ]
              }
            },
            {
              "id": "AID-H-013.004",
              "name": "RL Agent Behavioral Audit & Sandbox Evaluation",
              "description": "Before deploying an RL policy (or promoting it to production), analyze the agents actual behavior and not just its numeric reward. The goal is to catch 'creative exploits' where the policy earns huge reward by doing something undesired, unsafe, or reputation-damaging, even if it technically maximizes the current reward function.",
              "pillar": "model",
              "phase": "validation",
              "implementationStrategies": [
                "Continuously log structured behavioral metrics and correlate them with reward to surface suspicious strategies."
              ],
              "toolsOpenSource": [
                "Pandas / NumPy for metric analysis",
                "MLflow or Weights & Biases for experiment tracking",
                "Gymnasium wrappers for episode logging and video capture"
              ],
              "toolsCommercial": [
                "Enterprise RL platforms (AnyLogic, Microsoft Bonsai)",
                "AI Observability platforms (Arize AI, Fiddler, WhyLabs) for behavior analytics dashboards"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0048 External Harms"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Agent Goal Manipulation (L7)",
                    "Compromised Agents (L7)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM06:2025 Excessive Agency"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML08:2023 Model Skewing"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "forensic evasion",
                  "attack surface",
                  "suspicious",
                  "repudiation",
                  "tampering",
                  "unauthorized",
                  "exploitation",
                  "vulnerability",
                  "misconfiguration",
                  "bypass"
                ],
                "defense": [
                  "agent",
                  "behavioral",
                  "audit",
                  "sandbox",
                  "evaluation",
                  "policy",
                  "analyze",
                  "log",
                  "hardening",
                  "protection",
                  "prevention",
                  "secure",
                  "robust",
                  "resilient",
                  "mitigation"
                ]
              }
            },
            {
              "id": "AID-H-013.005",
              "name": "Secure Reward Signal Integrity & Transport",
              "description": "Protect the integrity and authenticity of the reward signal itself. In many real systems, the reward is computed by an external service (simulation backend, telemetry service, rules arbiter). If an attacker can tamper with that reward in transit, the agent will learn a malicious or useless policy. This subtechnique treats the reward channel like a critical security surface: mutually authenticated transport plus cryptographic signing of the reward payload.",
              "pillar": "model",
              "phase": "operation",
              "implementationStrategies": [
                "Secure the reward channel using mutual TLS (mTLS) and per-service identities.",
                "Digitally sign the reward payload and verify it inside the agent before using it for learning."
              ],
              "toolsOpenSource": [
                "Istio / Linkerd (mTLS and workload identity enforcement)",
                "SPIFFE / SPIRE (workload identity management for services and agents)",
                "PyCA / hashlib / hmac (for signing and verifying reward payloads)"
              ],
              "toolsCommercial": [
                "Service mesh offerings in managed Kubernetes platforms",
                "Zero Trust identity platforms for workloads",
                "Enterprise RL / robotics stacks that run policy training over the network"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0048 External Harms",
                    "AML.T0018 Manipulate AI Model"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Compromised Agents (L7)",
                    "Agent Goal Manipulation (L7)",
                    "Training Data / Signal Injection (L2/L7)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM06:2025 Excessive Agency",
                    "LLM03:2025 Supply Chain (applied here as integrity of upstream control signals)"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML08:2023 Model Skewing",
                    "ML10:2023 Model Poisoning"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "unauthorized modification",
                  "man in the middle",
                  "attacker",
                  "tamper",
                  "malicious",
                  "tampering",
                  "corruption",
                  "manipulation",
                  "forgery",
                  "interception"
                ],
                "defense": [
                  "cryptographic signing",
                  "identity management",
                  "zero trust",
                  "secure",
                  "reward",
                  "signal",
                  "integrity",
                  "transport",
                  "protect",
                  "rules",
                  "policy",
                  "authenticated",
                  "cryptographic",
                  "signing",
                  "tls"
                ]
              }
            }
          ],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "attack surface",
              "exploiting",
              "flaws",
              "harmful",
              "manipulation",
              "exploitation",
              "vulnerability",
              "misconfiguration",
              "bypass"
            ],
            "defense": [
              "reinforcement",
              "learning",
              "reward",
              "hacking",
              "prevention",
              "safeguards",
              "prevent",
              "protecting",
              "policies",
              "hardening",
              "protection",
              "secure",
              "robust",
              "resilient",
              "mitigation"
            ]
          }
        },
        {
          "id": "AID-H-014",
          "name": "Proactive Data Perturbation & Watermarking",
          "description": "Intentionally altering or embedding signals into source data (images, text, etc.) before it is used or shared, in order to disrupt, trace, or defend against misuse in downstream AI systems.",
          "pillar": "data",
          "phase": "building",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0048 External Harms",
                "AML.T0024.002 Invert AI Model"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Data Operations (L2)",
                "Misinformation Generation (L1/L7)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM09:2025 Misinformation",
                "LLM02:2025 Sensitive Information Disclosure"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML09:2023 Output Integrity Attack",
                "ML05:2023 Model Theft"
              ]
            }
          ],
          "implementationStrategies": [],
          "toolsOpenSource": [],
          "toolsCommercial": [],
          "subTechniques": [
            {
              "id": "AID-H-014.001",
              "name": "Digital Content & Data Watermarking",
              "description": "This subtechnique covers the methods for embedding robust, imperceptible signals into various data types (including images, audio, video, text, and code) for the purposes of tracing provenance, detecting misuse, or identifying AI-generated content. The watermark is designed to be resilient to common transformations, allowing an owner to prove that a piece of content originated from their system even after it has been distributed or modified.",
              "pillar": "data",
              "phase": "building",
              "implementationStrategies": [
                "For image and video data, embed watermarks into the frequency domain.",
                "For text data, subtly alter word choices or token frequencies based on a secret key."
              ],
              "toolsOpenSource": [
                "OpenCV, Pillow (for images)",
                "Librosa, pydub (for audio)",
                "MarkLLM, SynthID (for text/images)",
                "Steganography libraries"
              ],
              "toolsCommercial": [
                "Digimarc, Verance, Irdeto (commercial watermarking services)",
                "Sensity AI (deepfake detection/watermarking)",
                "Truepic (for content authenticity)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0024.002 Invert AI Model",
                    "AML.T0057 LLM Data Leakage"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Model Stealing (L1)",
                    "Data Exfiltration (L2)",
                    "Misinformation Generation (L1/L7)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM02:2025 Sensitive Information Disclosure",
                    "LLM09:2025 Misinformation"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML05:2023 Model Theft",
                    "ML09:2023 Output Integrity Attack"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "data poisoning",
                  "supply chain attack",
                  "untrusted data",
                  "model theft",
                  "model extraction",
                  "ip theft",
                  "unauthorized use",
                  "embedding",
                  "misuse",
                  "token",
                  "secret",
                  "key",
                  "tampering"
                ],
                "defense": [
                  "digital",
                  "content",
                  "data",
                  "watermarking",
                  "robust",
                  "tracing",
                  "provenance",
                  "detecting",
                  "resilient",
                  "token",
                  "secret",
                  "key"
                ]
              }
            },
            {
              "id": "AID-H-014.002",
              "name": "Adversarial Data Cloaking",
              "description": "This subtechnique covers the approach of adding small, targeted, and often imperceptible perturbations to source data. Unlike watermarking, which aims for a signal to be robustly detected, cloaking aims to disrupt or 'cloak' the data from being effectively used by specific downstream AI models. This is a proactive defense to sabotage the utility of stolen data for malicious purposes like deepfake generation or unauthorized model training.",
              "pillar": "data",
              "phase": "building",
              "implementationStrategies": [
                "Generate perturbations that target the embedding space of common recognition or feature-extraction models.",
                "Apply cloaking as a pre-processing step before data is shared publicly."
              ],
              "toolsOpenSource": [
                "Fawkes (academic project for image cloaking)",
                "Adversarial Robustness Toolbox (ART)",
                "PyTorch, TensorFlow",
                "deepface (for accessing public feature extractors)"
              ],
              "toolsCommercial": [
                "Sensity AI (deepfake detection and prevention services)",
                "Truepic (content authenticity and provenance)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0048 External Harms",
                    "AML.T0043 Craft Adversarial Data"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Data Operations (L2)",
                    "Misinformation Generation (L1/L7)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM09:2025 Misinformation"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML09:2023 Output Integrity Attack"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "model theft",
                  "model extraction",
                  "ip theft",
                  "unauthorized use",
                  "adversarial",
                  "stolen",
                  "malicious",
                  "unauthorized",
                  "embedding"
                ],
                "defense": [
                  "adversarial",
                  "data",
                  "cloaking",
                  "detected",
                  "defense",
                  "hardening",
                  "protection",
                  "prevention",
                  "secure",
                  "robust",
                  "resilient",
                  "mitigation",
                  "control"
                ]
              }
            }
          ],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "model theft",
              "model extraction",
              "ip theft",
              "unauthorized use",
              "attack surface",
              "embedding",
              "misuse",
              "exploitation",
              "vulnerability",
              "misconfiguration",
              "bypass"
            ],
            "defense": [
              "proactive",
              "data",
              "perturbation",
              "watermarking",
              "trace",
              "defend",
              "hardening",
              "protection",
              "prevention",
              "secure",
              "robust",
              "resilient",
              "mitigation",
              "defense",
              "control"
            ]
          }
        },
        {
          "id": "AID-H-015",
          "name": "Ensemble Methods for Robustness",
          "description": "An architectural defense that improves a system's resilience by combining the predictions of multiple, independently trained AI models. An attacker must now successfully deceive a majority of the models in the ensemble to cause a misclassification, significantly increasing the difficulty of a successful evasion attack. This technique is applied at inference time and is distinct from training-time hardening methods.",
          "pillar": "model",
          "phase": "building",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0015: Evade ML Model",
                "AML.T0043: Craft Adversarial Data",
                "AML.T0018: Manipulate AI Model"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Adversarial Examples (L1)",
                "Data Poisoning (L2) (mitigates impact of isolated poisoned members)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "Not directly applicable"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML01:2023 Input Manipulation Attack",
                "ML10:2023 Model Poisoning"
              ]
            }
          ],
          "implementationStrategies": [
            "Implement a majority voting or plurality voting ensemble.",
            "Use averaging of output probabilities for a more nuanced ensemble.",
            "Implement stacked generalization (stacking) for advanced ensembles."
          ],
          "toolsOpenSource": [
            "scikit-learn (VotingClassifier, StackingClassifier)",
            "PyTorch, TensorFlow (for building the base models)",
            "MLflow (for tracking and managing multiple models in an ensemble)"
          ],
          "toolsCommercial": [
            "MLOps Platforms (Amazon SageMaker, Google Vertex AI, Databricks) that facilitate training and deploying multiple models.",
            "Some AutoML platforms offer automated ensembling as a feature."
          ],
          "subTechniques": [],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "evasion attack",
              "attack surface",
              "attacker",
              "evasion",
              "attack",
              "exploitation",
              "vulnerability",
              "misconfiguration"
            ],
            "defense": [
              "ensemble",
              "methods",
              "robustness",
              "defense",
              "resilience",
              "hardening",
              "protection",
              "prevention",
              "secure",
              "robust",
              "resilient",
              "mitigation",
              "control"
            ]
          }
        },
        {
          "id": "AID-H-016",
          "name": "Certified Defenses",
          "description": "A set of advanced techniques that provide a mathematical, provable guarantee that a model's output will not change for any input within a defined 'robustness radius'. Unlike empirical defenses like standard adversarial training, which improve resilience against known attack types, certified defenses use formal methods to prove that no attack within a certain magnitude (e.g., L-infinity norm) can cause a misclassification. This is a highly specialized task that offers the highest level of assurance against evasion attacks.",
          "pillar": "model",
          "phase": "building",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0015: Evade ML Model",
                "AML.T0043: Craft Adversarial Data",
                "AML.T0031: Erode AI Model Integrity"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Adversarial Examples (L1)",
                "Unpredictable agent behavior / Performance Degradation (L5)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "Not directly applicable"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML01:2023 Input Manipulation Attack"
              ]
            }
          ],
          "implementationStrategies": [
            "Use Interval Bound Propagation (IBP) for fast but conservative robustness certification.",
            "Use Linear Relaxation-based Perturbation Analysis (e.g., CROWN / LiRPA) for tighter certified bounds.",
            "Use Randomized Smoothing for model-agnostic certification.",
            "Track the certified robustness radius as a gated release metric."
          ],
          "toolsOpenSource": [
            "auto_LiRPA",
            "IBM Adversarial Robustness Toolbox (ART)",
            "Research / JAX-based robustness libraries",
            "PyTorch",
            "TensorFlow"
          ],
          "toolsCommercial": [
            "Model validation and AI robustness assessment platforms (e.g., Robust Intelligence)",
            "Formal verification / assurance services for safety-critical ML deployments"
          ],
          "subTechniques": [],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "evasion attack",
              "attack surface",
              "adversarial",
              "attack",
              "evasion",
              "exploitation",
              "vulnerability",
              "misconfiguration",
              "bypass"
            ],
            "defense": [
              "adversarial training",
              "certified",
              "defenses",
              "robustness",
              "resilience",
              "certification",
              "analysis",
              "track",
              "hardening",
              "protection",
              "prevention",
              "secure",
              "robust",
              "resilient",
              "mitigation"
            ]
          }
        },
        {
          "id": "AID-H-017",
          "name": "System Prompt Hardening",
          "description": "Design and maintain robust, unambiguous system prompts that clearly separate trusted instructions from untrusted content, enforce instruction hierarchy, and minimize the attack surface for prompt injection and jailbreak attempts. This technique focuses on structure (delimiters, namespaces), precedence (policy > system > developer > user), and safe inclusion of dynamic context so the agent reliably follows organizational guardrails.",
          "pillar": "app",
          "phase": "building",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0051: LLM Prompt Injection",
                "AML.T0054: LLM Jailbreak",
                "AML.T0048: External Harms"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Agent Goal Manipulation (L7)",
                "Agent Tool Misuse (L7)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM01:2025 Prompt Injection",
                "LLM06:2025 Excessive Agency"
              ]
            }
          ],
          "implementationStrategies": [
            "Use strict, namespaced delimiters to separate trusted instructions from untrusted content.",
            "Enforce an explicit instruction hierarchy and conflict resolution rule in the prompt.",
            "Quarantine untrusted inputs and tool outputs as data-only sections.",
            "Neutralize risky input patterns (encoding, quoting, and length budgets).",
            "Inject version-controlled policies into the system prompt at runtime (fail-closed, namespaced, version-pinned).",
            "Use a feature flag system for real-time policy control."
          ],
          "toolsOpenSource": [
            "Structured prompt templates (YAML / JSON)",
            "Open Policy Agent (OPA), Conftest (validation of policy blocks before injection)",
            "Pydantic / JSON Schema (prompt/section schema validation)",
            "LangChain / LlamaIndex callback & template systems"
          ],
          "toolsCommercial": [
            "Configuration stores (AWS AppConfig, Azure App Configuration, HashiCorp Consul)",
            "Feature flag platforms (LaunchDarkly, Split.io, Flagsmith)",
            "Secret & key management (AWS KMS, Azure Key Vault, GCP KMS) for signing/verification of policy files"
          ],
          "subTechniques": [],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "prompt injection",
              "jailbreak attempt",
              "system prompt",
              "prompt",
              "prompts",
              "untrusted",
              "attack",
              "injection",
              "jailbreak",
              "context",
              "risky",
              "inject"
            ],
            "defense": [
              "key management",
              "system",
              "prompt",
              "hardening",
              "guardrail",
              "guardrails",
              "robust",
              "trusted",
              "enforce",
              "policy",
              "safe",
              "rule",
              "quarantine",
              "budgets",
              "policies"
            ]
          }
        },
        {
          "id": "AID-H-018",
          "name": "Secure Agent Architecture",
          "description": "This technique covers the secure-by-design architectural principles for building autonomous AI agents. It focuses on proactively designing the agent's core componentssuch as its reasoning loop, tool-use mechanism, state management, and action dispatchersto be inherently more robust, auditable, and resistant to manipulation. This is distinct from monitoring agent behavior; it is about building the agent securely from the ground up.",
          "pillar": "app",
          "phase": "building",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0050: Command and Scripting Interpreter",
                "AML.T0049: Exploit Public-Facing Application",
                "AML.T0048: External Harms"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Agent Goal Manipulation (L7)",
                "Agent Tool Misuse (L7)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM06:2025 Excessive Agency",
                "LLM05:2025 Improper Output Handling",
                "LLM03:2025 Supply Chain (by securing the tool interface)"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML06:2023 ML Supply Chain Attacks",
                "ML09:2023 Output Integrity Attack"
              ]
            }
          ],
          "implementationStrategies": [],
          "toolsOpenSource": [],
          "toolsCommercial": [],
          "subTechniques": [
            {
              "id": "AID-H-018.001",
              "name": "Interruptible & Auditable Reasoning Loops",
              "description": "Architect an agent's main operational loop (e.g., ReAct) as a state machine or generator that yields control after each discrete step. This design makes the agent's 'thought process' observable, allowing an external orchestrator to inspect, audit, and potentially interrupt or require human approval for high-risk actions before they are executed.",
              "pillar": "app",
              "phase": "building",
              "implementationStrategies": [
                "Implement a step-wise agent executor that yields control for external validation."
              ],
              "toolsOpenSource": [
                "Agentic frameworks with callback handlers (LangChain, LlamaIndex, AutoGen)",
                "Queuing systems (RabbitMQ, Redis Pub/Sub) for holding tasks for approval",
                "Feature flag services (Flagsmith, Unleash)",
                "Workflow Orchestrators (Apache Airflow, Prefect, Kubeflow Pipelines)"
              ],
              "toolsCommercial": [
                "SOAR platforms (Palo Alto XSOAR, Splunk SOAR) for orchestrating approvals",
                "Incident Management platforms (PagerDuty)",
                "Commercial Feature Flag services (LaunchDarkly, Split.io)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0048: External Harms"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Agent Goal Manipulation (L7)",
                    "Agent Tool Misuse (L7)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM06:2025 Excessive Agency"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "malicious input",
                  "invalid input",
                  "forensic evasion",
                  "state",
                  "injection",
                  "bypass",
                  "exploit",
                  "repudiation",
                  "tampering",
                  "unauthorized"
                ],
                "defense": [
                  "interruptible",
                  "auditable",
                  "reasoning",
                  "loops",
                  "control",
                  "observable",
                  "inspect",
                  "audit",
                  "validation",
                  "hardening",
                  "protection",
                  "prevention",
                  "secure",
                  "robust",
                  "resilient"
                ]
              }
            },
            {
              "id": "AID-H-018.002",
              "name": "Least-Privilege Tool Architecture",
              "description": "Design an agent's capabilities with the principle of least privilege by providing small, single-purpose tools with strongly-typed parameters instead of generic, powerful tools (e.g., a raw SQL executor). This reduces the attack surface and minimizes potential damage from tool misuse.",
              "pillar": "app",
              "phase": "building",
              "implementationStrategies": [
                "Design granular, single-purpose tools instead of overly-permissive ones."
              ],
              "toolsOpenSource": [
                "Pydantic (for defining typed tool inputs)",
                "JSON Schema (for a language-agnostic way to define tool parameters)",
                "FastAPI, Flask (for wrapping tools as secure microservices)",
                "Microsoft Semantic Kernel"
              ],
              "toolsCommercial": [
                "API Gateway solutions (Kong, Apigee, AWS API Gateway) for managing tool access"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0050: Command and Scripting Interpreter",
                    "AML.T0049: Exploit Public-Facing Application"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Agent Tool Misuse (L7)",
                    "Input Validation Attacks (L3)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM06:2025 Excessive Agency",
                    "LLM05:2025 Improper Output Handling"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "tool misuse",
                  "attack surface",
                  "parameters",
                  "attack",
                  "misuse",
                  "exploitation",
                  "vulnerability",
                  "misconfiguration",
                  "bypass"
                ],
                "defense": [
                  "least privilege",
                  "least-privilege",
                  "architecture",
                  "privilege",
                  "hardening",
                  "protection",
                  "prevention",
                  "secure",
                  "robust",
                  "resilient",
                  "mitigation",
                  "defense",
                  "control"
                ]
              }
            },
            {
              "id": "AID-H-018.003",
              "name": "Decoupled Plan-Then-Execute Architecture",
              "description": "Architect the agent so that all high-level reasoning and task planning happen in a non-side-effecting LLM phase, separate from any code or tool execution. The LLM is restricted to outputting a structured plan (for example JSON with action names and parameters) and never calls tools directly. A deterministic Action Selector or Orchestrator then validates, transforms, and dispatches this plan against explicit safety policies and allowlists before invoking real tools. This combines planning and explanation separation from side-effectful actions, making it harder for hallucinated or injected instructions to directly reach infrastructure or sensitive APIs.",
              "pillar": "app",
              "phase": "building",
              "implementationStrategies": [
                "Implement an 'Action Selector' middleware to gatekeep LLM plans."
              ],
              "toolsOpenSource": [
                "FastAPI / Flask (middleware logic)",
                "Open Policy Agent (OPA) (for action selection rules)",
                "Pydantic / pydantic-core (for schema enforcement)",
                "LangChain / LangGraph (custom chains and graph-based orchestration)"
              ],
              "toolsCommercial": [
                "Kong API Gateway (for dispatch routing)",
                "Styra DAS"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0050: Command and Scripting Interpreter",
                    "AML.T0051: LLM Prompt Injection",
                    "AML.T0054: LLM Jailbreak"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Agent Tool Misuse (L7)",
                    "Input Validation Attacks (L3)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM06:2025 Excessive Agency",
                    "LLM05:2025 Improper Output Handling",
                    "LLM01:2025 Prompt Injection"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "tool execution",
                  "attack surface",
                  "parameters",
                  "exploitation",
                  "vulnerability",
                  "misconfiguration",
                  "bypass"
                ],
                "defense": [
                  "decoupled",
                  "plan-then-execute",
                  "architecture",
                  "safety",
                  "policies",
                  "hardening",
                  "protection",
                  "prevention",
                  "secure",
                  "robust",
                  "resilient",
                  "mitigation",
                  "defense",
                  "control"
                ]
              }
            },
            {
              "id": "AID-H-018.004",
              "name": "Transient & Isolated State Management",
              "description": "Harden an agent's memory against persistent attacks by treating its short-term memory as ephemeral and isolated per session. Agents should be designed to be as stateless as possible, reloading their core, signed mission objectives at the start of each new interaction to prevent malicious instructions from being carried over between tasks.",
              "pillar": "app",
              "phase": "building",
              "implementationStrategies": [
                "Implement stateless agents with ephemeral memory for each task."
              ],
              "toolsOpenSource": [
                "LangChain memory modules (e.g., ConversationBufferWindowMemory)",
                "In-memory caches (Redis, Memcached) for session state management"
              ],
              "toolsCommercial": [
                "Managed caching services (AWS ElastiCache, Google Cloud Memorystore, Azure Cache for Redis)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0018.001: Manipulate AI Model: Poison LLM Memory"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Agent Goal Manipulation (L7)",
                    "Data Poisoning (L2)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM01:2025 Prompt Injection",
                    "LLM04:2025 Data and Model Poisoning"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "attack surface",
                  "state",
                  "memory",
                  "malicious",
                  "exploitation",
                  "vulnerability",
                  "misconfiguration",
                  "bypass"
                ],
                "defense": [
                  "transient",
                  "isolated",
                  "state",
                  "management",
                  "harden",
                  "session",
                  "signed",
                  "prevent",
                  "hardening",
                  "protection",
                  "prevention",
                  "secure",
                  "robust",
                  "resilient",
                  "mitigation"
                ]
              }
            },
            {
              "id": "AID-H-018.005",
              "name": "Agent Behavior Certification & Runtime Enforcement",
              "description": "Codify an agent's authorized capabilities (tools, file scopes, network access) into a signed, machine-readable manifest (the 'Behavior Certificate'). This certificate is then enforced at runtime by middleware hooks that intercept and validate every agent action on a deny-by-default basis, providing a concrete and auditable contract for agent behavior.",
              "pillar": "app",
              "phase": "building",
              "implementationStrategies": [
                "Define agent capabilities in a structured, signed manifest (Behavior Certificate).",
                "Implement a runtime enforcement gate to validate actions against the certificate."
              ],
              "toolsOpenSource": [
                "GnuPG, pyca/cryptography (for signing certificates)",
                "Open Policy Agent (OPA) (for complex enforcement logic)",
                "JSON Schema, Pydantic (for defining certificate structure)"
              ],
              "toolsCommercial": [
                "Cloud Provider KMS (AWS KMS, Azure Key Vault) for signing operations",
                "Enterprise Policy Management (Styra DAS)",
                "CI/CD platforms with secret management for signing keys"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0050: Command and Scripting Interpreter",
                    "AML.T0048: External Harms"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Agent Tool Misuse (L7)",
                    "Agent Goal Manipulation (L7)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM06:2025 Excessive Agency"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "forensic evasion",
                  "attack surface",
                  "certificate",
                  "repudiation",
                  "tampering",
                  "unauthorized",
                  "exploitation",
                  "vulnerability",
                  "misconfiguration",
                  "bypass"
                ],
                "defense": [
                  "secret management",
                  "agent",
                  "behavior",
                  "certification",
                  "runtime",
                  "enforcement",
                  "authorized",
                  "scopes",
                  "signed",
                  "certificate",
                  "enforced",
                  "validate"
                ]
              }
            },
            {
              "id": "AID-H-018.006",
              "name": "Agent Capability Discovery & Delegation Control",
              "description": "In multi-agent systems (for example those using MCP or custom A2A protocols), validate not only the identity of an agent but also its authorized capabilities before accepting delegated tasks. Use signed capability manifests or JWT-SVID style tokens to assert which tools and data scopes an agent is allowed to access, and enforce hop limits and loop detection on delegation chains. This prevents low-privilege or compromised agents from falsely advertising high-privilege abilities or triggering unbounded recursive delegation.",
              "pillar": "app",
              "phase": "operation",
              "implementationStrategies": [
                "Enforce signed Capability Manifests for agent registration and discovery.",
                "Implement 'Hop Limits' and 'Trace IDs' in inter-agent messages to prevent recursive DoS."
              ],
              "toolsOpenSource": [
                "SPIFFE/SPIRE (for identity and attribute attestation)",
                "Open Policy Agent (OPA) (for delegation policy)",
                "gRPC Interceptors (for hop limit enforcement)",
                "Model Context Protocol (MCP) SDKs"
              ],
              "toolsCommercial": [
                "Service Mesh (Istio, Linkerd) with custom attributes",
                "API Gateways (Kong, Apigee) with inter-service routing policies"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0073 Impersonation",
                    "AML.T0029 Denial of AI Service"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Inaccurate Agent Capability Description (L7)",
                    "Agent Identity Attack (L7)",
                    "Orchestration Attacks (L4)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM06:2025 Excessive Agency"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML09:2023 Output Integrity Attack"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "attack surface",
                  "tokens",
                  "compromised",
                  "evasion",
                  "bypass",
                  "stealth",
                  "obfuscation",
                  "hiding",
                  "exploitation",
                  "vulnerability",
                  "misconfiguration"
                ],
                "defense": [
                  "agent",
                  "capability",
                  "discovery",
                  "delegation",
                  "control",
                  "validate",
                  "authorized",
                  "signed",
                  "tokens",
                  "scopes",
                  "enforce",
                  "limits",
                  "detection",
                  "trace",
                  "ids"
                ]
              }
            },
            {
              "id": "AID-H-018.007",
              "name": "Dual-LLM Isolation Pattern",
              "description": "Implement a Privileged-LLM (P-LLM) and Quarantined-LLM (Q-LLM) split to isolate untrusted data processing from privileged planning and tool execution. The Q-LLM is the only component allowed to directly read raw untrusted content (web pages, emails, retrieved documents, tool outputs). The P-LLM can plan and request tool actions, but it must never see raw untrusted content; it only consumes Q-LLM's structured, validated outputs. This pattern reduces the impact of indirect prompt injection: even if untrusted content contains malicious instructions, they are confined to the Q-LLM, which has no tool privileges. <p>This privileged/quarantined separation pattern is formally described as the CaMeL (CApabilities for MachinE Learning) architecture by Google DeepMind, utilizing a dedicated coding/parsing model to isolate control flow from untrusted data.</p>",
              "pillar": "app",
              "phase": "building",
              "implementationStrategies": [
                "Enforce a hard trust boundary: Q-LLM reads raw untrusted content and outputs validated structured data; P-LLM plans/actions using only that structured data, never raw content."
              ],
              "toolsOpenSource": [
                "Pydantic (structured outputs + validation)",
                "jsonschema (schema validation)",
                "Envoy / API Gateway (routing and policy enforcement)",
                "Service mesh (Istio/Linkerd) (network isolation)",
                "LangChain / LangGraph (separating planning vs execution flows)"
              ],
              "toolsCommercial": [
                "LLM gateways (routing, policy, observability)",
                "Cloud KMS / Secrets Manager (key and secret isolation)",
                "Enterprise service mesh offerings (policy enforcement and telemetry)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0051: LLM Prompt Injection",
                    "AML.T0051.001: LLM Prompt Injection: Indirect",
                    "AML.T0054: LLM Jailbreak"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM01:2025 Prompt Injection",
                    "LLM02:2025 Sensitive Information Disclosure",
                    "LLM06:2025 Excessive Agency"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Compromised RAG Pipelines (L2)",
                    "Input Validation Attacks (L3)",
                    "Data Leakage (Cross-Layer)"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "prompt injection",
                  "indirect prompt injection",
                  "direct prompt injection",
                  "untrusted data",
                  "tool execution",
                  "lateral movement",
                  "privilege escalation",
                  "containment escape",
                  "untrusted",
                  "prompt",
                  "injection",
                  "malicious",
                  "breakout"
                ],
                "defense": [
                  "policy enforcement",
                  "dual-llm",
                  "isolation",
                  "pattern",
                  "isolate",
                  "validated",
                  "privileges",
                  "quarantined",
                  "control",
                  "enforce",
                  "trust"
                ]
              }
            }
          ],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "stealth attack",
              "persistent threat",
              "forensic evasion",
              "state",
              "manipulation",
              "evasion",
              "undetected",
              "repudiation",
              "tampering",
              "unauthorized"
            ],
            "defense": [
              "secure",
              "agent",
              "architecture",
              "robust",
              "monitoring",
              "hardening",
              "protection",
              "prevention",
              "resilient",
              "mitigation",
              "defense",
              "control"
            ]
          }
        },
        {
          "id": "AID-H-019",
          "name": "Tool Authorization & Capability Scoping",
          "description": "Establish and enforce strict authorization and capability limits for tools invocable by an AI agent. Apply least privilege with allowlists, parameter boundaries, and mandatory structured inputs/outputs to constrain agent capabilities and prevent unauthorized or dangerous operations even under prompt manipulation.",
          "pillar": "app",
          "phase": "building",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0053 AI Agent Tool Invocation"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Agent Tool Misuse (L7)",
                "Privilege Escalation (L6)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM06:2025 Excessive Agency",
                "LLM05:2025 Improper Output Handling"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML09:2023 Output Integrity Attack"
              ]
            }
          ],
          "implementationStrategies": [],
          "toolsOpenSource": [],
          "toolsCommercial": [],
          "subTechniques": [
            {
              "id": "AID-H-019.001",
              "name": "Tool Parameter Constraint & Schema Validation",
              "description": "Define strict, machine-readable schemas for each agent tool's input parameters and enforce validation before execution (types, formats, ranges, enums). Prevents malicious parameter injection (command/SQL injection, path traversal).",
              "pillar": "app",
              "phase": "building",
              "implementationStrategies": [
                "Use Pydantic or JSON Schema to enforce tool parameter schemas at dispatch time.",
                "Policy checks + high-impact escalation"
              ],
              "toolsOpenSource": [
                "Pydantic",
                "JSON Schema",
                "Instructor",
                "Microsoft TypeChat"
              ],
              "toolsCommercial": [
                "Kong API Gateway",
                "Apigee"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0053 AI Agent Tool Invocation"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Agent Tool Misuse (L7)",
                    "Input Validation Attacks (L3)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM06:2025 Excessive Agency",
                    "LLM05:2025 Improper Output Handling"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML09:2023 Output Integrity Attack"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "malicious input",
                  "invalid input",
                  "parameter",
                  "parameters",
                  "malicious",
                  "injection",
                  "bypass",
                  "exploit"
                ],
                "defense": [
                  "parameter",
                  "constraint",
                  "schema",
                  "validation",
                  "enforce",
                  "policy",
                  "hardening",
                  "protection",
                  "prevention",
                  "secure",
                  "robust",
                  "resilient",
                  "mitigation",
                  "defense",
                  "control"
                ]
              }
            },
            {
              "id": "AID-H-019.002",
              "name": "Policy-Based Access Control",
              "description": "Externalize authorization decisions for tool usage with a policy engine (e.g., OPA), enabling context-aware, stateful rules that decouple policy from application code.",
              "pillar": "app",
              "phase": "building",
              "implementationStrategies": [
                "Use Open Policy Agent (OPA) or Casbin to decide allow/deny for tool calls based on role, time, data sensitivity."
              ],
              "toolsOpenSource": [
                "Open Policy Agent (OPA)",
                "Casbin"
              ],
              "toolsCommercial": [
                "Styra DAS",
                "Permit.io"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0053 AI Agent Tool Invocation",
                    "AML.T0012 Valid Accounts"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Agent Tool Misuse (L7)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM06:2025 Excessive Agency",
                    "LLM05:2025 Improper Output Handling"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML09:2023 Output Integrity Attack"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "unauthorized access",
                  "privilege escalation",
                  "lateral movement",
                  "attack surface",
                  "bypass",
                  "exploitation",
                  "vulnerability",
                  "misconfiguration"
                ],
                "defense": [
                  "access control",
                  "policy-based",
                  "access",
                  "control",
                  "authorization",
                  "policy",
                  "rules",
                  "deny",
                  "role",
                  "hardening",
                  "protection",
                  "prevention",
                  "secure",
                  "robust",
                  "resilient"
                ]
              }
            },
            {
              "id": "AID-H-019.003",
              "name": "High-Impact Two-Channel Validator",
              "description": "Before executing high-impact tool calls (payments, infra changes, code execution, knowledge-base/memory writes), require a second, independent validation channel (separate model family or rules/OPA bundle) to affirm goal alignment, policy compliance, evidence sufficiency, and bounded blast radius. Deny or auto-degrade if validator confidence is below threshold or violations exist.",
              "pillar": "app",
              "phase": "validation",
              "implementationStrategies": [
                "Validator microservice (independent channel) + policy gate"
              ],
              "toolsOpenSource": [
                "Open Policy Agent (OPA) / Rego bundles",
                "Kyverno",
                "ajv / Pydantic (struct validation around the validator call)"
              ],
              "toolsCommercial": [
                "Google Vertex AI Safety/Evaluation",
                "AWS Bedrock Guardrails",
                "Azure AI Content Safety",
                "Protect AI Platform (policy/scans)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0053 AI Agent Tool Invocation"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Agent Tool Misuse (L7)",
                    "Goal Manipulation (L7)",
                    "Data Poisoning (L2)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM01:2025 Prompt Injection",
                    "LLM06:2025 Excessive Agency",
                    "LLM05:2025 Improper Output Handling"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML09:2023 Output Integrity Attack"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "code execution",
                  "malicious input",
                  "invalid input",
                  "attack surface",
                  "memory",
                  "injection",
                  "bypass",
                  "exploit",
                  "exploitation",
                  "vulnerability",
                  "misconfiguration"
                ],
                "defense": [
                  "blast radius",
                  "high-impact",
                  "two-channel",
                  "validator",
                  "validation",
                  "rules",
                  "policy",
                  "compliance",
                  "deny",
                  "threshold"
                ]
              }
            },
            {
              "id": "AID-H-019.004",
              "name": "Intent-Based Dynamic Capability Scoping",
              "description": "Dynamically restrict an agent's tool capabilities per request (or per session) based on the user's initial intent, instead of granting a broad, static role-based allowlist. The system derives a minimal set of allowed tools and action budgets before execution and enforces it in a tamper-proof way using a signed session scope. This prevents prompt injection from expanding an agent's effective privileges: even if the model \"decides\" to call a disallowed tool, the runtime enforcement layer blocks it. <p>This approach aligns with the capability-based defense strategy proposed in Google DeepMind's CaMeL (CApabilities for MachinE Learning), designed to prevent prompt injection from expanding runtime privileges.</p>",
              "pillar": "app",
              "phase": "building",
              "implementationStrategies": [
                "Generate a minimal, signed per-request capability scope from the trusted user query, then enforce it at the tool dispatcher (deny-by-default)."
              ],
              "toolsOpenSource": [
                "Open Policy Agent (OPA)",
                "Cedar (policy language for authorization)",
                "python-jose / jwcrypto (JWS/JWT signing and verification)",
                "PyJWT (JWT handling; prefer JWS libraries for signed scopes)",
                "cryptography (Ed25519/RSA signature verification)",
                "Envoy Proxy (L7 policy / enforcement building block)",
                "LangChain / LangGraph (tool dispatch hooks)",
                "Microsoft Semantic Kernel (function invocation filters)"
              ],
              "toolsCommercial": [
                "Cloud KMS / HSM (AWS KMS, Google Cloud KMS, Azure Key Vault) for signing keys",
                "API Gateway policy enforcement (Apigee, Kong Enterprise, AWS API Gateway, Azure API Management)",
                "Service mesh platforms with centralized policy management (vendor distributions)",
                "LLM security gateways (vendor products) with tool-policy enforcement"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0051: LLM Prompt Injection",
                    "AML.T0051.000: LLM Prompt Injection: Direct",
                    "AML.T0051.001: LLM Prompt Injection: Indirect",
                    "AML.T0054: LLM Jailbreak"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Agent Tool Misuse (L7)",
                    "Agent Goal Manipulation (L7)",
                    "Framework Evasion (L3)",
                    "Privilege Escalation (Cross-Layer)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM01:2025 Prompt Injection",
                    "LLM06:2025 Excessive Agency"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "prompt injection",
                  "attack surface",
                  "prompt",
                  "injection",
                  "exploitation",
                  "vulnerability",
                  "misconfiguration",
                  "bypass"
                ],
                "defense": [
                  "policy enforcement",
                  "intent-based",
                  "dynamic",
                  "capability",
                  "scoping",
                  "session",
                  "allowlist",
                  "budgets",
                  "signed",
                  "scope",
                  "privileges",
                  "enforcement",
                  "defense",
                  "prevent",
                  "trusted"
                ]
              }
            },
            {
              "id": "AID-H-019.005",
              "name": "Value-Level Capability Metadata & Data Flow Sink Enforcement",
              "description": "Enforce data-flow safety for agentic systems by tracking the provenance and sensitivity of runtime values (e.g., tool outputs, RAG chunks, web content) and blocking unsafe transfers into sensitive sinks (e.g., outbound email, external HTTP requests, database writes). This closes a critical 'data flow diversion' gap: even if an attacker cannot change which tool is called, they may still manipulate tool parameters (e.g., recipients, URLs, file targets) to exfiltrate data. This sub-technique complements role-based allowlists and parameter schema validation by answering a different question: not only 'Is this parameter well-formed?' but also 'Should this value be allowed to flow to this destination?'.",
              "pillar": "app",
              "phase": "building",
              "implementationStrategies": [
                "Application-layer value tagging (taint) plus sensitive-sink checks in the tool dispatcher, backed by a versioned policy engine (OPA) and session-isolated metadata storage.",
                "Network-layer egress filtering (service mesh / firewall) to enforce outbound constraints even if application-layer checks fail, with default-deny posture and centrally managed allowlists."
              ],
              "toolsOpenSource": [
                "Open Policy Agent (OPA) / Rego (policy evaluation)",
                "Envoy Proxy (L7 policy / egress control)",
                "Istio Service Mesh (Egress Gateway)",
                "Cilium (eBPF-based network policy and observability)",
                "Kubernetes NetworkPolicy (baseline egress restrictions)"
              ],
              "toolsCommercial": [
                "Palo Alto Networks (NGFW / Prisma Access)",
                "Zscaler Internet Access (ZIA) / Cloud firewall",
                "Cloudflare Gateway / Zero Trust",
                "AWS Network Firewall / Gateway Load Balancer",
                "Azure Firewall / Private Link / NAT Gateway controls"
              ],
              "defendsAgainst": [
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM01:2025 Prompt Injection"
                  ]
                },
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0051: LLM Prompt Injection"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Data Exfiltration (L2)",
                    "Compromised RAG Pipelines (L2)",
                    "Data Tampering (L2)",
                    "Input Validation Attacks (L3)",
                    "Privilege Escalation (Cross-Layer)",
                    "Data Leakage (Cross-Layer)"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "data poisoning",
                  "supply chain attack",
                  "untrusted data",
                  "malicious input",
                  "invalid input",
                  "gap",
                  "attacker",
                  "manipulate",
                  "parameters",
                  "exfiltrate",
                  "parameter",
                  "tampering",
                  "injection",
                  "bypass",
                  "exploit"
                ],
                "defense": [
                  "zero trust",
                  "value-level",
                  "capability",
                  "metadata",
                  "data",
                  "flow",
                  "sink",
                  "enforcement",
                  "enforce",
                  "safety",
                  "tracking",
                  "provenance",
                  "blocking",
                  "validation",
                  "backed"
                ]
              }
            },
            {
              "id": "AID-H-019.006",
              "name": "Continuous Authorization Verification (Anti-TOCTOU)",
              "description": "Prevent Time-of-Check to Time-of-Use (TOCTOU) failures in agentic workflows by re-verifying authorization at execution time for each sensitive step (not only at workflow start). Bind authorization decisions to an immutable execution context (session/task/plan + delegation chain) and invalidate stale decisions on context change.",
              "pillar": "app",
              "phase": "operation",
              "implementationStrategies": [
                "Execution-time (JIT) authorization: re-check policy immediately before high-impact tool calls using a central policy engine (PDP).",
                "Short TTL + resume re-check: decisions must expire quickly and must be re-verified on workflow resume or context switch."
              ],
              "toolsOpenSource": [
                "Open Policy Agent (OPA)",
                "OpenFGA (relationship-based access control)",
                "Cedar (policy language, open source)",
                "Casbin (authorization library)",
                "Keycloak (IdP / session management)"
              ],
              "toolsCommercial": [
                "Styra DAS (OPA platform)",
                "Okta (IdP + policy integration)",
                "Auth0 (IdP + policy integration)",
                "Aserto (authorization platform)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0053: LLM Plugin Compromise"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Agent Tool Misuse (L7)",
                    "Privilege Escalation (L6)"
                  ]
                },
                {
                  "framework": "OWASP Agentic Top 10 2026",
                  "items": [
                    "ASI03: Identity and Privilege Abuse"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "privilege escalation",
                  "unauthorized access",
                  "attack surface",
                  "context",
                  "bypass",
                  "exploitation",
                  "vulnerability",
                  "misconfiguration"
                ],
                "defense": [
                  "access control",
                  "continuous",
                  "authorization",
                  "verification",
                  "anti-toctou",
                  "prevent",
                  "session",
                  "policy",
                  "hardening",
                  "protection",
                  "prevention",
                  "secure",
                  "robust",
                  "resilient",
                  "mitigation"
                ]
              }
            }
          ],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "privilege escalation",
              "unauthorized access",
              "parameter",
              "unauthorized",
              "dangerous",
              "prompt",
              "manipulation",
              "bypass"
            ],
            "defense": [
              "least privilege",
              "authorization",
              "capability",
              "scoping",
              "enforce",
              "limits",
              "privilege",
              "prevent",
              "hardening",
              "protection",
              "prevention",
              "secure",
              "robust",
              "resilient",
              "mitigation"
            ]
          }
        },
        {
          "id": "AID-H-020",
          "name": "Safe Fetch & Browser Sandbox for Agents",
          "description": "Impose strict controls on agent-initiated HTTP requests and web browsing to prevent SSRF, internal reconnaissance, and attacks via malicious web content. All external content is treated as untrusted and demoted to safe text before LLM consumption.",
          "pillar": "app",
          "phase": "building",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0049 Exploit Public-Facing Application",
                "AML.T0072 Reverse Shell"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "SSRF Attacks (L4)",
                "Lateral Movement (Cross-Layer)",
                "Compromised RAG Pipelines (L2)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM01:2025 Prompt Injection (indirect)",
                "LLM03:2025 Supply Chain (untrusted data sources)"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML06:2023 AI Supply Chain Attacks"
              ]
            }
          ],
          "implementationStrategies": [],
          "toolsOpenSource": [],
          "toolsCommercial": [],
          "subTechniques": [
            {
              "id": "AID-H-020.001",
              "name": "URL Normalization & Allowlist Filtering",
              "description": "Create a safe HTTP wrapper that normalizes URLs, enforces scheme/domain allowlists, resolves DNS and blocks private/internal IP ranges to prevent SSRF.",
              "pillar": "app",
              "phase": "building",
              "implementationStrategies": [
                "Build a \"safe_fetch\" abstraction to gatekeep all outbound requests."
              ],
              "toolsOpenSource": [
                "requests",
                "urllib.parse",
                "ipaddress"
              ],
              "toolsCommercial": [
                "Noname Security (API Security Platform)",
                "Salt Security (API Security Platform)",
                "Zscaler (Secure Web Gateway)",
                "Netskope (Secure Web Gateway)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0049 Exploit Public-Facing Application"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "SSRF Attacks (L4)",
                    "Lateral Movement (Cross-Layer)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM03:2025 Supply Chain (untrusted data sources)"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML06:2023 AI Supply Chain Attacks"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "malicious content",
                  "harmful output",
                  "data exfiltration",
                  "attack surface",
                  "injection",
                  "exploitation",
                  "vulnerability",
                  "misconfiguration",
                  "bypass"
                ],
                "defense": [
                  "url",
                  "normalization",
                  "allowlist",
                  "filtering",
                  "safe",
                  "private",
                  "prevent",
                  "hardening",
                  "protection",
                  "prevention",
                  "secure",
                  "robust",
                  "resilient",
                  "mitigation",
                  "defense"
                ]
              }
            },
            {
              "id": "AID-H-020.002",
              "name": "Secure HTML Rendering & Content Demotion",
              "description": "Strip scripts, styles, iframes, and active content; extract plain text before passing to LLM to mitigate stored XSS and indirect prompt injection.",
              "pillar": "app",
              "phase": "building",
              "implementationStrategies": [
                "Sanitize and convert HTML to text using hardened libraries and/or CDR/browser isolation."
              ],
              "toolsOpenSource": [
                "bleach",
                "BeautifulSoup4",
                "html5lib"
              ],
              "toolsCommercial": [
                "Votiro (CDR)",
                "OPSWAT MetaDefender (CDR)",
                "Glasswall (CDR)",
                "Menlo Security (Browser Isolation)",
                "Proofpoint (Browser Isolation)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0051 LLM Prompt Injection",
                    "AML.T0049 Exploit Public-Facing Application"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Compromised RAG Pipelines (L2)",
                    "Input Validation Attacks (L3)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM01:2025 Prompt Injection (indirect)",
                    "LLM05:2025 Improper Output Handling"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML01:2023 Input Manipulation Attack"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "prompt injection",
                  "indirect prompt injection",
                  "direct prompt injection",
                  "lateral movement",
                  "privilege escalation",
                  "containment escape",
                  "extract",
                  "prompt",
                  "injection",
                  "breakout"
                ],
                "defense": [
                  "secure",
                  "html",
                  "rendering",
                  "content",
                  "demotion",
                  "mitigate",
                  "sanitize",
                  "hardened",
                  "isolation",
                  "hardening",
                  "protection",
                  "prevention",
                  "robust",
                  "resilient",
                  "mitigation"
                ]
              }
            }
          ],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "external content",
              "attack surface",
              "malicious",
              "untrusted",
              "exploitation",
              "vulnerability",
              "misconfiguration",
              "bypass"
            ],
            "defense": [
              "safe",
              "fetch",
              "browser",
              "sandbox",
              "agents",
              "prevent",
              "hardening",
              "protection",
              "prevention",
              "secure",
              "robust",
              "resilient",
              "mitigation",
              "defense",
              "control"
            ]
          }
        },
        {
          "id": "AID-H-021",
          "name": "RAG Index Hygiene & Signing",
          "description": "Implement integrity and provenance controls during RAG indexing and maintenance. Cryptographically sign chunks/embeddings and weight content by source trust to prevent index poisoning and enable verification at retrieval time.",
          "pillar": "data",
          "phase": "building",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0070 RAG Poisoning",
                "AML.T0071 False RAG Entry Injection",
                "AML.T0059 Erode Dataset Integrity"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Compromised RAG Pipelines (L2)",
                "Data Poisoning (L2)",
                "Misinformation Generation (Cross-Layer)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM08:2025 Vector and Embedding Weaknesses",
                "LLM04:2025 Data and Model Poisoning"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML02:2023 Data Poisoning Attack"
              ]
            }
          ],
          "implementationStrategies": [],
          "toolsOpenSource": [],
          "toolsCommercial": [],
          "subTechniques": [
            {
              "id": "AID-H-021.001",
              "name": "Chunk-Level Integrity Signing",
              "description": "Provides fine-grained RAG data unit integrity by computing and storing cryptographic hashes or digital signatures per chunk at ingestion time, with verify-on-retrieval semantics to detect tampering before inclusion in LLM context. This micro-level integrity control complements macro-scale artifact integrity (AID-M-002.002) by protecting individual retrieval units within the vector index.",
              "pillar": "data",
              "phase": "building",
              "implementationStrategies": [
                "Hash each chunk payload and metadata at ingestion; verify integrity at retrieval time before inclusion in LLM context."
              ],
              "toolsOpenSource": [
                "hashlib",
                "GnuPG"
              ],
              "toolsCommercial": [
                "AWS KMS",
                "Azure Key Vault",
                "Google Cloud KMS"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0070 RAG Poisoning",
                    "AML.T0059 Erode Dataset Integrity"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Data Tampering (L2)",
                    "Compromised RAG Pipelines (L2)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM08:2025 Vector and Embedding Weaknesses",
                    "LLM04:2025 Data and Model Poisoning"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML02:2023 Data Poisoning Attack"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "unauthorized modification",
                  "man in the middle",
                  "tampering",
                  "context",
                  "artifact",
                  "vector",
                  "corruption",
                  "manipulation",
                  "forgery",
                  "interception"
                ],
                "defense": [
                  "digital signature",
                  "chunk-level",
                  "integrity",
                  "signing",
                  "cryptographic",
                  "detect",
                  "control",
                  "protecting",
                  "hash",
                  "verify"
                ]
              }
            },
            {
              "id": "AID-H-021.002",
              "name": "Source Reputation Weighting",
              "description": "Assign and store per-chunk reputation scores based on source trust. Re-rank retrievals by combining similarity and reputation to bias toward trusted sources.",
              "pillar": "data",
              "phase": "building",
              "implementationStrategies": [
                "Maintain a reputation lookup and apply post-retrieval re-ranking."
              ],
              "toolsOpenSource": [
                "LangChain (custom retrievers)",
                "LlamaIndex (postprocessors)"
              ],
              "toolsCommercial": [
                "Cohere Rerank",
                "Alation",
                "Collibra"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0070 RAG Poisoning",
                    "AML.T0066 Retrieval Content Crafting",
                    "AML.T0071 False RAG Entry Injection"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Data Poisoning (L2)",
                    "Misinformation Generation (Cross-Layer)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM08:2025 Vector and Embedding Weaknesses",
                    "LLM09:2025 Misinformation"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML02:2023 Data Poisoning Attack"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "attack surface",
                  "exploitation",
                  "vulnerability",
                  "misconfiguration",
                  "bypass"
                ],
                "defense": [
                  "source",
                  "reputation",
                  "weighting",
                  "trust",
                  "trusted",
                  "hardening",
                  "protection",
                  "prevention",
                  "secure",
                  "robust",
                  "resilient",
                  "mitigation",
                  "defense",
                  "control"
                ]
              }
            }
          ],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "data poisoning",
              "supply chain attack",
              "untrusted data",
              "unauthorized modification",
              "man in the middle",
              "embeddings",
              "weight",
              "poisoning",
              "tampering",
              "corruption",
              "manipulation",
              "forgery",
              "interception"
            ],
            "defense": [
              "rag",
              "index",
              "hygiene",
              "signing",
              "integrity",
              "provenance",
              "sign",
              "trust",
              "prevent",
              "verification"
            ]
          }
        },
        {
          "id": "AID-H-022",
          "name": "AI Agent Configuration Integrity & Hardening",
          "description": "This technique establishes and enforces the integrity and security of an AI agent's instructional configurations (e.g., system prompts, operational parameters, external configuration files like `CLAUDE.md`). It employs a defense-in-depth strategy, combining client-side controls to prevent the creation of insecure configurations in development environments with cryptographic verification at runtime to ensure the agent only operates based on a trusted, untampered directive. **Crucially, this technique mandates the use of structured, non-executable formats (e.g., schema-validated JSON, YAML) for configurations, prohibiting the use of formats that could be interpreted as executable code.** This directly counters attacks where adversaries manipulate an agent's behavior by injecting malicious instructions through external or modified configuration files.",
          "pillar": "infra",
          "phase": "building",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0051 LLM Prompt Injection",
                "AML.T0018 Manipulate AI Model",
                "AML.T0054 LLM Jailbreak"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Agent Goal Manipulation (L7)",
                "Reprogramming Attacks (L1)",
                "Compromised Framework Components (L3)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM01:2025 Prompt Injection",
                "LLM06:2025 Excessive Agency",
                "LLM07:2025 System Prompt Leakage",
                "LLM02:2025 Sensitive Information Disclosure"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML01:2023 Input Manipulation Attack",
                "ML06:2023 AI Supply Chain Attacks"
              ]
            }
          ],
          "implementationStrategies": [],
          "toolsOpenSource": [],
          "toolsCommercial": [],
          "subTechniques": [
            {
              "id": "AID-H-022.001",
              "name": "Client-Side Configuration Enforcement",
              "description": "Proactively prevents the creation and use of insecure AI agent configurations on developer endpoints. This is a 'shift-left' defense that uses endpoint security tools, policy-as-code scanners, and local development guardrails to block high-risk settings before they can ever be committed to source control or executed.",
              "pillar": "infra",
              "phase": "building",
              "implementationStrategies": [
                "Use EDR/XDR to block dangerous agent configurations at the endpoint.",
                "Implement a Git pre-commit hook to scan for insecure settings before they enter the repository.",
                "Enforce configuration schema validation and prohibit local overrides."
              ],
              "toolsOpenSource": [
                "Git (pre-commit hooks)",
                "EDR custom rules engines (osquery)",
                "VS Code extensions APIs",
                "Semgrep",
                "Open Policy Agent (OPA)",
                "Conftest"
              ],
              "toolsCommercial": [
                "EDR/XDR Platforms (CrowdStrike Falcon, SentinelOne, Microsoft Defender for Endpoint)",
                "Mobile Device Management (MDM) solutions (Jamf, Intune)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0051 LLM Prompt Injection",
                    "AML.T0054 LLM Jailbreak"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Agent Goal Manipulation (L7)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM01:2025 Prompt Injection",
                    "LLM06:2025 Excessive Agency"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML01:2023 Input Manipulation Attack"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "malicious input",
                  "invalid input",
                  "harmful output",
                  "policy violation",
                  "insecure",
                  "dangerous",
                  "injection",
                  "bypass",
                  "exploit",
                  "jailbreak"
                ],
                "defense": [
                  "client-side",
                  "configuration",
                  "enforcement",
                  "guardrail",
                  "guardrails",
                  "defense",
                  "block",
                  "control",
                  "scan",
                  "repository",
                  "enforce",
                  "validation"
                ]
              }
            },
            {
              "id": "AID-H-022.002",
              "name": "Runtime Integrity Enforcement (Signed Configurations)",
              "description": "Ensures that an AI agent, at the moment of execution, loads and operates on a configuration that is cryptographically verified to be authentic and untampered. This is a critical runtime check that serves as a final guardrail, protecting the agent even if client-side or repository controls fail. It forms a verifiable trust chain from the secure build pipeline to the running agent.",
              "pillar": "app",
              "phase": "operation",
              "implementationStrategies": [
                "Cryptographically sign approved configuration files in the CI/CD pipeline.",
                "Require the agent to verify the configuration signature at startup before executing.",
                "Generate verifiable supply chain attestations for agent configurations.",
                "Implement secure key management and rotation for signing keys."
              ],
              "toolsOpenSource": [
                "pyca/cryptography, GnuPG (for signing/verification)",
                "SPIFFE/SPIRE (for workload identity to fetch configs)",
                "Sigstore/cosign",
                "in-toto (for supply chain attestations)"
              ],
              "toolsCommercial": [
                "Cloud Provider Secret Managers (AWS Secrets Manager, Azure Key Vault)",
                "HashiCorp Vault, AWS/GCP/Azure KMS (for signing operations)",
                "Enterprise PKI Solutions (Venafi, DigiCert)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0018 Manipulate AI Model",
                    "AML.T0051 LLM Prompt Injection"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Reprogramming Attacks (L1)",
                    "Compromised Framework Components (L3)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM07:2025 System Prompt Leakage",
                    "LLM02:2025 Sensitive Information Disclosure"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML06:2023 AI Supply Chain Attacks"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "unauthorized modification",
                  "man in the middle",
                  "harmful output",
                  "policy violation",
                  "key",
                  "keys",
                  "tampering",
                  "corruption",
                  "manipulation",
                  "forgery",
                  "interception",
                  "jailbreak",
                  "bypass"
                ],
                "defense": [
                  "key management",
                  "secret management",
                  "runtime",
                  "integrity",
                  "enforcement",
                  "signed",
                  "configurations",
                  "guardrail",
                  "verified",
                  "protecting",
                  "repository",
                  "trust",
                  "secure",
                  "sign",
                  "verify"
                ]
              }
            }
          ],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "system prompt",
              "unauthorized modification",
              "man in the middle",
              "prompts",
              "parameters",
              "insecure",
              "adversaries",
              "manipulate",
              "malicious",
              "tampering",
              "corruption",
              "manipulation",
              "forgery",
              "interception",
              "exploitation"
            ],
            "defense": [
              "secure configuration",
              "agent",
              "configuration",
              "integrity",
              "hardening",
              "prevent",
              "cryptographic",
              "verification",
              "trusted",
              "protection",
              "prevention",
              "secure",
              "robust",
              "resilient",
              "mitigation"
            ]
          }
        },
        {
          "id": "AID-H-023",
          "name": "Secure Build & Dependency Installation Environment",
          "description": "A foundational 'shift-left' defense that treats the software dependency installation process (e.g., `npm ci`, `pip install`) as a potentially hostile, untrusted step. This technique focuses on isolating the build environment to prevent malicious installation scripts (like those used in the Shai-Hulud attack) from exfiltrating data, accessing the network, or gaining persistence on the developer machine or CI/CD runner. It ensures that even if a malicious package is inadvertently downloaded, its ability to cause harm is severely contained.",
          "pillar": "infra",
          "phase": "building",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0011.001: User Execution: Malicious Package",
                "AML.T0072: Reverse Shell",
                "AML.T0025: Exfiltration via Cyber Means"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Lateral Movement (Cross-Layer)",
                "Compromised Container Images (L4)",
                "Supply Chain Attacks (Cross-Layer)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM03:2025 Supply Chain"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML06:2023 ML Supply Chain Attacks"
              ]
            }
          ],
          "implementationStrategies": [],
          "toolsOpenSource": [],
          "toolsCommercial": [],
          "subTechniques": [
            {
              "id": "AID-H-023.001",
              "name": "Sandboxed Dependency Installation",
              "description": "Executes the dependency installation process within a strongly isolated, ephemeral environment with restricted network access and permissions, directly neutralizing the threat of malicious `postinstall` scripts.",
              "pillar": "infra",
              "phase": "building",
              "implementationStrategies": [
                "Use ephemeral, network-restricted containers for all dependency installations.",
                "Enforce strict lockfile discipline and globally disable automatic script execution.",
                "Harden internal package mirrors with a quarantine-and-promote workflow."
              ],
              "toolsOpenSource": [
                "Docker, Podman (for containerized builds)",
                "npm, pnpm, yarn, Corepack (as the package managers to be controlled)"
              ],
              "toolsCommercial": [
                "JFrog Artifactory / Xray (as the secure internal mirror/proxy)",
                "Sonatype Nexus Firewall"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0011.001: User Execution: Malicious Package",
                    "AML.T0072: Reverse Shell",
                    "AML.T0025: Exfiltration via Cyber Means"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Lateral Movement (Cross-Layer)",
                    "Supply Chain Attacks (Cross-Layer)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM03:2025 Supply Chain"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML06:2023 ML Supply Chain Attacks"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "supply chain attack",
                  "dependency confusion",
                  "vulnerable dependency",
                  "attack surface",
                  "threat",
                  "malicious",
                  "compromised",
                  "exploitation",
                  "vulnerability",
                  "misconfiguration",
                  "bypass"
                ],
                "defense": [
                  "sandboxed",
                  "dependency",
                  "installation",
                  "isolated",
                  "permissions",
                  "containers",
                  "enforce",
                  "harden",
                  "hardening",
                  "protection",
                  "prevention",
                  "secure",
                  "robust",
                  "resilient",
                  "mitigation"
                ]
              }
            },
            {
              "id": "AID-H-023.002",
              "name": "Proactive Package Vetting",
              "description": "Integrates automated security and reputation analysis into the developer workflow, providing intelligence about a package's risks *before* it is added as a dependency. This 'shift-left' approach prevents malicious packages from ever entering the codebase.",
              "pillar": "infra",
              "phase": "building",
              "implementationStrategies": [
                "Integrate package reputation and behavioral analysis tools into the Pull Request process."
              ],
              "toolsOpenSource": [
                "OpenSSF Scorecard",
                "OpenSSF Dependency Review Action",
                "Socket.dev (CLI tool)",
                "Semgrep (for static code analysis)"
              ],
              "toolsCommercial": [
                "Snyk",
                "Socket.dev (Platform)",
                "GitHub Advanced Security"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0010: AI Supply Chain Compromise",
                    "AML.T0011.001: User Execution: Malicious Package",
                    "AML.T0074: Masquerading"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Supply Chain Attacks (Cross-Layer)",
                    "Compromised Framework Components (L3)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM03:2025 Supply Chain"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML06:2023 ML Supply Chain Attacks"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "malicious package",
                  "supply chain attack",
                  "dependency confusion",
                  "vulnerable dependency",
                  "risks",
                  "malicious",
                  "compromised",
                  "untrusted",
                  "backdoor"
                ],
                "defense": [
                  "proactive",
                  "package",
                  "vetting",
                  "analysis",
                  "hardening",
                  "protection",
                  "prevention",
                  "secure",
                  "robust",
                  "resilient",
                  "mitigation",
                  "defense",
                  "control"
                ]
              }
            }
          ],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "malicious package",
              "supply chain attack",
              "dependency confusion",
              "vulnerable dependency",
              "persistence",
              "hostile",
              "untrusted",
              "malicious",
              "attack",
              "compromised"
            ],
            "defense": [
              "secure",
              "build",
              "dependency",
              "installation",
              "environment",
              "defense",
              "isolating",
              "prevent",
              "contained",
              "hardening",
              "protection",
              "prevention",
              "robust",
              "resilient",
              "mitigation"
            ]
          }
        },
        {
          "id": "AID-H-024",
          "name": "Publisher Integrity & Workflow Hardening",
          "description": "A critical supply chain defense that ensures software packages (e.g., NPM, PyPI) are published only from a secure, auditable, and authorized source. This technique breaks the attack chain where a stolen developer credential is used to publish a malicious version of a legitimate package. It achieves this by prohibiting publications from local developer machines and mandating that all releases originate from a hardened CI/CD pipeline that authenticates using short-lived, identity-based tokens and generates verifiable provenance attestations.",
          "pillar": "infra",
          "phase": "building",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0010: AI Supply Chain Compromise",
                "AML.T0012: Valid Accounts",
                "AML.T0074: Masquerading",
                "AML.T0058: Publish Poisoned Models"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Supply Chain Attacks (Cross-Layer)",
                "Model Tampering (L1)",
                "Compromised Framework Components (L3)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM03:2025 Supply Chain"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML06:2023 ML Supply Chain Attacks"
              ]
            }
          ],
          "implementationStrategies": [
            "Mandate CI/CD-only publishing using npm Trusted Publishing (OIDC).",
            "Harden the CI/CD workflow definition to prevent tampering and enforce least privilege.",
            "Implement consumer-side policies to enforce publisher integrity."
          ],
          "toolsOpenSource": [
            "GitHub Actions, GitLab CI (with OIDC support)",
            "Sigstore (for signing and attestations)",
            "Open Policy Agent (OPA), Conftest (for policy checks on workflows)",
            "npm CLI"
          ],
          "toolsCommercial": [
            "JFrog Artifactory, Sonatype Nexus (as secure internal registries/proxies)",
            "CircleCI, Jenkins Enterprise",
            "GitHub Advanced Security",
            "CI/CD security tools (e.g., Snyk, Socket.dev for workflow analysis)"
          ],
          "subTechniques": [],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "data poisoning",
              "supply chain attack",
              "untrusted data",
              "unauthorized modification",
              "attack",
              "stolen",
              "credential",
              "malicious",
              "tokens",
              "tampering",
              "corruption",
              "manipulation",
              "forgery",
              "repudiation",
              "unauthorized"
            ],
            "defense": [
              "least privilege",
              "publisher",
              "integrity",
              "workflow",
              "hardening",
              "defense",
              "secure",
              "authorized",
              "credential",
              "hardened",
              "tokens",
              "provenance",
              "trusted",
              "oidc",
              "harden"
            ]
          }
        },
        {
          "id": "AID-H-025",
          "name": "Tool & MCP Resolution Integrity",
          "description": "Enforce deterministic, verified resolution of tools and Model Context Protocol (MCP) endpoints to prevent typosquatting, alias collision, and registry-based misdirection. Resolution must be fail-closed: ambiguous matches require explicit disambiguation rather than best-guess selection. This ensures that when an agent calls a tool, it binds to the exact, authorized namespace, version, and descriptor bytes intended.",
          "pillar": "infra",
          "phase": "operation",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0010.001: AI Supply Chain Compromise: AI Software",
                "AML.T0074: Masquerading",
                "AML.T0073: Impersonation"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Compromised Agent Registry (L7)",
                "Agent Masquerading (L7)",
                "Supply Chain Attacks (Cross-Layer)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM03:2025 Supply Chain",
                "LLM06:2025 Excessive Agency"
              ]
            },
            {
              "framework": "OWASP Agentic Top 10 2026",
              "items": [
                "ASI02: Tool Misuse and Exploitation",
                "ASI04: Agentic Supply Chain Vulnerabilities"
              ]
            }
          ],
          "implementationStrategies": [],
          "toolsOpenSource": [],
          "toolsCommercial": [],
          "subTechniques": [
            {
              "id": "AID-H-025.001",
              "name": "Fully Qualified Tool ID & Version Pinning",
              "description": "Require fully qualified references (namespace/tool@version) for all tool calls; reject bare names and floating aliases to prevent ambiguous or hijacked resolution.",
              "pillar": "app",
              "phase": "operation",
              "implementationStrategies": [
                "Fail-closed resolver entry point: only accept namespace/tool@x.y.z; never resolve bare names or floating tags."
              ],
              "toolsOpenSource": [
                "semver (language-specific libs: python-semver, node-semver, etc.)",
                "python-jsonschema (descriptor/schema validation)",
                "Open Policy Agent (OPA) (optional policy gate)",
                "OpenTelemetry (resolution decision tracing)"
              ],
              "toolsCommercial": [
                "Kong Gateway / Kong Enterprise (enforce resolver policy at gateway)",
                "Apigee (API gateway policy enforcement)",
                "Datadog (logs/alerts)",
                "Splunk (SIEM)"
              ],
              "defendsAgainst": [
                {
                  "framework": "OWASP Agentic Top 10 2026",
                  "items": [
                    "ASI02: Tool Misuse and Exploitation",
                    "ASI04: Agentic Supply Chain Vulnerabilities"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "attack surface",
                  "hijacked",
                  "exploitation",
                  "vulnerability",
                  "misconfiguration",
                  "bypass"
                ],
                "defense": [
                  "policy enforcement",
                  "fully",
                  "qualified",
                  "version",
                  "pinning",
                  "reject",
                  "prevent",
                  "hardening",
                  "protection",
                  "prevention",
                  "secure",
                  "robust",
                  "resilient",
                  "mitigation",
                  "defense"
                ]
              }
            },
            {
              "id": "AID-H-025.002",
              "name": "MCP Tool Descriptor Hash Binding & Drift Detection",
              "description": "Pin MCP/tool descriptors by content hash and fail closed on drift at resolution time. Scoped to MCP/tool descriptors only (non-goal: agent capability manifests).",
              "pillar": "infra",
              "phase": "operation",
              "implementationStrategies": [
                "Canonicalize descriptor JSON  SHA-256 hash  compare with pinned value; block on mismatch and alert."
              ],
              "toolsOpenSource": [
                "Sigstore cosign (optional signing/verification)",
                "jq (safe diff/debug for JSON)",
                "python-jsonschema (descriptor/schema validation)",
                "OpenTelemetry (integrity check tracing)"
              ],
              "toolsCommercial": [
                "Splunk (SIEM alerts on drift)",
                "Datadog (alerting / monitors)",
                "Sumo Logic (log analytics)"
              ],
              "defendsAgainst": [
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM03:2025 Supply Chain"
                  ]
                },
                {
                  "framework": "OWASP Agentic Top 10 2026",
                  "items": [
                    "ASI04: Agentic Supply Chain Vulnerabilities"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "attack surface",
                  "evasion",
                  "bypass",
                  "stealth",
                  "obfuscation",
                  "hiding",
                  "exploitation",
                  "vulnerability",
                  "misconfiguration"
                ],
                "defense": [
                  "drift detection",
                  "mcp",
                  "descriptor",
                  "hash",
                  "binding",
                  "drift",
                  "detection",
                  "block",
                  "alert",
                  "hardening",
                  "protection",
                  "prevention",
                  "secure",
                  "robust",
                  "resilient"
                ]
              }
            },
            {
              "id": "AID-H-025.003",
              "name": "Fail-Closed Ambiguous Resolution with Typosquat Detection",
              "description": "Detect ambiguous matches and near-miss tool names and fail closed. Similarity checks must trigger explicit disambiguation rather than auto-selection.",
              "pillar": "app",
              "phase": "operation",
              "implementationStrategies": [
                "If multiple candidates exist, stop and require explicit selection; block near-miss names based on similarity threshold."
              ],
              "toolsOpenSource": [
                "RapidFuzz (string similarity)",
                "jellyfish (string distance alternatives)",
                "OpenTelemetry (trace ambiguous resolution events)"
              ],
              "toolsCommercial": [
                "Splunk (SIEM)",
                "Datadog (alerting)",
                "CrowdStrike (optional correlation with endpoint events)"
              ],
              "defendsAgainst": [
                {
                  "framework": "OWASP Agentic Top 10 2026",
                  "items": [
                    "ASI04: Agentic Supply Chain Vulnerabilities"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "attack surface",
                  "evasion",
                  "bypass",
                  "stealth",
                  "obfuscation",
                  "hiding",
                  "exploitation",
                  "vulnerability",
                  "misconfiguration"
                ],
                "defense": [
                  "fail-closed",
                  "ambiguous",
                  "resolution",
                  "typosquat",
                  "detection",
                  "detect",
                  "block",
                  "threshold",
                  "hardening",
                  "protection",
                  "prevention",
                  "secure",
                  "robust",
                  "resilient",
                  "mitigation"
                ]
              }
            }
          ],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "unauthorized modification",
              "attack surface",
              "typosquatting",
              "context",
              "tampering",
              "corruption",
              "manipulation",
              "forgery",
              "exploitation",
              "vulnerability",
              "misconfiguration",
              "bypass"
            ],
            "defense": [
              "mcp",
              "resolution",
              "integrity",
              "enforce",
              "verified",
              "prevent",
              "authorized",
              "hardening",
              "protection",
              "prevention",
              "secure",
              "robust",
              "resilient",
              "mitigation",
              "defense"
            ]
          }
        },
        {
          "id": "AID-H-026",
          "name": "Unsafe Code Execution Prevention",
          "description": "Prevent unexpected code execution (RCE) through unsafe evaluation paths by banning dangerous constructs, enforcing safe interpreters, and requiring a pre-execution static scan gate for agent-generated code. This is a cheap, fail-fast preventive layer that complements sandbox containment and dynamic behavioral analysis.",
          "pillar": "app",
          "phase": "building",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0050: Command and Scripting Interpreter",
                "AML.T0011.001: User Execution: Malicious Package"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Runtime Code Injection (L4)",
                "Agent Tool Misuse (L7)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM05:2025 Improper Output Handling",
                "LLM06:2025 Excessive Agency"
              ]
            },
            {
              "framework": "OWASP Agentic Top 10 2026",
              "items": [
                "ASI05: Unexpected Code Execution (RCE)"
              ]
            }
          ],
          "implementationStrategies": [],
          "toolsOpenSource": [],
          "toolsCommercial": [],
          "subTechniques": [
            {
              "id": "AID-H-026.001",
              "name": "Dangerous Construct Detection & Blocking",
              "description": "Detect and block known-dangerous constructs (eval/exec/unsafe deserialization/shell injection patterns) in agent-generated code or execution requests before they reach any interpreter or tool runner.",
              "pillar": "app",
              "phase": "building",
              "implementationStrategies": [
                "Use AST-based scanning (not regex-only) to block forbidden imports/calls; support block/warn/log policy modes."
              ],
              "toolsOpenSource": [
                "Semgrep",
                "Bandit (Python)",
                "CodeQL (community)",
                "ESLint + eslint-plugin-security (JS/TS)",
                "tree-sitter (multi-language parsing)"
              ],
              "toolsCommercial": [
                "GitHub Advanced Security (CodeQL)",
                "Snyk Code",
                "Veracode",
                "Checkmarx",
                "SonarQube (Commercial editions)"
              ],
              "defendsAgainst": [
                {
                  "framework": "OWASP Agentic Top 10 2026",
                  "items": [
                    "ASI05: Unexpected Code Execution (RCE)"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "attack surface",
                  "dangerous",
                  "injection",
                  "evasion",
                  "bypass",
                  "stealth",
                  "obfuscation",
                  "hiding",
                  "exploitation",
                  "vulnerability",
                  "misconfiguration"
                ],
                "defense": [
                  "dangerous",
                  "construct",
                  "detection",
                  "blocking",
                  "detect",
                  "block",
                  "scanning",
                  "log",
                  "policy",
                  "hardening",
                  "protection",
                  "prevention",
                  "secure",
                  "robust",
                  "resilient"
                ]
              }
            },
            {
              "id": "AID-H-026.002",
              "name": "Safe Interpreter Enforcement",
              "description": "When dynamic evaluation is required, force safe/restricted interpreters inside the execution boundary so code cannot access filesystem, network, or privileged APIs by design. For interactive vibe coding, run evaluation only inside ephemeral sandboxes.",
              "pillar": "app",
              "phase": "building",
              "implementationStrategies": [
                "Allowlist restricted interpreters and enforce interpreter selection via an execution gateway (policy), not via model output."
              ],
              "toolsOpenSource": [
                "RestrictedPython (Python)",
                "vm2 / isolated-vm (JavaScript sandbox runtimes)",
                "Docker / Podman (ephemeral sandboxes)",
                "gVisor / Firecracker (optional hardened sandbox layers)",
                "wasmtime / wasmer (WASM runtimes)"
              ],
              "toolsCommercial": [
                "AWS Lambda (ephemeral execution model)",
                "Google Cloud Run (sandboxed containers)",
                "Azure Container Apps (sandboxed containers)"
              ],
              "defendsAgainst": [
                {
                  "framework": "OWASP Agentic Top 10 2026",
                  "items": [
                    "ASI05: Unexpected Code Execution (RCE)"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "attack surface",
                  "exploitation",
                  "vulnerability",
                  "misconfiguration",
                  "bypass"
                ],
                "defense": [
                  "safe",
                  "interpreter",
                  "enforcement",
                  "evaluation",
                  "sandboxes",
                  "allowlist",
                  "enforce",
                  "gateway",
                  "policy",
                  "hardening",
                  "protection",
                  "prevention",
                  "secure",
                  "robust",
                  "resilient"
                ]
              }
            },
            {
              "id": "AID-H-026.003",
              "name": "Pre-Execution Static Scan",
              "description": "Require a mandatory static analysis gate (SAST/rules) before any agent-generated code or executable artifact is run. Fail-fast on high severity findings and persist scan evidence for audit.",
              "pillar": "app",
              "phase": "building",
              "implementationStrategies": [
                "Run SAST in CI and (for interactive sessions) right before execution; block on critical findings."
              ],
              "toolsOpenSource": [
                "Semgrep",
                "Bandit (Python)",
                "ESLint + eslint-plugin-security (JS/TS)",
                "CodeQL (community)"
              ],
              "toolsCommercial": [
                "GitHub Advanced Security (CodeQL)",
                "Snyk Code",
                "Veracode",
                "Checkmarx"
              ],
              "defendsAgainst": [
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM05:2025 Improper Output Handling"
                  ]
                },
                {
                  "framework": "OWASP Agentic Top 10 2026",
                  "items": [
                    "ASI05: Unexpected Code Execution (RCE)"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "forensic evasion",
                  "attack surface",
                  "artifact",
                  "repudiation",
                  "tampering",
                  "unauthorized",
                  "exploitation",
                  "vulnerability",
                  "misconfiguration",
                  "bypass"
                ],
                "defense": [
                  "static analysis",
                  "pre-execution",
                  "static",
                  "scan",
                  "analysis",
                  "sast",
                  "rules",
                  "audit",
                  "sessions",
                  "block"
                ]
              }
            }
          ],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "code execution",
              "attack surface",
              "dangerous",
              "exploitation",
              "vulnerability",
              "misconfiguration",
              "bypass"
            ],
            "defense": [
              "unsafe",
              "code",
              "execution",
              "prevention",
              "prevent",
              "evaluation",
              "enforcing",
              "safe",
              "scan",
              "sandbox",
              "containment",
              "analysis"
            ]
          }
        }
      ]
    },
    {
      "id": "detect",
      "name": "Detect",
      "description": "The \"Detect\" tactic focuses on the timely identification of intrusions, malicious activities, anomalous behaviors, or policy violations occurring within or targeting AI systems. This involves continuous or periodic monitoring of various aspects of the AI ecosystem, including inputs (prompts, data feeds), outputs (predictions, generated content, agent actions), model behavior (performance metrics, drift), system logs (API calls, resource usage), and the integrity of AI artifacts (models, datasets).",
      "techniques": [
        {
          "id": "AID-D-001",
          "name": "Adversarial Input & Prompt Injection Detection",
          "description": "Continuously monitor and analyze inputs to AI models to detect characteristics of adversarial manipulation, malicious prompt content, or jailbreak attempts.<p>Key defense capabilities:</p><ul><li>Detecting statistically anomalous inputs (e.g., out-of-distribution samples).</li><li>Scanning for known malicious patterns, hidden commands, and jailbreak sequences.</li><li>Identifying attempts to inject executable code or harmful instructions.</li></ul><p>The goal is to block, flag, or sanitize such inputs before they can significantly impact the model's behavior.</p>",
          "pillar": "app",
          "phase": "operation",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0015: Evade AI Model",
                "AML.T0043: Craft Adversarial Data",
                "AML.T0051: LLM Prompt Injection",
                "AML.T0054: LLM Jailbreak",
                "AML.T0068: LLM Prompt Obfuscation"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Adversarial Examples (L1)",
                "Evasion of Security AI Agents (L6)",
                "Input Validation Attacks (L3)",
                "Reprogramming Attacks (L1)",
                "Cross-Modal Manipulation Attacks (L1)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM01:2025 Prompt Injection"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML01:2023 Input Manipulation Attack"
              ]
            }
          ],
          "implementationStrategies": [],
          "toolsOpenSource": [],
          "toolsCommercial": [],
          "subTechniques": [
            {
              "id": "AID-D-001.001",
              "name": "Per-Prompt Content & Obfuscation Analysis",
              "description": "Performs real-time analysis on individual prompts to detect malicious content, prompt injection, and jailbreaking attempts. This sub-technique combines two key functions: 1) identifying known malicious patterns and harmful intent using heuristics, regex, and specialized guardrail models, and 2) detecting attempts to hide or obscure these attacks through obfuscation techniques like character encoding (e.g., Base64), homoglyphs, or high-entropy strings. It acts as a primary, synchronous guardrail at the input layer.",
              "pillar": "app",
              "phase": "operation",
              "implementationStrategies": [
                "Use a secondary, smaller 'guardrail' model to inspect prompts for harmful intent or policy violations.",
                "Ingest and analyze synchronous gate decisions to detect evasions, correlate incidents, and escalate",
                "Analyze prompt characteristics like entropy and character distribution to detect obfuscation.",
                "Implement multi-step decoding to handle layered obfuscation."
              ],
              "toolsOpenSource": [
                "NVIDIA NeMo Guardrails",
                "Rebuff.ai",
                "Llama Guard (Meta)",
                "LangChain Guardrails",
                "Python `re` and `collections` modules"
              ],
              "toolsCommercial": [
                "OpenAI Moderation API",
                "Google Perspective API",
                "Lakera Guard",
                "Protect AI Guardian",
                "CalypsoAI Validator",
                "Securiti LLM Firewall"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0051: LLM Prompt Injection",
                    "AML.T0054: LLM Jailbreak",
                    "AML.T0068: LLM Prompt Obfuscation"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Input Validation Attacks (L3)",
                    "Reprogramming Attacks (L1)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM01:2025 Prompt Injection"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML01:2023 Input Manipulation Attack"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "prompt injection",
                  "harmful output",
                  "policy violation",
                  "prompts",
                  "malicious",
                  "prompt",
                  "injection",
                  "jailbreaking",
                  "key",
                  "harmful",
                  "jailbreak",
                  "bypass"
                ],
                "defense": [
                  "per-prompt",
                  "content",
                  "obfuscation",
                  "analysis",
                  "guardrail",
                  "detect",
                  "key",
                  "detecting",
                  "inspect",
                  "policy",
                  "analyze"
                ]
              }
            },
            {
              "id": "AID-D-001.002",
              "name": "Synthetic Media & Deepfake Forensics",
              "description": "Detects manipulated or synthetically generated media (e.g., deepfakes) by performing a forensic analysis that identifies a combination of specific technical artifacts and inconsistencies. This technique fuses evidence from multiple indicators across different modalitiessuch as image compression anomalies, unnatural biological signals (blinking, vocal patterns), audio-visual mismatches, and hidden data payloadsto provide a more robust and reliable assessment of the media's authenticity.",
              "pillar": "data",
              "phase": "validation",
              "implementationStrategies": [
                "Analyze for digital manipulation artifacts in images.",
                "Analyze for unnatural biological signals in video and audio.",
                "Perform cross-modal consistency checks to detect conflicting information.",
                "Scan all media for hidden data payloads and embedded commands."
              ],
              "toolsOpenSource": [
                "OpenCV, Pillow (for image processing)",
                "dlib, Mediapipe (for facial landmark detection)",
                "Librosa (for audio feature extraction)",
                "pyzbar, pytesseract, stegano (for hidden data detection)",
                "Hugging Face Transformers, sentence-transformers (for cross-modal analysis)"
              ],
              "toolsCommercial": [
                "Sensity AI, Truepic, Hive AI (Deepfake detection and content authenticity)",
                "Pindrop (Voice security and liveness)",
                "Cloud Provider Vision/Audio APIs (AWS Rekognition, Google Vision AI, Azure Cognitive Services)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0043: Craft Adversarial Data",
                    "AML.T0048: External Harms",
                    "AML.T0073: Impersonation"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Adversarial Examples (L1)",
                    "Misinformation Generation (Cross-Layer)",
                    "Cross-Modal Manipulation Attacks (L1)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM01:2025 Prompt Injection",
                    "LLM09:2025 Misinformation"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML01:2023 Input Manipulation Attack",
                    "ML09:2023 Output Integrity Attack"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "stealth attack",
                  "anomalous behavior",
                  "manipulated",
                  "artifacts",
                  "manipulation",
                  "evasion",
                  "obfuscation",
                  "intrusion"
                ],
                "defense": [
                  "synthetic",
                  "media",
                  "deepfake",
                  "forensics",
                  "analysis",
                  "robust",
                  "reliable",
                  "assessment",
                  "analyze",
                  "detect",
                  "scan"
                ]
              }
            },
            {
              "id": "AID-D-001.003",
              "name": "Vector-Space Anomaly Detection",
              "description": "Detects semantically novel or anomalous inputs by operating on their vector embeddings rather than their raw content. This technique establishes a baseline of 'normal' inputs by clustering the embeddings of known-good data. At inference time, inputs whose embeddings are statistical outliers or fall far from the normal cluster centroids are flagged as suspicious. This is effective against novel attacks that bypass keyword or pattern-based filters by using unusual but semantically malicious phrasing.",
              "pillar": "model",
              "phase": "operation",
              "implementationStrategies": [
                "Establish a baseline of normal prompt embeddings.",
                "Detect anomalous prompts in real-time using distance from the baseline centroid.",
                "Use clustering algorithms in near real-time to detect anomalous groups of prompts."
              ],
              "toolsOpenSource": [
                "sentence-transformers (for generating embeddings)",
                "scikit-learn (for KMeans, DBSCAN, PCA)",
                "FAISS (Facebook AI Similarity Search) (for efficient nearest neighbor search)",
                "Vector Databases (Chroma, Weaviate, Milvus, Qdrant)"
              ],
              "toolsCommercial": [
                "AI Observability Platforms (Arize AI, Fiddler, WhyLabs)",
                "Managed Vector Databases (Pinecone, Zilliz Cloud, cloud provider offerings)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0015: Evade AI Model",
                    "AML.T0051: LLM Prompt Injection",
                    "AML.T0054: LLM Jailbreak"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Adversarial Examples (L1)",
                    "Input Validation Attacks (L3)",
                    "Misinformation Generation (Cross-Layer)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM01:2025 Prompt Injection",
                    "LLM08:2025 Vector and Embedding Weaknesses"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML01:2023 Input Manipulation Attack"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "anomalous behavior",
                  "anomalous",
                  "vector",
                  "embeddings",
                  "suspicious",
                  "bypass",
                  "malicious",
                  "prompt",
                  "prompts",
                  "drift",
                  "deviation",
                  "degradation",
                  "manipulation",
                  "evasion",
                  "stealth"
                ],
                "defense": [
                  "anomaly detection",
                  "vector-space",
                  "anomaly",
                  "detection",
                  "baseline",
                  "detect",
                  "monitoring",
                  "alerting",
                  "analysis",
                  "inspection",
                  "observability",
                  "logging",
                  "audit"
                ]
              }
            },
            {
              "id": "AID-D-001.004",
              "name": "LLM Guardrail for Intent/Privilege Escalation",
              "description": "Use a fast secondary LLM (guardrail) to classify prompts for intent switching, instruction bypass, or privilege escalation before reaching the primary model.",
              "pillar": "app",
              "phase": "operation",
              "implementationStrategies": [
                "Inline gate with strict classifier prompt and fail-closed behavior."
              ],
              "toolsOpenSource": [
                "Llama Guard",
                "Guardrails.ai",
                "NVIDIA NeMo Guardrails"
              ],
              "toolsCommercial": [
                "Protect AI Guardian",
                "Lakera Guard"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0051 LLM Prompt Injection",
                    "AML.T0054 LLM Jailbreak"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Reprogramming Attacks (L1)",
                    "Agent Goal Manipulation (L7)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM01:2025 Prompt Injection"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML01:2023 Input Manipulation Attack"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "privilege escalation",
                  "harmful output",
                  "policy violation",
                  "stealth attack",
                  "anomalous behavior",
                  "prompts",
                  "bypass",
                  "prompt",
                  "jailbreak",
                  "evasion",
                  "obfuscation",
                  "intrusion"
                ],
                "defense": [
                  "llm",
                  "guardrail",
                  "intent",
                  "privilege",
                  "escalation",
                  "detection",
                  "monitoring",
                  "alerting",
                  "analysis",
                  "anomaly",
                  "inspection",
                  "observability",
                  "logging",
                  "audit"
                ]
              }
            },
            {
              "id": "AID-D-001.005",
              "name": "Active Prompt Integrity Check (Canary Tokens)",
              "description": "Proactively inject a random, secret 'canary token' or a specific 'known-answer' challenge into the system prompt or hidden context window. The model is instructed to include this token only in a non-user-visible field (for example a JSON metadata field returned to the backend). If the response metadata fails to contain the correct token, or the output breaks the expected structured format, it strongly suggests that the system prompt has been overridden or ignored due to a prompt injection or jailbreak attempt. This turns prompt injection detection from a purely heuristic signal into a much more reliable, explicit integrity check.",
              "pillar": "app",
              "phase": "operation",
              "implementationStrategies": [
                "Inject a dynamic session secret into the System Prompt and verify its presence in the response metadata."
              ],
              "toolsOpenSource": [
                "Python 'secrets' module (for token generation)",
                "LangChain (for prompt template injection)",
                "litellm (Python package) / OpenAI Python SDK"
              ],
              "toolsCommercial": [
                "Enterprise Gateway Policies (e.g., Cloudflare AI Gateway custom rules)",
                "Lakera Guard (uses similar active probing concepts)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0051: LLM Prompt Injection",
                    "AML.T0054: LLM Jailbreak"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Reprogramming Attacks (L1)",
                    "Evasion of Security AI Agents (L6)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM01:2025 Prompt Injection"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML01:2023 Input Manipulation Attack"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "prompt injection",
                  "jailbreak attempt",
                  "system prompt",
                  "context window",
                  "prompt",
                  "tokens",
                  "inject",
                  "secret",
                  "token",
                  "context",
                  "injection",
                  "jailbreak"
                ],
                "defense": [
                  "canary token",
                  "active",
                  "prompt",
                  "integrity",
                  "check",
                  "canary",
                  "tokens",
                  "secret",
                  "token",
                  "contain",
                  "detection",
                  "reliable",
                  "session",
                  "verify"
                ]
              }
            }
          ],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "prompt injection",
              "jailbreak attempt",
              "adversarial input",
              "adversarial",
              "prompt",
              "injection",
              "manipulation",
              "malicious",
              "jailbreak",
              "key",
              "anomalous",
              "inject",
              "harmful"
            ],
            "defense": [
              "adversarial",
              "input",
              "prompt",
              "injection",
              "detection",
              "monitor",
              "analyze",
              "detect",
              "key",
              "defense",
              "detecting",
              "scanning",
              "block",
              "sanitize"
            ]
          }
        },
        {
          "id": "AID-D-002",
          "name": "AI Model Anomaly & Performance Drift Detection",
          "description": "Continuously monitor the outputs, performance metrics (e.g., accuracy, confidence scores, precision, recall, F1-score, output distribution), and potentially internal states or feature attributions of AI models during operation. This monitoring aims to detect significant deviations from established baselines or expected behavior. Such anomalies or drift can indicate various issues, including concept drift (changes in the underlying data distribution), data drift (changes in input data characteristics), or malicious activities like ongoing data poisoning attacks, subtle model evasion attempts, or model skewing.",
          "pillar": "model",
          "phase": "operation",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0031: Erode AI Model Integrity",
                "AML.T0015: Evade ML Model",
                "AML.T0020: Poison Training Data"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Unpredictable agent behavior / Performance Degradation (L5)",
                "Model Skewing (L2)",
                "Manipulation of Evaluation Metrics (L5)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM09:2025 Misinformation (by detecting drift that leads to it)"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML08:2023 Model Skewing"
              ]
            }
          ],
          "implementationStrategies": [
            "Monitor the statistical distribution of model inputs to detect data drift.",
            "Monitor the statistical distribution of model outputs to detect concept drift.",
            "Monitor model performance metrics by comparing predictions to ground truth labels.",
            "Detect anomalous attention patterns in Transformer-based models.",
            "Detect anomalous input parameters for generative models."
          ],
          "toolsOpenSource": [
            "Evidently AI, NannyML, Alibi Detect (for drift detection)",
            "scikit-learn (for metrics), SciPy (for statistical tests)",
            "MLflow (for logging and tracking metrics over time)",
            "Prometheus, Grafana (for time-series monitoring and alerting)"
          ],
          "toolsCommercial": [
            "AI Observability Platforms (Arize AI, Fiddler, WhyLabs, Truera)",
            "Cloud Provider Model Monitoring (Amazon SageMaker Model Monitor, Google Vertex AI Model Monitoring, Azure Model Monitor)",
            "Application Performance Monitoring (APM) tools (Datadog, New Relic, Dynatrace)",
            "Weights & Biases (for logging and tracking metrics over time)"
          ],
          "subTechniques": [],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "data poisoning",
              "anomalous behavior",
              "stealth attack",
              "persistent threat",
              "malicious",
              "poisoning",
              "evasion",
              "anomalous",
              "parameters",
              "drift",
              "deviation",
              "degradation",
              "manipulation",
              "undetected",
              "bypass"
            ],
            "defense": [
              "drift detection",
              "model",
              "anomaly",
              "performance",
              "drift",
              "detection",
              "monitor",
              "monitoring",
              "detect",
              "alerting",
              "analysis",
              "inspection",
              "observability",
              "logging",
              "audit"
            ]
          }
        },
        {
          "id": "AID-D-003",
          "name": "AI Output Monitoring & Policy Enforcement",
          "description": "Actively inspect the outputs generated by AI models (for example, text responses, classifications, and agent tool calls) in near real time. The system enforces predefined safety, security, privacy, and business policies on those outputs and takes action (block, sanitize, alert, require human approval) when violations are detected. This closes the loop after inference and prevents unsafe or out-of-policy behavior from ever reaching end users or downstream systems.",
          "pillar": "app",
          "phase": "operation",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0048.002: External Harms: Societal Harm",
                "AML.T0057: LLM Data Leakage",
                "AML.T0052: Phishing",
                "AML.T0047: AI-Enabled Product or Service",
                "AML.T0061: LLM Prompt Self-Replication",
                "AML.T0053: AI Agent Tool Invocation",
                "AML.T0067: LLM Trusted Output Components Manipulation",
                "AML.T0077: LLM Response Rendering"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Misinformation Generation (L1/L7)",
                "Data Exfiltration (L2)",
                "Data Leakage through Observability (L5)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM02:2025 Sensitive Information Disclosure",
                "LLM05:2025 Improper Output Handling",
                "LLM09:2025 Misinformation",
                "LLM07:2025 System Prompt Leakage"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML03:2023 Model Inversion Attack",
                "ML09:2023 Output Integrity Attack"
              ]
            }
          ],
          "implementationStrategies": [],
          "toolsOpenSource": [],
          "toolsCommercial": [],
          "subTechniques": [
            {
              "id": "AID-D-003.001",
              "name": "Harmful Content & Policy Filtering",
              "description": "Inspect model-generated text before it is returned to the user. The goal is to stop content that violates safety, compliance, trust & safety, or brand rules. This includes hate speech, self-harm encouragement, explicit content, criminal instructions, phishing-style scams, or content that would create legal or reputational risk.",
              "pillar": "app",
              "phase": "operation",
              "implementationStrategies": [
                "Deploy a fast safety and abuse classifier to scan AI output for policy violations.",
                "Use a dedicated critic LLM to review the main model's output for safety and compliance.",
                "Apply rule-based filters (keywords and regex) as a deterministic final gate."
              ],
              "toolsOpenSource": [
                "Hugging Face Transformers (for custom classifiers)",
                "spaCy, NLTK (for rule-based filtering)",
                "Open-source LLM-based guardrails (for example, Llama Guard, NVIDIA NeMo Guardrails)"
              ],
              "toolsCommercial": [
                "OpenAI Moderation API",
                "Azure Content Safety",
                "Google Perspective API",
                "Clarifai",
                "Hive AI",
                "Lakera Guard",
                "Protect AI Guardian",
                "Securiti LLM Firewall"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0048.002: External Harms: Societal Harm",
                    "AML.T0057: LLM Data Leakage"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Misinformation Generation (L1/L7)",
                    "Data Exfiltration (L2)",
                    "Agent Tool Misuse (L7)",
                    "Compromised Agent Registry (L7)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM05:2025 Improper Output Handling",
                    "LLM06:2025 Excessive Agency",
                    "LLM09:2025 Misinformation"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML09:2023 Output Integrity Attack"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "malicious content",
                  "harmful output",
                  "data exfiltration",
                  "phishing",
                  "harmful",
                  "risk",
                  "abuse",
                  "injection"
                ],
                "defense": [
                  "harmful",
                  "content",
                  "policy",
                  "filtering",
                  "inspect",
                  "safety",
                  "compliance",
                  "trust",
                  "rules",
                  "scan",
                  "rule"
                ]
              }
            },
            {
              "id": "AID-D-003.002",
              "name": "Sensitive Information & Data Leakage Detection",
              "description": "Prevent the model from leaking confidential data (for example, PII, secrets, source code, internal project names, private tickets) in its output. The system scans every response before it is shown or logged in the clear. If sensitive content is detected, the response is redacted, blocked, or escalated.",
              "pillar": "data",
              "phase": "operation",
              "implementationStrategies": [
                "Detect common sensitive formats (PII, secrets, credentials) using pattern matching.",
                "Use Named Entity Recognition (NER) and Presidio to detect and redact unstructured PII.",
                "Detect direct memorization / regurgitation of training data.",
                "Detect organization-specific secrets and internal identifiers."
              ],
              "toolsOpenSource": [
                "Microsoft Presidio (for PII detection and anonymization)",
                "NLP libraries (spaCy, NLTK, Hugging Face Transformers) for custom NER models",
                "FlashText (high-performance exact phrase / keyword matching)",
                "Open-source secret scanners adapted for model output (for example, truffleHog-style logic)"
              ],
              "toolsCommercial": [
                "Google Cloud DLP API",
                "AWS Macie",
                "Azure Purview",
                "Gretel.ai",
                "Tonic.ai",
                "Enterprise DLP platforms (for example, Symantec DLP, Forcepoint DLP)",
                "AI security / model monitoring platforms (for example, HiddenLayer, Protect AI)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0024.000: Exfiltration via AI Inference API: Infer Training Data Membership",
                    "AML.T0024.001: Exfiltration via AI Inference API: Invert AI Model",
                    "AML.T0057: LLM Data Leakage",
                    "AML.T0048.003: External Harms: User Harm",
                    "AML.T0047: AI-Enabled Product or Service",
                    "AML.T0077: LLM Response Rendering"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Data Exfiltration (L2)",
                    "Data Leakage through Observability (L5)",
                    "Model Inversion / Extraction (L2)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM02:2025 Sensitive Information Disclosure"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML03:2023 Model Inversion Attack",
                    "ML04:2023 Membership Inference Attack"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "data leakage",
                  "leakage",
                  "leaking",
                  "secrets",
                  "credentials",
                  "evasion",
                  "bypass",
                  "stealth",
                  "obfuscation",
                  "hiding"
                ],
                "defense": [
                  "sensitive",
                  "information",
                  "data",
                  "leakage",
                  "detection",
                  "prevent",
                  "confidential",
                  "secrets",
                  "private",
                  "logged",
                  "detected",
                  "blocked",
                  "detect",
                  "credentials"
                ]
              }
            },
            {
              "id": "AID-D-003.003",
              "name": "Agentic Tool Use & Action Policy Monitoring",
              "description": "Before an autonomous agent is allowed to execute a tool call (for example, call an API, read a file, draft an email, trigger payment), enforce hard guardrails. Each proposed action is checked against: (1) an allowlist of which tools this agent role is allowed to use, (2) strict parameter schemas, (3) stateful business policies like 'human approval required', and (4) audit logging. This prevents a compromised agent from doing something dangerous, high-impact, or illegal.",
              "pillar": "app",
              "phase": "operation",
              "implementationStrategies": [
                "Enforce least-privilege tool access using a per-role allowlist.",
                "Validate tool parameters against strict schemas before execution.",
                "Use a policy engine (for example, OPA) to enforce stateful business rules.",
                "Log every tool decision (allowed and denied) to a central SIEM for audit and alerting."
              ],
              "toolsOpenSource": [
                "Open Policy Agent (OPA) for stateful policy-as-code",
                "Pydantic (for strict parameter validation and typing)",
                "Agent frameworks with explicit tool invocation (for example, LangChain, AutoGen, CrewAI)",
                "JSON Schema (for defining and validating tool parameters)"
              ],
              "toolsCommercial": [
                "Lakera Guard",
                "Protect AI Guardian",
                "Enterprise policy control platforms (for example, Styra DAS)",
                "API gateways with advanced policy enforcement (for example, Kong, Apigee)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0053: AI Agent Tool Invocation",
                    "AML.T0048: External Harms",
                    "AML.TA0005: Execution"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Agent Goal Manipulation (L7)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM06:2025 Excessive Agency",
                    "LLM01:2025 Prompt Injection",
                    "LLM05:2025 Improper Output Handling"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML09:2023 Output Integrity Attack"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "stealth attack",
                  "persistent threat",
                  "forensic evasion",
                  "cover tracks",
                  "log injection",
                  "parameter",
                  "compromised",
                  "dangerous",
                  "parameters",
                  "evasion",
                  "undetected",
                  "repudiation",
                  "tampering",
                  "unauthorized",
                  "jailbreak"
                ],
                "defense": [
                  "audit logging",
                  "policy enforcement",
                  "agentic",
                  "use",
                  "action",
                  "policy",
                  "monitoring",
                  "guardrail",
                  "guardrails",
                  "enforce",
                  "allowlist",
                  "role",
                  "policies",
                  "audit",
                  "logging"
                ]
              }
            },
            {
              "id": "AID-D-003.004",
              "name": "Tool-Call Sequence Anomaly Detection",
              "description": "Model and continuously score the sequence of tool calls made by an agent (for example: search_knowledge_base  summarize  create_support_ticket). A healthy agent follows predictable flows. A hijacked agent may suddenly jump to unusual or high-risk tools (for example: read_internal_db  send_email  execute_payment). By learning 'normal' transition probabilities, you can flag suspicious sessions in real time.",
              "pillar": "app",
              "phase": "operation",
              "implementationStrategies": [
                "Learn normal tool-call transitions, then alert on low-likelihood sequences."
              ],
              "toolsOpenSource": [
                "pandas",
                "NumPy",
                "scikit-learn"
              ],
              "toolsCommercial": [
                "Splunk User Behavior Analytics (UBA)",
                "Elastic Security"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0053: AI Agent Tool Invocation",
                    "AML.TA0005: Execution"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Agent Tool Misuse (L7)",
                    "Agent Goal Manipulation (L7)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM06:2025 Excessive Agency"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML09:2023 Output Integrity Attack"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "novel attack",
                  "unknown threat",
                  "hijacked",
                  "suspicious",
                  "evasion",
                  "bypass",
                  "stealth",
                  "obfuscation",
                  "hiding",
                  "zero-day"
                ],
                "defense": [
                  "anomaly detection",
                  "tool-call",
                  "sequence",
                  "anomaly",
                  "detection",
                  "sessions",
                  "alert",
                  "monitoring",
                  "alerting",
                  "analysis",
                  "inspection",
                  "observability",
                  "logging",
                  "audit"
                ]
              }
            },
            {
              "id": "AID-D-003.005",
              "name": "Stateful Session Monitoring: Intent Drift + Safety Invariants",
              "description": "Attackers can split one malicious goal into many small, individually normal requests. Stateful monitoring reconnects those steps into one picture. This is to detect multi-step, stateful attacks that bypass single-turn guardrails by maintaining session memory and enforcing session-level safety invariants. <p>The monitor tracks (1) intent drift over time (e.g., a conversation slowly shifting from 'summarize emails' to 'forward confidential content externally'), and (2) invariant violations (e.g., 'sensitive data read in this session must never be sent to external recipients').</p> <p>When thresholds are exceeded, the system blocks execution, escalates to human-in-the-loop (HITL), or triggers incident response.",
              "pillar": "app",
              "phase": "operation",
              "implementationStrategies": [
                "Maintain a session security ledger (state + intent vector) and enforce versioned safety invariants on every tool call and high-risk model output."
              ],
              "toolsOpenSource": [
                "Redis (session state + TTL) or Postgres (durable session ledger)",
                "OpenTelemetry (distributed tracing)",
                "WhyLogs / LangKit (LLM telemetry features)",
                "FAISS / pgvector (optional intent vector storage)",
                "OPA / Rego (invariant policy evaluation)"
              ],
              "toolsCommercial": [
                "SIEM/SOAR (Splunk, Sentinel, Chronicle) for correlation and response",
                "Managed Redis / database services",
                "Observability platforms (Datadog, New Relic) for tracing/metrics"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0051: LLM Prompt Injection",
                    "AML.T0054: LLM Jailbreak",
                    "AML.T0051.001: LLM Prompt Injection: Indirect"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM01:2025 Prompt Injection",
                    "LLM06:2025 Excessive Agency",
                    "LLM02:2025 Sensitive Information Disclosure"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Evasion of Detection (L6)",
                    "Data Leakage through Observability (L6)",
                    "Privilege Escalation (Cross-Layer)",
                    "Data Leakage (Cross-Layer)"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "stealth attack",
                  "persistent threat",
                  "harmful output",
                  "policy violation",
                  "malicious",
                  "bypass",
                  "memory",
                  "state",
                  "vector",
                  "execution",
                  "evasion",
                  "undetected",
                  "jailbreak"
                ],
                "defense": [
                  "incident response",
                  "stateful",
                  "session",
                  "monitoring",
                  "intent",
                  "drift",
                  "safety",
                  "invariants",
                  "guardrail",
                  "guardrails",
                  "human-in-the-loop",
                  "detect",
                  "enforcing",
                  "monitor",
                  "confidential"
                ]
              }
            },
            {
              "id": "AID-D-003.006",
              "name": "Memory Write-Abuse & Drift Monitoring",
              "description": "Detect and respond to runtime memory poisoning and persistence abuse by monitoring abnormal memory write patterns (rate spikes, repeated content fingerprints, cross-namespace writes) and read-path integrity failures (signature/HMAC verification failures, quarantine hit-rate anomalies). This sub-technique produces SIEM-grade signals and triggers policy-driven containment actions (write throttling/blocks, quarantine routing, privilege step-down, or session quarantine) while preserving auditability and minimizing false positives. This complements cryptographic integrity controls (e.g., signed writes / verified reads) by turning integrity and lifecycle signals into actionable detections and response playbooks.",
              "pillar": "app",
              "phase": "operation",
              "implementationStrategies": [
                "Emit SIEM-grade telemetry for memory write behavior (rate spikes, repeated fingerprints, cross-namespace writes) using a shared store; trigger automated containment via policy hooks.",
                "Monitor read-path integrity signals (verification failure rate, quarantine hit rate) and fail-closed to keep tainted memory out of context."
              ],
              "toolsOpenSource": [
                "OpenTelemetry (metrics/traces/log export)",
                "Prometheus (metrics + alerting)",
                "Grafana (dashboards)",
                "Redis (shared counters + sliding windows)",
                "Apache Kafka (security event pipeline)",
                "OpenSearch (log indexing)",
                "OPA / Rego (policy-based response hooks)"
              ],
              "toolsCommercial": [
                "Datadog (observability + alerting)",
                "Splunk Enterprise Security (SIEM)",
                "Microsoft Sentinel (SIEM)",
                "Elastic Observability"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0080 AI Agent Context Poisoning",
                    "AML.T0080.000 AI Agent Context Poisoning: Memory"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Data Poisoning (L2)",
                    "Agent Goal Manipulation (L7)",
                    "Agent Tool Misuse (L7)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM04:2025 Data and Model Poisoning",
                    "LLM08:2025 Vector and Embedding Weaknesses",
                    "LLM10:2025 Unbounded Consumption"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML06:2023 AI Supply Chain Attacks",
                    "ML09:2023 Output Integrity Attack"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "unauthorized modification",
                  "man in the middle",
                  "denial of service",
                  "resource exhaustion",
                  "persistence",
                  "memory",
                  "poisoning",
                  "abuse",
                  "abnormal",
                  "context",
                  "tampering",
                  "corruption",
                  "manipulation",
                  "forgery",
                  "interception"
                ],
                "defense": [
                  "memory",
                  "write-abuse",
                  "drift",
                  "monitoring",
                  "detect",
                  "integrity",
                  "signature",
                  "hmac",
                  "verification",
                  "quarantine",
                  "containment",
                  "privilege",
                  "session",
                  "cryptographic",
                  "signed"
                ]
              }
            }
          ],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "stealth attack",
              "persistent threat",
              "anomalous behavior",
              "evasion",
              "undetected",
              "obfuscation",
              "intrusion"
            ],
            "defense": [
              "policy enforcement",
              "output",
              "monitoring",
              "policy",
              "enforcement",
              "inspect",
              "safety",
              "privacy",
              "policies",
              "block",
              "sanitize",
              "alert",
              "detected"
            ]
          }
        },
        {
          "id": "AID-D-004",
          "name": "Model & AI Artifact Integrity Monitoring, Audit & Tamper Detection",
          "description": "Regularly verify the cryptographic integrity and authenticity of deployed AI models, their parameters, associated datasets, and critical components of their runtime environment. This process aims to detect any unauthorized modifications, tampering, or the insertion of backdoors that could compromise the model's behavior, security, or data confidentiality. It ensures that the AI artifacts in operation are the approved, untampered versions.",
          "pillar": "infra",
          "phase": "building",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0018: Manipulate AI Model",
                "AML.T0018.002: Manipulate AI Model: Embed Malware",
                "AML.T0058: Publish Poisoned Models",
                "AML.T0069: Discover LLM System Information (detects unauthorized changes that expand unintended disclosure surface)",
                "AML.T0074 Masquerading"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Data Tampering (L2)",
                "Model Tampering (L1)",
                "Runtime Code Injection (L4)",
                "Memory Corruption (L4)",
                "Misconfigurations (L4)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM03:2025 Supply Chain",
                "LLM04:2025 Data and Model Poisoning"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML06:2023 AI Supply Chain Attacks",
                "ML10:2023 Model Poisoning"
              ]
            }
          ],
          "implementationStrategies": [],
          "toolsOpenSource": [],
          "toolsCommercial": [],
          "subTechniques": [
            {
              "id": "AID-D-004.001",
              "name": "Static Artifact Hash & Signature Verification",
              "description": "Acts as the verifier and auditor for macro-scale artifact integrity established by AID-M-002.002. Computes and verifies cryptographic hashes of stored model artifacts, datasets, and container image layers against authorized manifests or registries. Detects unauthorized modifications, signature failures, and drift from approved baselines before deployment or promotion. This detection technique validates that artifacts signed during building remain untampered through the validation pipeline.",
              "pillar": "infra",
              "phase": "building",
              "implementationStrategies": [
                "Verify artifact hashes against authorized manifest in a write-once model registry.",
                "Detect drift from baseline manifests using file integrity monitoring (sha256sum, Tripwire).",
                "Trigger immediate security escalation if an artifact hash deviates or goes missing."
              ],
              "toolsOpenSource": [
                "MLflow Model Registry",
                "DVC (Data Version Control)",
                "Notary",
                "Sigstore/cosign",
                "sha256sum (Linux utility)",
                "Tripwire",
                "AIDE (Advanced Intrusion Detection Environment)"
              ],
              "toolsCommercial": [
                "Databricks Model Registry",
                "Amazon SageMaker Model Registry",
                "Google Vertex AI Model Registry",
                "Protect AI (ModelScan)",
                "JFrog Artifactory",
                "Snyk Container (for image integrity)",
                "Tenable.io (for file integrity monitoring)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0018 Manipulate AI Model",
                    "AML.T0018.002 Manipulate AI Model: Embed Malware",
                    "AML.T0020 Poison Training Data",
                    "AML.T0058 Publish Poisoned Models",
                    "AML.T0076 Corrupt AI Model",
                    "AML.T0010.003 AI Supply Chain Compromise: Model",
                    "AML.T0010.004 AI Supply Chain Compromise: Container Registry"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Model Tampering (L1)",
                    "Data Tampering (L2)",
                    "Compromised Container Images (L4)",
                    "Supply Chain Attacks (Cross-Layer)",
                    "Backdoor Attacks (L1)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM03:2025 Supply Chain",
                    "LLM04:2025 Data and Model Poisoning"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML06:2023 AI Supply Chain Attacks",
                    "ML10:2023 Model Poisoning",
                    "ML02:2023 Data Poisoning Attack",
                    "ML09:2023 Output Integrity Attack"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "unauthorized modification",
                  "man in the middle",
                  "anomalous behavior",
                  "artifact",
                  "artifacts",
                  "unauthorized",
                  "tampering",
                  "corruption",
                  "manipulation",
                  "forgery",
                  "interception",
                  "drift",
                  "deviation",
                  "degradation",
                  "injection"
                ],
                "defense": [
                  "model registry",
                  "intrusion detection",
                  "static",
                  "artifact",
                  "hash",
                  "signature",
                  "verification",
                  "tripwire",
                  "integrity",
                  "cryptographic",
                  "container",
                  "authorized",
                  "registries",
                  "detection",
                  "signed"
                ]
              }
            },
            {
              "id": "AID-D-004.002",
              "name": "Runtime Attestation & Memory Integrity",
              "description": "Attest the running model process (code, weights, enclave MRENCLAVE) to detect in-memory patching or DLL injection.",
              "pillar": "infra",
              "phase": "operation",
              "implementationStrategies": [
                "Start inference in a TEE (SGX, SEV, Nitro Enclave) and verify measurement before releasing traffic.",
                "Use remote-attestation APIs; deny requests if the quote is stale or unrecognised.",
                "Monitor loaded shared-object hashes with eBPF kernel probes."
              ],
              "toolsOpenSource": [
                "Intel SGX SDK",
                "Open Enclave SDK",
                "AWS Nitro Enclaves SDK",
                "Google Asylo SDK",
                "Verifiable Confidential AI (VCAI) projects",
                "eBPF tools (e.g., Falco, Cilium Tetragon, bcc)",
                "Open-source attestation services (e.g., from Confidential Computing Consortium)"
              ],
              "toolsCommercial": [
                "Microsoft Azure Confidential Computing",
                "Google Cloud Confidential Computing",
                "AWS Nitro Enclaves",
                "Intel TDX (Trust Domain Extensions)",
                "AMD SEV (Secure Encrypted Virtualization)",
                "Verifiable Computing solutions (e.g., from various startups in confidential computing space)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0018 Manipulate AI Model",
                    "AML.T0018.002 Manipulate AI Model: Embed Malware",
                    "AML.T0072 Reverse Shell",
                    "AML.T0025 Exfiltration via Cyber Means"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Memory Corruption (L4)",
                    "Runtime Code Injection (L4)",
                    "Compromised Training Environment (L4)",
                    "Data Exfiltration (L2)",
                    "Model Tampering (L1)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM03:2025 Supply Chain",
                    "LLM06:2025 Excessive Agency",
                    "LLM02:2025 Sensitive Information Disclosure"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML06:2023 AI Supply Chain Attacks",
                    "ML05:2023 Model Theft",
                    "ML09:2023 Output Integrity Attack"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "unauthorized modification",
                  "memory",
                  "weights",
                  "injection",
                  "tampering",
                  "corruption",
                  "manipulation",
                  "forgery"
                ],
                "defense": [
                  "runtime",
                  "attestation",
                  "memory",
                  "integrity",
                  "enclave",
                  "detect",
                  "patching",
                  "verify",
                  "deny",
                  "monitor"
                ]
              }
            },
            {
              "id": "AID-D-004.003",
              "name": "Runtime Configuration & Policy Drift Detection and Monitoring",
              "description": "Continuously detect and respond to unauthorized or out-of-process changes to AI-serving configurations, such as model-serving YAMLs, feature-store ACLs, RAG index schemas, and inference-time policy files. The goal is to ensure that what is actually running in production always matches what was formally approved, version-controlled, and reviewed. This prevents silent config drift and prevents attackers or rushed operators from weakening runtime protections.",
              "pillar": "infra",
              "phase": "operation",
              "implementationStrategies": [
                "Store configs in Git with signed commits; enable a secured webhook that validates commit signatures.",
                "Continuously diff live cluster state (e.g. Kubernetes ConfigMaps) against the declared IaC, and auto-heal drift.",
                "Block risky runtime changes (privileged mounts, unsafe network exposure, downgraded access controls) using admission policies."
              ],
              "toolsOpenSource": [
                "Git (for version control and signed commits)",
                "GitHub/GitLab/Bitbucket webhooks",
                "Argo CD",
                "Flux CD",
                "Open Policy Agent (OPA) / Gatekeeper",
                "Kyverno",
                "Terraform, CloudFormation, Ansible (for IaC enforcement)"
              ],
              "toolsCommercial": [
                "Cloud Security Posture Management (CSPM) tools (e.g., Wiz, Prisma Cloud, Microsoft Defender for Cloud)",
                "Configuration Management Databases (CMDBs)",
                "Enterprise Git solutions (e.g., GitHub Enterprise, GitLab Ultimate)",
                "Commercial GitOps platforms"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0018 Manipulate AI Model",
                    "AML.T0069 Discover LLM System Information"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Misconfigurations (L4: Deployment & Infrastructure)",
                    "Data Tampering (L2: Data Operations)",
                    "Unauthorized Access (Cross-Layer)",
                    "Compromised Agent Registry (L7: Agent Ecosystem)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM03:2025 Supply Chain",
                    "LLM07:2025 System Prompt Leakage",
                    "LLM08:2025 Vector and Embedding Weaknesses"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML06:2023 AI Supply Chain Attacks",
                    "ML09:2023 Output Integrity Attack",
                    "ML10:2023 Model Poisoning",
                    "ML08:2023 Model Skewing"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "unauthorized access",
                  "privilege escalation",
                  "lateral movement",
                  "stealth attack",
                  "persistent threat",
                  "unauthorized",
                  "state",
                  "risky",
                  "exposure",
                  "evasion",
                  "undetected",
                  "bypass",
                  "stealth",
                  "obfuscation",
                  "hiding"
                ],
                "defense": [
                  "access control",
                  "drift detection",
                  "runtime",
                  "configuration",
                  "policy",
                  "drift",
                  "detection",
                  "monitoring",
                  "detect",
                  "signed",
                  "secured",
                  "block",
                  "policies"
                ]
              }
            },
            {
              "id": "AID-D-004.004",
              "name": "Model Source & Namespace Drift Detection",
              "description": "A set of high-signal detective controls that monitor for symptoms of a model namespace reuse attack or supply chain policy failure. This technique focuses on detecting lifecycle changes in external model repositories (e.g., deletions, redirects) during the curation process and on identifying unexpected network traffic from production systems to public model hubs at runtime.",
              "pillar": "infra",
              "phase": "validation",
              "implementationStrategies": [
                "Alert on 404 or 3xx status codes when validating external model URLs during CI curation.",
                "Monitor for runtime DNS queries or egress traffic from production pods to public model hubs."
              ],
              "toolsOpenSource": [
                "Falco, Cilium Tetragon",
                "ELK Stack/OpenSearch, Splunk",
                "Custom scripts using `curl`"
              ],
              "toolsCommercial": [
                "SIEM Platforms (Splunk, Sentinel, Chronicle)",
                "Cloud Provider Network Monitoring (VPC Flow Logs, AWS GuardDuty)",
                "EDR/XDR solutions"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0010.003: AI Supply Chain Compromise: Model",
                    "AML.T0074 Masquerading"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Supply Chain Attacks (Cross-Layer)",
                    "Lateral Movement (Cross-Layer)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM03:2025 Supply Chain"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML06:2023 AI Supply Chain Attacks"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "stealth attack",
                  "anomalous behavior",
                  "attack",
                  "evasion",
                  "bypass",
                  "stealth",
                  "obfuscation",
                  "hiding",
                  "intrusion"
                ],
                "defense": [
                  "drift detection",
                  "model",
                  "source",
                  "namespace",
                  "drift",
                  "detection",
                  "monitor",
                  "policy",
                  "detecting",
                  "repositories",
                  "alert",
                  "validating"
                ]
              }
            },
            {
              "id": "AID-D-004.005",
              "name": "Runtime Prompt Integrity Verification",
              "description": "A runtime mechanism that ensures the integrity and provenance of every turn in a conversational context. It involves cryptographically binding each prompt or tool output to its content and origin within a structured, canonical 'turn envelope'. This creates a verifiable, chained history that is validated before every LLM call to detect and block tampering, context manipulation, or prompt infection attacks. This technique adds a crucial layer of runtime security for the dynamic conversational state, complementing static artifact integrity checks.",
              "pillar": "app",
              "phase": "operation",
              "implementationStrategies": [
                "Define a canonical 'turn envelope' for all context entries.",
                "Chain context entries using an anchored integrity mechanism (Signing or HMAC).",
                "Verify the entire context chain before every LLM call.",
                "Handle binary or multimodal content via out-of-band hashing.",
                "Maintain a secure audit ledger for provenance and non-repudiation."
              ],
              "toolsOpenSource": [
                "Cryptographic libraries (Python's hashlib, pyca/cryptography; Node.js's crypto)",
                "Workload Identity Systems (SPIFFE/SPIRE)",
                "Key Management (HashiCorp Vault)",
                "SIEM/Log Analytics (ELK Stack, OpenSearch) for audit ledgers"
              ],
              "toolsCommercial": [
                "Key Management Services (AWS KMS, Azure Key Vault, Google Cloud KMS)",
                "Hardware Security Modules (HSMs) for signing operations",
                "IDaaS Platforms (Okta, Auth0) for user identity context",
                "SIEM Platforms (Splunk, Datadog, Microsoft Sentinel)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0018 Manipulate AI Model (via context manipulation)",
                    "AML.T0074 Masquerading (by binding prompts to a verifiable origin)",
                    "AML.T0061 LLM Prompt Self-Replication (Prompt Infection)"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Agent Goal Manipulation (L7)",
                    "Repudiation (L7)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM01:2025 Prompt Injection (especially indirect and multi-turn attacks)",
                    "LLM03:2025 Supply Chain (by verifying outputs from chained tools)"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML09:2023 Output Integrity Attack (when verifying tool outputs)"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "data poisoning",
                  "supply chain attack",
                  "untrusted data",
                  "unauthorized modification",
                  "man in the middle",
                  "prompt",
                  "context",
                  "tampering",
                  "manipulation",
                  "state",
                  "artifact",
                  "corruption",
                  "forgery",
                  "interception",
                  "repudiation"
                ],
                "defense": [
                  "integrity verification",
                  "key management",
                  "runtime",
                  "prompt",
                  "integrity",
                  "verification",
                  "provenance",
                  "validated",
                  "detect",
                  "block",
                  "signing",
                  "hmac",
                  "verify",
                  "hashing",
                  "secure"
                ]
              }
            }
          ],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "unauthorized modification",
              "man in the middle",
              "stealth attack",
              "artifact",
              "tamper",
              "parameters",
              "unauthorized",
              "tampering",
              "compromise",
              "artifacts",
              "corruption",
              "manipulation",
              "forgery",
              "interception",
              "evasion"
            ],
            "defense": [
              "model",
              "artifact",
              "integrity",
              "monitoring",
              "audit",
              "tamper",
              "detection",
              "verify",
              "cryptographic",
              "detect",
              "confidentiality"
            ]
          }
        },
        {
          "id": "AID-D-005",
          "name": "AI Activity Logging, Monitoring & Threat Hunting",
          "description": "Establish and maintain detailed, comprehensive, and auditable logs of all significant activities related to AI systems. This includes user queries and prompts, model responses and confidence scores, decisions made by AI (especially autonomous agents), tools invoked by agents, data accessed or modified, API calls (to and from the AI system), system errors, and security-relevant events. These logs are then ingested into security monitoring systems (e.g., SIEM) for correlation, automated alerting on suspicious patterns, and proactive threat hunting by security analysts to identify indicators of compromise (IoCs) or novel attack patterns targeting AI systems.",
          "pillar": "infra",
          "phase": "operation",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0024.002 Exfiltration via AI Inference API: Extract AI Model (query patterns)",
                "AML.T0051 LLM Prompt Injection (repeated attempts)",
                "AML.T0057 LLM Data Leakage (output logging)",
                "AML.T0012 Valid Accounts (anomalous usage)",
                "AML.T0046 Spamming AI System with Chaff Data"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Model Stealing (L1)",
                "Agent Tool Misuse (L7)",
                "Compromised RAG Pipelines (L2)",
                "Data Exfiltration (L2)",
                "Repudiation (L7)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM10:2025 Unbounded Consumption (usage patterns)",
                "LLM01:2025 Prompt Injection (logged attempts)",
                "LLM02:2025 Sensitive Information Disclosure (logged outputs)",
                "LLM06:2025 Excessive Agency (logged actions)"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML05:2023 Model Theft (query patterns)",
                "ML01:2023 Input Manipulation Attack (logged inputs)"
              ]
            }
          ],
          "implementationStrategies": [],
          "toolsOpenSource": [],
          "toolsCommercial": [],
          "subTechniques": [
            {
              "id": "AID-D-005.001",
              "name": "AI System Log Generation & Collection",
              "description": "This foundational technique covers the instrumentation of AI applications to produce detailed, structured logs for all significant events, and the implementation of a secure pipeline to collect and forward these logs to a central analysis platform. The goal is to create a high-fidelity, auditable record of system activity, which is a prerequisite for all other detection, investigation, and threat hunting capabilities.",
              "pillar": "infra",
              "phase": "operation",
              "implementationStrategies": [
                "Implement structured, context-rich logging for all AI interactions.",
                "Log agentic intermediate steps (thoughts, plans, tool calls).",
                "Use a dedicated log shipper for secure and reliable collection.",
                "Ensure logs are timestamped, immutable, and stored in a tamper-evident archive."
              ],
              "toolsOpenSource": [
                "logging (Python library), loguru, structlog",
                "Fluentd, Vector, Logstash (log shippers)",
                "Apache Kafka, AWS Kinesis (event streaming)",
                "OpenTelemetry",
                "Prometheus (for metrics)"
              ],
              "toolsCommercial": [
                "Datadog",
                "Splunk Enterprise",
                "New Relic",
                "Logz.io",
                "AWS CloudWatch Logs",
                "Google Cloud Logging",
                "Azure Monitor Logs"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "Enables detection of: AML.TA0002 Reconnaissance (unusual query patterns)",
                    "AML.T0024 Exfiltration via AI Inference API (anomalous data in logs)",
                    "AML.T0051 LLM Prompt Injection (repeated injection attempts)",
                    "AML.T0046 Spamming AI System with Chaff Data (high volume from single source)"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Enables detection for: Misinformation Generation (by logging outputs)",
                    "Agent Tool Misuse (L7)",
                    "Data Exfiltration (L2)",
                    "Resource Hijacking (L4)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "Enables detection of: LLM01:2025 Prompt Injection (logging the attempts)",
                    "LLM02:2025 Sensitive Information Disclosure (logging the outputs)",
                    "LLM06:2025 Excessive Agency (logging agent actions)",
                    "LLM10:2025 Unbounded Consumption (logging usage patterns)"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "Enables detection of: ML01:2023 Input Manipulation Attack (logging malicious inputs)",
                    "ML05:2023 Model Theft (logging high-volume query patterns)"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "forensic evasion",
                  "cover tracks",
                  "log injection",
                  "threat",
                  "evasion",
                  "bypass",
                  "stealth",
                  "obfuscation",
                  "hiding",
                  "repudiation",
                  "tampering",
                  "unauthorized"
                ],
                "defense": [
                  "system",
                  "log",
                  "generation",
                  "collection",
                  "secure",
                  "analysis",
                  "detection",
                  "logging",
                  "reliable",
                  "monitoring",
                  "alerting",
                  "anomaly",
                  "inspection",
                  "observability",
                  "audit"
                ]
              }
            },
            {
              "id": "AID-D-005.002",
              "name": "Security Monitoring & Alerting for AI",
              "description": "This technique covers the real-time monitoring of ingested AI system logs and the creation of specific rules to detect and generate alerts for known suspicious or malicious patterns. It focuses on the operational security task of identifying potential threats as they occur by comparing live activity against predefined attack signatures and behavioral heuristics. This is the core function of a Security Operations Center (SOC) in defending AI systems.",
              "pillar": "infra",
              "phase": "operation",
              "implementationStrategies": [
                "Ingest AI-specific logs into a centralized SIEM/log analytics platform.",
                "Develop and deploy AI-specific detection rules.",
                "Correlate AI system logs with other security data sources.",
                "Integrate SIEM alerts with SOAR platforms for automated response."
              ],
              "toolsOpenSource": [
                "ELK Stack / OpenSearch (with alerting features)",
                "Grafana Loki with Promtail",
                "Wazuh",
                "Sigma (for defining SIEM rules in a standard format)",
                "ElastAlert"
              ],
              "toolsCommercial": [
                "Splunk Enterprise Security",
                "Microsoft Sentinel",
                "Google Chronicle",
                "IBM QRadar",
                "Datadog Security Platform",
                "Exabeam",
                "LogRhythm"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0051 LLM Prompt Injection (detecting repeated attempts)",
                    "AML.T0024.002 Invert AI Model (detecting high query volumes)",
                    "AML.T0012 Valid Accounts (detecting anomalous usage from an account)",
                    "AML.T0046 Spamming AI System with Chaff Data",
                    "AML.T0055 Unsecured Credentials (detecting use of known compromised keys)"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Model Stealing (L1)",
                    "Agent Tool Misuse (L7)",
                    "DoS on Framework APIs (L3)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM01:2025 Prompt Injection",
                    "LLM06:2025 Excessive Agency",
                    "LLM10:2025 Unbounded Consumption"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML05:2023 Model Theft"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "stealth attack",
                  "persistent threat",
                  "suspicious",
                  "malicious",
                  "attack",
                  "evasion",
                  "undetected",
                  "bypass",
                  "stealth",
                  "obfuscation",
                  "hiding"
                ],
                "defense": [
                  "security monitoring",
                  "real-time monitoring",
                  "security",
                  "monitoring",
                  "alerting",
                  "rules",
                  "detect",
                  "defending",
                  "siem",
                  "log",
                  "detection",
                  "soar"
                ]
              }
            },
            {
              "id": "AID-D-005.003",
              "name": "Proactive AI Threat Hunting",
              "description": "This technique covers the proactive, hypothesis-driven search through AI system logs and telemetry for subtle, unknown, or 'low-and-slow' attacks that do not trigger predefined alerts. Threat hunting assumes an attacker may already be present and evading standard detections. It focuses on identifying novel attack patterns, reconnaissance activities, and anomalous behaviors by using exploratory data analysis, complex queries, and machine learning on historical data.",
              "pillar": "infra",
              "phase": "operation",
              "implementationStrategies": [
                "Formulate hypotheses based on AI threat models (ATLAS, MAESTRO) and hunt for corresponding TTPs.",
                "Use clustering to find anomalous user or agent sessions.",
                "Hunt for data exfiltration patterns in RAG systems."
              ],
              "toolsOpenSource": [
                "Jupyter Notebooks (with Pandas, Scikit-learn, Matplotlib)",
                "SIEM query languages (Splunk SPL, OpenSearch DQL)",
                "Graph analytics tools (NetworkX)",
                "Threat intelligence platforms (MISP)",
                "Data processing frameworks (Apache Spark)"
              ],
              "toolsCommercial": [
                "Threat hunting platforms (Splunk User Behavior Analytics, Elastic Security, SentinelOne)",
                "Notebook environments (Databricks, Hex)",
                "Threat intelligence feeds (Mandiant, Recorded Future)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0024.002 Invert AI Model (by finding probing patterns)",
                    "AML.TA0002 Reconnaissance (finding subtle scanning)",
                    "AML.T0057 LLM Data Leakage (finding low-and-slow exfiltration)",
                    "Novel variants of AML.T0015 (Evade AI Model)"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Model Stealing (L1)",
                    "Evasion of Detection (L5)",
                    "Malicious Agent Discovery (L7)",
                    "Data Exfiltration (L2)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM02:2025 Sensitive Information Disclosure (finding subtle leaks)",
                    "LLM05:2025 Improper Output Handling (finding patterns of abuse)"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML05:2023 Model Theft",
                    "ML04:2023 Membership Inference Attack (detecting probing patterns)"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "data exfiltration",
                  "anomalous behavior",
                  "threat",
                  "attacker",
                  "evading",
                  "attack",
                  "anomalous",
                  "exfiltration",
                  "evasion",
                  "bypass",
                  "stealth",
                  "obfuscation",
                  "hiding"
                ],
                "defense": [
                  "threat modeling",
                  "proactive",
                  "threat",
                  "hunting",
                  "analysis",
                  "sessions",
                  "detection",
                  "monitoring",
                  "alerting",
                  "anomaly",
                  "inspection",
                  "observability",
                  "logging",
                  "audit"
                ]
              }
            },
            {
              "id": "AID-D-005.004",
              "name": "Specialized Agent & Session Logging",
              "description": "This technique covers the highly specialized logging required for autonomous and agentic AI systems, which goes beyond standard API request/response logging. It involves instrumenting the agent's internal decision-making loop to capture its goals, plans, intermediate thoughts, tool selections, and interactions with memory or knowledge bases. This detailed audit trail is essential for debugging, ensuring compliance, and detecting complex threats like goal manipulation or emergent, unsafe behaviors.",
              "pillar": "app",
              "phase": "operation",
              "implementationStrategies": [
                "Log the agent's full reasoning loop (goal, thought, action, observation) with forensic quality.",
                "Log all interactions with external knowledge bases (RAG) in a minimally sensitive form.",
                "Log secure session initialization to bind identity, integrity, and trust state.",
                "Log every Human-in-the-Loop (HITL) intervention with operator identity and justification."
              ],
              "toolsOpenSource": [
                "Agentic frameworks with callback/handler systems (LangChain, AutoGen, CrewAI, LlamaIndex)",
                "Standard logging libraries (Python `logging`, `loguru`)",
                "Workload identity systems (SPIFFE/SPIRE)",
                "OpenTelemetry (for distributed tracing of agent actions)"
              ],
              "toolsCommercial": [
                "AI Observability and monitoring platforms (Arize AI, Fiddler, WhyLabs, Datadog, New Relic)",
                "Agent-specific security and governance platforms (Lasso Security, Credo AI)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0053 (AI Agent Tool Invocation)",
                    "AML.T0061 (LLM Prompt Self-Replication)"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Enables detection for: Agent Goal Manipulation (L7)",
                    "Agent Tool Misuse (L7)",
                    "Repudiation (L7)",
                    "Evasion of Auditing/Compliance (L6)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "Enables detection of: LLM06:2025 (Excessive Agency)",
                    "LLM01:2025 (Prompt Injection, by logging the full chain of events)",
                    "LLM08:2025 (Vector and Embedding Weaknesses, by logging RAG interactions)"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "Can help diagnose ML08:2023 (Model Skewing) if it manifests as anomalous agent behavior."
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "knowledge base",
                  "unauthorized modification",
                  "forensic evasion",
                  "cover tracks",
                  "log injection",
                  "apt",
                  "memory",
                  "manipulation",
                  "state",
                  "tampering",
                  "corruption",
                  "forgery",
                  "repudiation",
                  "unauthorized"
                ],
                "defense": [
                  "audit trail",
                  "specialized",
                  "agent",
                  "session",
                  "logging",
                  "human-in-the-loop",
                  "audit",
                  "compliance",
                  "detecting",
                  "log",
                  "secure",
                  "integrity",
                  "trust"
                ]
              }
            },
            {
              "id": "AID-D-005.005",
              "name": "Accelerator Telemetry Anomaly Detection",
              "description": "Continuously baseline and monitor accelerator telemetry (power, temperature, utilization, PMCs). Alert on deviations indicating cryptomining, DoS, or side-channel probing.",
              "pillar": "infra",
              "phase": "operation",
              "implementationStrategies": [
                "Establish statistical baselines (mean/std) under representative workloads; compare live metrics and alert on 3-sigma deviations."
              ],
              "toolsOpenSource": [
                "NVIDIA DCGM Exporter",
                "Prometheus",
                "Grafana"
              ],
              "toolsCommercial": [
                "Datadog",
                "New Relic",
                "Splunk Observability"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0029 Denial of AI Service",
                    "AML.T0034 Cost Harvesting",
                    "AML.T0024.002 Invert AI Model (if using side-channels)"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Resource Hijacking (L4)",
                    "Side-Channel Attacks (L4)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM10:2025 Unbounded Consumption"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML06:2023 AI Supply Chain Attacks (if malware is introduced)"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "anomalous behavior",
                  "novel attack",
                  "unknown threat",
                  "drift",
                  "deviation",
                  "degradation",
                  "manipulation",
                  "evasion",
                  "bypass",
                  "stealth",
                  "obfuscation",
                  "hiding",
                  "zero-day"
                ],
                "defense": [
                  "anomaly detection",
                  "accelerator",
                  "telemetry",
                  "anomaly",
                  "detection",
                  "baseline",
                  "monitor",
                  "alert",
                  "monitoring",
                  "alerting",
                  "analysis",
                  "inspection",
                  "observability",
                  "logging",
                  "audit"
                ]
              }
            },
            {
              "id": "AID-D-005.006",
              "name": "ANS Registry & Resolution Telemetry Monitoring",
              "description": "Monitors Agent Name Service (ANS) registration events and resolution traffic to identify anomalies indicative of registry poisoning, Sybil-style namespace abuse, directory reconnaissance, or credential churn. It correlates identity, issuer, and query outcomes (e.g., NXDOMAIN/Agent Not Found, version-range mismatches) into actionable security alerts.",
              "pillar": "infra",
              "phase": "operation",
              "implementationStrategies": [
                "Registration Churn & Namespace Abuse Detection",
                "Resolution Anomaly & Reconnaissance Detection"
              ],
              "toolsOpenSource": [
                "Prometheus",
                "Grafana",
                "ELK Stack",
                "Nginx",
                "Falco"
              ],
              "toolsCommercial": [
                "Datadog",
                "Splunk",
                "Dynatrace",
                "AWS CloudWatch"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0029 Denial of AI Service",
                    "AML.T0034 Cost Harvesting"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Agent Impersonation (L7)",
                    "Directory Scanning (L7)",
                    "Resource Hijacking (L4)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM10:2025 Unbounded Consumption"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "stealth attack",
                  "persistent threat",
                  "novel attack",
                  "unknown threat",
                  "poisoning",
                  "abuse",
                  "credential",
                  "evasion",
                  "undetected",
                  "bypass",
                  "stealth",
                  "obfuscation",
                  "hiding",
                  "zero-day"
                ],
                "defense": [
                  "ans",
                  "registry",
                  "resolution",
                  "telemetry",
                  "monitoring",
                  "credential",
                  "detection",
                  "alerting",
                  "analysis",
                  "anomaly",
                  "inspection",
                  "observability",
                  "logging",
                  "audit"
                ]
              }
            }
          ],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "stealth attack",
              "persistent threat",
              "forensic evasion",
              "cover tracks",
              "log injection",
              "threat",
              "prompts",
              "suspicious",
              "compromise",
              "attack",
              "evasion",
              "undetected",
              "repudiation",
              "tampering",
              "unauthorized"
            ],
            "defense": [
              "security monitoring",
              "activity logging",
              "activity",
              "logging",
              "monitoring",
              "threat",
              "hunting",
              "siem",
              "alerting",
              "detection",
              "analysis",
              "anomaly",
              "inspection",
              "observability",
              "audit"
            ]
          }
        },
        {
          "id": "AID-D-006",
          "name": "Explainability (XAI) Manipulation Detection",
          "description": "Implement mechanisms to monitor and validate the outputs and behavior of eXplainable AI (XAI) methods. The goal is to detect attempts by adversaries to manipulate or mislead these explanations, ensuring that XAI outputs accurately reflect the model's decision-making process and are not crafted to conceal malicious operations, biases, or vulnerabilities. This is crucial if XAI is used for debugging, compliance, security monitoring, or building user trust.",
          "pillar": "model",
          "phase": "validation",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.TA0007 Defense Evasion (if XAI is part of a defensive monitoring system and is itself targeted to be fooled). Potentially a new ATLAS technique: \"AML.TXXXX Manipulate AI Explainability\"."
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Evasion of Auditing/Compliance (L6: Security & Compliance, if manipulated XAI is used to mislead auditors)",
                "Manipulation of Evaluation Metrics (L5: Evaluation & Observability, if explanations are used as part of the evaluation and are unreliable)",
                "Obfuscation of Malicious Behavior (Cross-Layer).",
                "Lack of Explainability in Security AI Agents (L6)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "Indirectly supports investigation of LLM01:2025 Prompt Injection or LLM04:2025 Data and Model Poisoning by ensuring that any XAI methods used to understand the resulting behavior are themselves trustworthy."
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "Indirectly supports diagnosis of ML08:2023 Model Skewing or ML10:2023 Model Poisoning, by ensuring XAI methods used to identify these issues are not being manipulated."
              ]
            }
          ],
          "implementationStrategies": [
            "Employ multiple, diverse XAI methods to explain the same model decision and compare their outputs for consistency; significant divergence can indicate manipulation or instability.",
            "Establish baselines for typical explanation characteristics and monitor for deviations.",
            "Detect instability where tiny input perturbations cause radically different explanations while the prediction stays the same.",
            "Monitor for explanations that are overly simplistic, highlight irrelevant features, or ignore known-critical features.",
            "Specifically test against adversarial attacks designed to fool XAI methods (e.g., \"adversarial explanations\" where the explanation is misleading but the prediction remains unchanged or changes benignly).",
            "Log XAI outputs and any detected manipulation alerts for investigation by AI assurance and security teams."
          ],
          "toolsOpenSource": [
            "XAI libraries (e.g., SHAP, LIME, Captum for PyTorch, Alibi Explain, ELI5, InterpretML).",
            "Custom-developed logic for comparing and validating consistency between different explanation outputs.",
            "Research toolkits for adversarial attacks on XAI (if available for benchmarking)."
          ],
          "toolsCommercial": [
            "AI Observability and Monitoring platforms (e.g., Fiddler, Arize AI, WhyLabs) that include XAI features may incorporate or allow the development of robustness checks and manipulation detection for explanations.",
            "Specialized AI assurance or red teaming tools that assess XAI method reliability."
          ],
          "subTechniques": [],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "adversarial attack",
              "anomalous behavior",
              "bias injection",
              "hidden behavior",
              "manipulation",
              "adversaries",
              "manipulate",
              "malicious",
              "vulnerabilities",
              "adversarial",
              "drift",
              "deviation",
              "degradation",
              "deception",
              "evasion"
            ],
            "defense": [
              "security monitoring",
              "explainability",
              "xai",
              "manipulation",
              "detection",
              "monitor",
              "validate",
              "detect",
              "compliance",
              "monitoring",
              "trust",
              "log",
              "detected"
            ]
          }
        },
        {
          "id": "AID-D-007",
          "name": "Multimodal Inconsistency Detection",
          "description": "For AI systems processing multiple input modalities (e.g., text, image, audio, video), implement mechanisms to detect and respond to inconsistencies, contradictions, or malicious instructions hidden via cross-modal interactions. This involves analyzing inputs and outputs across modalities to identify attempts to bypass security controls or manipulate one modality using another, and applying defenses to mitigate such threats. This is especially critical for detecting multimodal prompt injection (e.g. hidden instructions in images or audio that override the text instruction channel) and preventing single-modality takeover of agent behavior.",
          "pillar": "data",
          "phase": "operation",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0051 LLM Prompt Injection (specifically cross-modal variants like in Scenario #7 of LLM01:2025 )",
                "AML.T0015 Evade AI Model (if evasion exploits multimodal vulnerabilities)",
                "AML.T0043 Craft Adversarial Data (for multimodal adversarial examples)."
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Cross-Modal Manipulation Attacks (L1: Foundation Models / L2: Data Operations)",
                "Input Validation Attacks (L3: Agent Frameworks, for multimodal inputs)",
                "Data Poisoning (L2: Data Operations, if multimodal data is used for poisoning and inconsistencies are introduced)."
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM01:2025 Prompt Injection (specifically Multimodal Injection Scenario #7)",
                "LLM04:2025 Data and Model Poisoning (if using tainted or inconsistent multimodal data)",
                "LLM08:2025 Vector and Embedding Weaknesses (if multimodal embeddings are manipulated or store inconsistent data)."
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML01:2023 Input Manipulation Attack (specifically for multimodal inputs)",
                "ML02:2023 Data Poisoning Attack (using inconsistent or malicious multimodal data)."
              ]
            }
          ],
          "implementationStrategies": [
            "Implement semantic consistency checks between information extracted from different modalities (e.g., verify alignment between text captions and image content; ensure audio commands do not contradict visual cues).",
            "Scan non-primary modalities for embedded instructions or payloads intended for other modalities (e.g., steganographically hidden text in images, QR codes containing malicious prompts, audio watermarks with commands).",
            "Utilize separate, specialized validation and sanitization pipelines for each modality before data fusion (as outlined in enhancements to AID-H-002).",
            "Monitor the AI model's internal attention mechanisms (if accessible and interpretable) for unusual or forced cross-modal attention patterns that might indicate manipulation.",
            "Develop and maintain a library of known cross-modal attack patterns and use this knowledge to inform detection rules and defensive transformations.",
            "During output generation, verify that outputs are consistent with the fused understanding from all input modalities and do not disproportionately reflect manipulation from a single, potentially compromised, modality.",
            "Employ ensemble methods where different sub-models or experts process different modalities, with a final decision layer that checks for consensus or flags suspicious discrepancies for human review or automated rejection.",
            "Implement context-aware filtering that considers the typical relationships and constraints between modalities for a given task."
          ],
          "toolsOpenSource": [
            "Computer vision libraries (OpenCV, Pillow) for image analysis (e.g., detecting text in images, QR code scanning, deepfake detection).",
            "NLP libraries (spaCy, NLTK, Hugging Face Transformers) for text analysis and cross-referencing with visual/audio data.",
            "Audio processing libraries (Librosa, PyAudio, SpeechRecognition) for audio analysis and transcription for cross-checking.",
            "Steganography detection tools (e.g., StegDetect, Aletheia, Zsteg).",
            "Custom rule engines (e.g., based on Drools, or custom Python scripting) for implementing consistency checks.",
            "Multimodal foundation models themselves (e.g., fine-tuned smaller models acting as \\\"watchdogs\\\" for larger ones)."
          ],
          "toolsCommercial": [
            "Multimodal AI security platforms (emerging market, offering integrated analysis).",
            "Advanced data validation platforms with support for multiple data types and cross-validation.",
            "Content moderation services that handle and analyze multiple modalities for policy violations or malicious content.",
            "AI red teaming services specializing in multimodal systems."
          ],
          "subTechniques": [],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "prompt injection",
              "apt",
              "malicious",
              "bypass",
              "manipulate",
              "prompt",
              "injection",
              "takeover",
              "prompts",
              "manipulation",
              "attack",
              "compromised",
              "suspicious"
            ],
            "defense": [
              "human review",
              "data validation",
              "multimodal",
              "inconsistency",
              "detection",
              "detect",
              "analyzing",
              "mitigate",
              "detecting",
              "preventing",
              "verify",
              "scan",
              "containing",
              "validation",
              "sanitization"
            ]
          }
        },
        {
          "id": "AID-D-008",
          "name": "AI-Based Security Analytics for AI systems",
          "description": "Employ specialized AI/ML models (secondary AI defenders) to analyze telemetry, logs, and behavioral patterns from primary AI systems to detect sophisticated, subtle, or novel attacks that may evade rule-based or traditional detection methods. This includes identifying anomalous interactions, emergent malicious behaviors, coordinated attacks, or signs of AI-generated attacks targeting the primary AI systems.",
          "pillar": "data",
          "phase": "operation",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "Many tactics by providing an advanced detection layer. Particularly useful against novel or evasive variants of AML.T0015 Evade AI Model, AML.T0051 LLM Prompt Injection, AML.T0024.002 Invert AI Model, AML.TA0007 Active Scanning & Probing, and sophisticated reconnaissance activities (AML.TA0001). Could also help detect AI-generated attacks if their patterns differ from human-initiated ones."
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Advanced Evasion Techniques (L1, L5, L6)",
                "Subtle Data or Model Poisoning effects not caught by simpler checks (L1, L2)",
                "Sophisticated Agent Manipulation (L7)",
                "Novel Attack Vectors (Cross-Layer)",
                "Resource Hijacking (L4, through anomalous pattern detection)."
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM01:2025 Prompt Injection (novel or obfuscated injections)",
                "LLM06:2025 Excessive Agency (subtle deviations in agent behavior)",
                "LLM10:2025 Unbounded Consumption (anomalous resource usage patterns indicating DoS or economic attacks)."
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML01:2023 Input Manipulation Attack (sophisticated adversarial inputs)",
                "ML05:2023 Model Theft (anomalous query patterns indicative of advanced extraction)",
                "ML02:2023 Data Poisoning Attack (detecting subtle behavioral shifts post-deployment)."
              ]
            }
          ],
          "implementationStrategies": [
            "Train anomaly detection models (e.g., autoencoders, GMMs, Isolation Forests) on logs and telemetry from AI systems, including API call sequences, resource usage patterns, query structures, and agent actions.",
            "Develop supervised classifiers (e.g., Random Forest, Gradient Boosting, Neural Networks) to categorize interactions as benign or potentially malicious based on learned patterns from known attacks and normal baselines.",
            "Use AI for advanced threat hunting within AI system logs, identifying complex attack sequences, low-and-slow reconnaissance, or unusual data access patterns by AI agents or users.",
            "Use AI-based drift detection to monitor for concept drift, data drift, or sudden performance degradation in primary AI models that might indicate an ongoing subtle attack (complements AID-D-002).",
            "Analyze AI agent behavior sequences (e.g., tool usage order, escalation of privileges, goal achievement patterns) for deviations from intended policies or safety constraints.",
            "Continuously retrain and update the secondary AI defender models with new attack data, evolving system behavior, and incident response feedback.",
            "Integrate outputs and alerts from AI defender models into the main SIEM/SOAR platforms for correlation, prioritization, and automated response orchestration.",
            "Use an ensemble of multiple anomaly detection techniques to reduce false positives and increase robustness against attacker evasion."
          ],
          "toolsOpenSource": [
            "General ML libraries (Scikit-learn, TensorFlow, PyTorch, Keras) for building custom detection models.",
            "Anomaly detection libraries (PyOD, Alibi Detect, TensorFlow Probability).",
            "Log analysis platforms (ELK Stack/OpenSearch with ML plugins, Apache Spot).",
            "Streaming data processing frameworks (Apache Kafka, Apache Flink, Apache Spark Streaming) for real-time AI analytics.",
            "Graph-based analytics libraries (NetworkX, PyTorch Geometric) for analyzing relationships in AI system activity."
          ],
          "toolsCommercial": [
            "Security AI platforms that offer AI-on-AI monitoring capabilities (e.g., some advanced EDR/XDR features, User and Entity Behavior Analytics (UEBA) tools).",
            "Specialized AI security monitoring solutions focusing on AI workload protection.",
            "AI-powered SIEMs or SOAR platforms with advanced analytics modules.",
            "Cloud provider ML services for building and deploying custom monitoring models (e.g., SageMaker, Vertex AI, Azure ML)."
          ],
          "subTechniques": [],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "agent actions",
              "coordinated attack",
              "performance degradation",
              "evade",
              "anomalous",
              "malicious",
              "gradient",
              "threat",
              "attack",
              "degradation",
              "attacker",
              "evasion"
            ],
            "defense": [
              "anomaly detection",
              "drift detection",
              "incident response",
              "ai-based",
              "security",
              "analytics",
              "systems",
              "analyze",
              "detect",
              "detection",
              "isolation",
              "monitor",
              "privileges",
              "policies",
              "safety"
            ]
          }
        },
        {
          "id": "AID-D-009",
          "name": "Cross-Agent Fact Verification & Hallucination Cascade Prevention",
          "description": "Implement real-time fact verification and consistency checking mechanisms across multiple AI agents to detect and prevent the propagation of hallucinated or false information through agent networks. This technique employs distributed consensus algorithms, external knowledge base validation, and inter-agent truth verification to break hallucination cascades before they spread through the system. This prevents a single compromised or hallucinating agent from polluting shared memory, RAG indexes, or downstream decision pipelines with fabricated or manipulated 'facts', and stops those false assertions from being amplified by other agents.",
          "pillar": "app",
          "phase": "operation",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0031 Erode AI Model Integrity",
                "AML.T0048.002 External Harms: Societal Harm",
                "AML.T0070 RAG Poisoning",
                "AML.T0066 Retrieval Content Crafting",
                "AML.T0067 LLM Trusted Output Components Manipulation",
                "AML.T0071 False RAG Entry Injection",
                "AML.T0062 Discover LLM Hallucinations (Prevents unverified hallucinations from being committed to shared memory and amplified by other agents)"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Data Poisoning (L2)",
                "Compromised RAG Pipelines (L2) (Prevents poisoned or unverified 'facts' from being persisted into shared retrieval indexes)",
                "Goal Misalignment Cascades (Cross-Layer) (Stops false statements from propagating across agents and being reinforced as 'truth')"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM09:2025 Misinformation (Prevents hallucinated or fabricated claims from being accepted, persisted, and rebroadcast as truth across agents)"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML08:2023 Model Skewing",
                "ML09:2023 Output Integrity Attack (Ensures fabricated agent claims aren't treated as authoritative facts or injected into downstream processes)"
              ]
            }
          ],
          "implementationStrategies": [
            "Deploy distributed fact-checking algorithms that cross-reference agent outputs with multiple trusted knowledge sources before accepting information as factual",
            "Implement inter-agent consensus mechanisms where critical facts must be verified by multiple independent agents before being accepted into shared knowledge bases",
            "Utilize external authoritative data sources (APIs, databases, knowledge graphs) for real-time fact verification of agent-generated content",
            "Deploy contradiction detection logic to block writes that conflict with existing 'truth' in the shared knowledge base",
            "Implement confidence scoring for agent-generated facts, and route low-confidence assertions for additional verification instead of auto-accepting them",
            "Create fact provenance tracking so every accepted fact has a verifiable origin, validation path, and audit trail",
            "Deploy circuit breakers that temporarily halt fact propagation if hallucination or verification failures spike"
          ],
          "toolsOpenSource": [
            "Apache Kafka with custom fact-verification consumers for distributed fact checking",
            "Neo4j or ArangoDB for knowledge graph-based fact verification",
            "Apache Airflow for orchestrating complex fact-verification workflows",
            "Redis or Apache Ignite for high-speed fact caching and consistency checking",
            "Custom Python libraries using spaCy, NLTK for natural language fact extraction and comparison"
          ],
          "toolsCommercial": [
            "Google Knowledge Graph API for external fact verification",
            "Microsoft Cognitive Services for content verification",
            "Palantir Foundry for large-scale data consistency and verification",
            "Databricks with MLflow for distributed ML-based fact verification",
            "Neo4j Enterprise for enterprise-grade knowledge graph verification"
          ],
          "subTechniques": [],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "knowledge base",
              "data poisoning",
              "supply chain attack",
              "untrusted data",
              "malicious input",
              "invalid input",
              "compromised",
              "memory",
              "manipulated",
              "tampering",
              "injection",
              "bypass",
              "exploit",
              "evasion",
              "stealth"
            ],
            "defense": [
              "circuit breaker",
              "audit trail",
              "provenance tracking",
              "cross-agent",
              "fact",
              "verification",
              "hallucination",
              "cascade",
              "prevention",
              "detect",
              "prevent",
              "validation",
              "trusted",
              "verified",
              "detection"
            ]
          }
        },
        {
          "id": "AID-D-010",
          "name": "AI Goal Integrity Monitoring & Deviation Detection",
          "description": "Continuously monitor and validate AI agent goals, objectives, and decision-making patterns to detect unauthorized goal manipulation or intent deviation. This technique establishes cryptographically signed goal states, implements goal consistency verification, and provides real-time alerting when agents deviate from their intended objectives or exhibit goal manipulation indicators.",
          "pillar": "app",
          "phase": "operation",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0051 LLM Prompt Injection",
                "AML.T0054 LLM Jailbreak",
                "AML.T0078 Drive-by Compromise",
                "AML.T0018 Manipulate AI Model"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Agent Goal Manipulation (L7)",
                "Agent Tool Misuse (L7)",
                "Agent Impersonation (L7)",
                "Agent Identity Attack (L7)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM01:2025 Prompt Injection",
                "LLM06:2025 Excessive Agency"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML01:2023 Input Manipulation Attack",
                "ML09:2023 Output Integrity Attack (Detects or prevents altered / fabricated model output that could mislead downstream systems)"
              ]
            }
          ],
          "implementationStrategies": [
            "Implement cryptographic signing of agent goals and objectives to prevent unauthorized modification",
            "Deploy continuous goal consistency checking algorithms that verify agent actions align with stated objectives",
            "Create goal deviation scoring systems that quantify how far agent behavior has drifted from intended goals",
            "Implement multi-agent goal verification where critical goal changes require consensus from multiple oversight agents",
            "Deploy behavioral pattern analysis to detect subtle goal manipulation that doesn't trigger direct goal modification alerts",
            "Create goal rollback mechanisms to restore agents to previous validated goal states when manipulation is detected",
            "Implement goal provenance tracking to audit the complete history of goal modifications and their sources"
          ],
          "toolsOpenSource": [
            "HashiCorp Vault for cryptographic goal signing and verification",
            "Apache Kafka for real-time goal monitoring event streaming",
            "Prometheus and Grafana for goal deviation metrics and alerting",
            "Redis for fast goal state caching and comparison",
            "Custom Python frameworks using cryptography libraries for goal integrity verification"
          ],
          "toolsCommercial": [
            "CyberArk for privileged goal management and protection",
            "Splunk for advanced goal deviation analytics and correlation",
            "Datadog for real-time goal monitoring and alerting",
            "HashiCorp Vault Enterprise for enterprise goal state management",
            "IBM QRadar for goal manipulation threat detection"
          ],
          "subTechniques": [],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "agent actions",
              "data poisoning",
              "supply chain attack",
              "untrusted data",
              "unauthorized modification",
              "man in the middle",
              "stealth attack",
              "persistent threat",
              "unauthorized",
              "manipulation",
              "tampering",
              "corruption",
              "forgery",
              "interception",
              "evasion"
            ],
            "defense": [
              "cryptographic signing",
              "deviation detection",
              "provenance tracking",
              "goal",
              "integrity",
              "monitoring",
              "deviation",
              "detection",
              "monitor",
              "validate",
              "detect",
              "signed",
              "verification",
              "alerting",
              "cryptographic"
            ]
          }
        },
        {
          "id": "AID-D-011",
          "name": "Agent Behavioral Attestation & Rogue Detection",
          "description": "Implement continuous behavioral monitoring and attestation mechanisms to identify rogue or compromised agents in multi-agent systems. This technique uses behavioral fingerprinting, anomaly detection, and peer verification to detect agents that deviate from expected behavioral patterns or exhibit malicious characteristics.",
          "pillar": "app",
          "phase": "operation",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0048 External Harms",
                "AML.T0073 Impersonation"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Compromised Agents (L7) (Detects/contains agents operating outside intended policy)",
                "Agent Identity Attack (L7)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM06:2025 Excessive Agency"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML06:2023 ML Supply Chain Attacks (Detects compromised or swapped models/agents introduced into the environment)"
              ]
            }
          ],
          "implementationStrategies": [],
          "toolsOpenSource": [],
          "toolsCommercial": [],
          "subTechniques": [
            {
              "id": "AID-D-011.001",
              "name": "Agent Behavioral Analytics & Anomaly Detection",
              "description": "This data science-driven technique focuses on detecting rogue or compromised agents by analyzing their behavior over time. It involves creating a quantitative 'fingerprint' of an agent's normal operational patterns from logs and telemetry. By continuously comparing an agent's live behavior against its established baseline, this technique can identify significant deviations, drifts, or anomalous patterns that indicate a compromise or hijacking.",
              "pillar": "app",
              "phase": "operation",
              "implementationStrategies": [
                "Create behavioral fingerprints for each agent based on normal operational patterns.",
                "Implement continuous behavioral scoring that tracks agent trustworthiness based on historical actions.",
                "Deploy behavioral drift detection to identify gradual changes in agent behavior."
              ],
              "toolsOpenSource": [
                "scikit-learn (for clustering and anomaly detection models like Isolation Forest, DBSCAN)",
                "Pandas, NumPy, SciPy (for data manipulation, feature engineering, and statistical analysis)",
                "Evidently AI, NannyML (for drift detection on behavioral features)",
                "MLflow, TensorBoard (for tracking behavioral model experiments)",
                "Jupyter Notebooks (for exploratory analysis and threat hunting)"
              ],
              "toolsCommercial": [
                "AI Observability Platforms (Arize AI, Fiddler, WhyLabs)",
                "User and Entity Behavior Analytics (UEBA) tools (Splunk UBA, Exabeam, Securonix)",
                "Datadog (Watchdog for anomaly detection)",
                "Splunk Machine Learning Toolkit (MLTK)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.TA0006 Persistence (if a rogue agent is the persistence mechanism)",
                    "AML.T0048 External Harms (by detecting the anomalous behavior that leads to harm)"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Compromised Agents (L7)",
                    "Agent Goal Manipulation (L7, by detecting the resulting behavioral changes)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM06:2025 Excessive Agency (detecting when an agent's behavior exceeds its normal operational envelope)"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML06:2023 ML Supply Chain Attacks (if a compromised dependency causes anomalous agent behavior)"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "anomalous behavior",
                  "rogue",
                  "compromised",
                  "anomalous",
                  "compromise",
                  "hijacking",
                  "drift",
                  "deviation",
                  "degradation",
                  "manipulation",
                  "evasion",
                  "bypass",
                  "stealth",
                  "obfuscation",
                  "hiding"
                ],
                "defense": [
                  "anomaly detection",
                  "drift detection",
                  "agent",
                  "behavioral",
                  "analytics",
                  "anomaly",
                  "detection",
                  "detecting",
                  "analyzing",
                  "baseline"
                ]
              }
            },
            {
              "id": "AID-D-011.002",
              "name": "Inter-Agent Security & Consensus Monitoring",
              "description": "This sub-technique covers the security of agent-to-agent interactions within a multi-agent system. It focuses on implementing mechanisms that allow agents to monitor and validate each other's behavior, report anomalies, and reach consensus before performing critical, system-wide actions. This creates a distributed, peer-to-peer defense layer within the agent ecosystem. Please note that, unlike AID-D-009 (which focuses on fact validation) and AID-D-010 (which focuses on goal integrity), AID-D-011.002 enforces peer governance over actions  especially high-impact or irreversible actions  via quorum and behavioral consistency checks.",
              "pillar": "app",
              "phase": "operation",
              "implementationStrategies": [
                "Deploy peer-based agent verification where agents cross-validate each other's behaviors and report anomalies",
                "Create behavioral consensus mechanisms where critical decisions require verification from multiple trusted agents"
              ],
              "toolsOpenSource": [
                "Agentic frameworks with inter-agent communication protocols (AutoGen, CrewAI)",
                "gRPC, ZeroMQ (for secure agent communication)",
                "Consensus libraries (RAFT, Paxos implementations if needed for custom logic)",
                "Python `multiprocessing` or `threading` for local peer monitoring"
              ],
              "toolsCommercial": [
                "Enterprise agentic platforms with built-in consensus and governance",
                "Secure messaging queues (e.g., TIBCO, RabbitMQ with security plugins)",
                "Distributed application platforms"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0048 External Harms (by preventing a single rogue agent from taking critical action alone)",
                    "AML.T0073 Impersonation"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Compromised Agents (L7)",
                    "Agent Identity Attack (L7, peer verification helps establish trust)",
                    "Agent Goal Manipulation (L7)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM06:2025 Excessive Agency (by requiring consensus for high-impact actions)"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML09:2023 Output Integrity Attack (if output affects other agents)"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "unauthorized modification",
                  "malicious input",
                  "invalid input",
                  "stealth attack",
                  "persistent threat",
                  "tampering",
                  "corruption",
                  "manipulation",
                  "forgery",
                  "injection",
                  "bypass",
                  "exploit",
                  "evasion",
                  "undetected"
                ],
                "defense": [
                  "inter-agent",
                  "security",
                  "consensus",
                  "monitoring",
                  "monitor",
                  "validate",
                  "defense",
                  "validation",
                  "integrity",
                  "governance",
                  "verification",
                  "trusted"
                ]
              }
            },
            {
              "id": "AID-D-011.003",
              "name": "Agent Infrastructure & Population Control",
              "description": "This sub-technique covers the infrastructure and orchestration-level controls for managing the agent population and responding to threats. It focuses on a top-down view of the agent ecosystem, ensuring that only authorized agents are running and providing mechanisms to rapidly isolate and contain agents that are confirmed to be rogue or malicious. These are typically automated responses triggered by other detection systems.",
              "pillar": "infra",
              "phase": "operation",
              "implementationStrategies": [
                "Create agent quarantine mechanisms that automatically isolate agents exhibiting rogue behavior pending investigation",
                "Implement agent population monitoring to detect unauthorized agent introduction or agent impersonation"
              ],
              "toolsOpenSource": [
                "Kubernetes (for pod management and network policies)",
                "Ansible, Terraform (for automating infrastructure response)",
                "Custom scripts using cloud provider SDKs/CLIs",
                "SOAR platforms (Shuffle, TheHive with Cortex)"
              ],
              "toolsCommercial": [
                "SOAR Platforms (Palo Alto XSOAR, Splunk SOAR)",
                "Cloud Security Posture Management (CSPM) tools (Wiz, Prisma Cloud)",
                "Endpoint Detection & Response (EDR) tools (CrowdStrike, SentinelOne)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0073 Impersonation",
                    "AML.T0074 Masquerading"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Compromised Agents (L7) (Actively isolates agents confirmed to be compromised)",
                    "Resource Hijacking (L4)",
                    "Compromised Container Images (L4)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM06:2025 Excessive Agency (containing the agent)",
                    "LLM03:2025 Supply Chain (preventing unauthorized agent code from running)"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML06:2023 ML Supply Chain Attacks"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "stealth attack",
                  "persistent threat",
                  "rogue",
                  "malicious",
                  "unauthorized",
                  "impersonation",
                  "evasion",
                  "undetected",
                  "bypass",
                  "stealth",
                  "obfuscation",
                  "hiding"
                ],
                "defense": [
                  "agent",
                  "infrastructure",
                  "population",
                  "control",
                  "authorized",
                  "isolate",
                  "contain",
                  "detection",
                  "quarantine",
                  "monitoring",
                  "detect"
                ]
              }
            }
          ],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "stealth attack",
              "persistent threat",
              "novel attack",
              "unknown threat",
              "model theft",
              "rogue",
              "compromised",
              "malicious",
              "evasion",
              "undetected",
              "bypass",
              "stealth",
              "obfuscation",
              "hiding",
              "zero-day"
            ],
            "defense": [
              "anomaly detection",
              "behavioral monitoring",
              "agent",
              "behavioral",
              "attestation",
              "rogue",
              "detection",
              "monitoring",
              "verification",
              "detect"
            ]
          }
        },
        {
          "id": "AID-D-012",
          "name": "Graph Anomaly & Backdoor Detection",
          "description": "Implements methods to identify malicious nodes, edges, or subgraphs within a graph dataset that are indicative of poisoning or backdoor attacks against Graph Neural Networks.",
          "pillar": "model",
          "phase": "validation",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0020 Poison Training Data (Detects and isolates poisoned training artifacts that insert hidden triggers into the model)",
                "AML.T0018 Manipulate AI Model (Surfaces persistent malicious model behavior caused by adversarial changes to weights or architecture)"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Data Poisoning (L2)",
                "Backdoor Attacks (L1)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM04:2025 Data and Model Poisoning (Detects and mitigates malicious data and hidden behaviors inserted into model training pipelines)"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML02:2023 Data Poisoning Attack",
                "ML10:2023 Model Poisoning"
              ]
            }
          ],
          "implementationStrategies": [],
          "toolsOpenSource": [],
          "toolsCommercial": [],
          "subTechniques": [
            {
              "id": "AID-D-012.001",
              "name": "GNN Backdoor Scanning Against Baselined Profiles",
              "description": "Consumes baseline artifacts generated by AID-M-003.007 (clean embedding distributions, drift profiles, discrepancy statistics) to detect backdoored nodes in a Graph Neural Network (GNN). Compares current model states against the persisted baselines to identify semantic drift and attribute over-emphasis indicative of backdoor attacks. Uses clustering algorithms to isolate anomalous node groups and triggers alerts when suspicious patterns are detected. Inputs: Baseline artifacts from AID-M-003.007 at baselines/ directory (clean_node_embeddings.npy, node_semantic_drift.npy, primary_embeddings.npy).",
              "pillar": "model",
              "phase": "validation",
              "implementationStrategies": [
                "Load baseline artifacts from AID-M-003.007 and compute semantic drift scores for anomaly detection.",
                "Compare feature importance vectors against baselines to detect attribute over-emphasis.",
                "Use clustering on the combined discrepancy scores to isolate the group of poisoned nodes.",
                "Set an automated detection threshold based on the size and separation of the anomalous cluster."
              ],
              "toolsOpenSource": [
                "PyTorch Geometric, Deep Graph Library (DGL)",
                "scikit-learn (for clustering algorithms like DBSCAN)",
                "NumPy, SciPy (for distance and vector calculations)",
                "GNNExplainer, Captum (for attribute importance analysis)"
              ],
              "toolsCommercial": [
                "Graph Database & Analytics Platforms (Neo4j, TigerGraph)",
                "AI Observability Platforms (Arize AI, Fiddler, WhyLabs)",
                "AI Security Platforms (Protect AI, HiddenLayer)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0018 Manipulate AI Model",
                    "AML.T0020 Poison Training Data (Detects malicious training data that implants targeted backdoors into graph models)"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Backdoor Attacks (L1)",
                    "Data Poisoning (L2)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "Not directly applicable"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML02:2023 Data Poisoning Attack",
                    "ML10:2023 Model Poisoning"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "backdoor attack",
                  "backdoor",
                  "artifacts",
                  "embedding",
                  "backdoored",
                  "anomalous",
                  "suspicious",
                  "embeddings",
                  "vectors",
                  "poisoned"
                ],
                "defense": [
                  "anomaly detection",
                  "gnn",
                  "backdoor",
                  "scanning",
                  "baselined",
                  "profiles",
                  "baseline",
                  "detect",
                  "isolate",
                  "detected",
                  "detection",
                  "threshold"
                ]
              }
            },
            {
              "id": "AID-D-012.002",
              "name": "Structure-Feature Relationship Analysis for GNN Defense",
              "description": "Detects and mitigates training-time adversarial attacks on Graph Neural Networks (GNNs) that perturb the graph structure. The core principle is to analyze the relationship between the graph's connectivity (structure) and the attributes of its nodes (features). By identifying and then pruning or down-weighting anomalous edges that violate expected structure-feature properties (e.g., connecting highly dissimilar nodes), this technique creates a revised, more robust graph for the GNN's message passing, hardening it against structural poisoning.",
              "pillar": "data",
              "phase": "operation",
              "implementationStrategies": [
                "Compute feature similarity scores for all connected nodes in the graph.",
                "Identify and flag anomalous edges where connected nodes are highly dissimilar.",
                "Prune or down-weight the influence of anomalous edges during GNN message passing.",
                "Implement a learnable attention mechanism (e.g., GAT) to allow the model to learn neighbor importance."
              ],
              "toolsOpenSource": [
                "PyTorch Geometric, Deep Graph Library (DGL) (for GNN models, including GAT)",
                "scikit-learn (for similarity metrics)",
                "NetworkX (for graph analysis)",
                "NumPy, SciPy"
              ],
              "toolsCommercial": [
                "Graph Database & Analytics Platforms (Neo4j, TigerGraph)",
                "AI Security Platforms (Protect AI, HiddenLayer)",
                "AI Observability Platforms (Arize AI, Fiddler)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0020 Poison Training Data",
                    "AML.T0043 Craft Adversarial Data"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Data Poisoning (L2)",
                    "Data Tampering (L2)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "Not directly applicable"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML01:2023 Input Manipulation Attack",
                    "ML02:2023 Data Poisoning Attack"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "adversarial attack",
                  "attack surface",
                  "adversarial",
                  "anomalous",
                  "poisoning",
                  "training",
                  "exploitation",
                  "vulnerability",
                  "misconfiguration"
                ],
                "defense": [
                  "structure-feature",
                  "relationship",
                  "analysis",
                  "gnn",
                  "defense",
                  "analyze",
                  "robust",
                  "hardening",
                  "detection",
                  "monitoring",
                  "alerting",
                  "anomaly",
                  "inspection",
                  "observability",
                  "logging"
                ]
              }
            },
            {
              "id": "AID-D-012.003",
              "name": "Structural & Topological Anomaly Detection",
              "description": "Detects potential poisoning or backdoor attacks in graphs by analyzing their topological structure, independent of node features. This technique identifies suspicious patterns such as unusually dense subgraphs (cliques), nodes with anomalously high centrality or degree, or other structural irregularities that deviate from the expected properties of the graph and are often characteristic of coordinated attacks.",
              "pillar": "data",
              "phase": "operation",
              "implementationStrategies": [
                "Analyze node centrality and degree distributions to find structural outliers.",
                "Implement subgraph anomaly detection to find suspicious dense clusters or cliques.",
                "Compare global graph properties against a baseline of known-good graphs."
              ],
              "toolsOpenSource": [
                "NetworkX (for graph algorithms like centrality and clique finding)",
                "PyTorch Geometric, Deep Graph Library (DGL) (for graph data structures)",
                "scikit-learn, NumPy, SciPy (for statistical analysis of graph properties)"
              ],
              "toolsCommercial": [
                "Graph Database & Analytics Platforms (Neo4j, TigerGraph, Memgraph)",
                "AI Security Platforms (Protect AI, HiddenLayer)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0020 Poison Training Data"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Data Poisoning (L2)",
                    "Data Tampering (L2)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "Not directly applicable"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML02:2023 Data Poisoning Attack"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "backdoor attack",
                  "coordinated attack",
                  "anomalous behavior",
                  "poisoning",
                  "backdoor",
                  "suspicious",
                  "drift",
                  "deviation",
                  "degradation",
                  "manipulation",
                  "evasion",
                  "bypass",
                  "stealth",
                  "obfuscation",
                  "hiding"
                ],
                "defense": [
                  "anomaly detection",
                  "structural",
                  "topological",
                  "anomaly",
                  "detection",
                  "analyzing",
                  "analyze",
                  "baseline",
                  "monitoring",
                  "alerting",
                  "analysis",
                  "inspection",
                  "observability",
                  "logging",
                  "audit"
                ]
              }
            }
          ],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "backdoor attack",
              "novel attack",
              "unknown threat",
              "backdoor",
              "malicious",
              "poisoning",
              "evasion",
              "bypass",
              "stealth",
              "obfuscation",
              "hiding",
              "zero-day"
            ],
            "defense": [
              "graph",
              "anomaly",
              "backdoor",
              "detection",
              "monitoring",
              "alerting",
              "analysis",
              "inspection",
              "observability",
              "logging",
              "audit"
            ]
          }
        },
        {
          "id": "AID-D-013",
          "name": "RL Reward & Policy Manipulation Detection",
          "description": "This technique focuses on monitoring and analyzing Reinforcement Learning (RL) systems to detect two primary threats: reward hacking and reward tampering. Reward hacking occurs when an agent discovers an exploit in the environment's reward function to achieve a high score for unintended or harmful behavior. Reward tampering involves an external actor manipulating the reward signal being sent to the agent. This technique uses statistical analysis of the reward stream and behavioral analysis of the agent's learned policy to detect these manipulations.",
          "pillar": "model",
          "phase": "operation",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0048 External Harms (by detecting the unintended behaviors that cause harm)",
                "AML.T0031 Erode AI Model Integrity (if the exploited policy is considered part of the model)"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Agent Goal Manipulation (L7)",
                "Manipulation of Evaluation Metrics (L5) (Detects agents that learn to game reward rather than actually achieve intended task success)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM06:2025 Excessive Agency (as reward hacking is a primary cause)"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML08:2023 Model Skewing (where agent behavior is skewed by an exploitable reward)"
              ]
            }
          ],
          "implementationStrategies": [
            "Monitor the reward stream for statistical anomalies.",
            "Analyze agent trajectories to detect pathological behaviors.",
            "Periodically test the agent's policy in a sandboxed 'honey-state'.",
            "Implement out-of-band reward signal verification."
          ],
          "toolsOpenSource": [
            "RL libraries with logging callbacks (Stable-Baselines3, RLlib)",
            "Monitoring and alerting tools (Prometheus, Grafana)",
            "Data analysis libraries (Pandas, NumPy, SciPy)",
            "Simulation environments (Gymnasium, MuJoCo)",
            "XAI libraries adaptable for policy analysis (SHAP, Captum)"
          ],
          "toolsCommercial": [
            "Enterprise RL platforms (Microsoft Bonsai, AnyLogic)",
            "AI Observability Platforms (Datadog, Arize AI, Fiddler)",
            "Simulation platforms (NVIDIA Isaac Sim)"
          ],
          "subTechniques": [],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "stealth attack",
              "persistent threat",
              "manipulation",
              "tampering",
              "exploit",
              "harmful",
              "manipulating",
              "evasion",
              "undetected",
              "bypass",
              "stealth",
              "obfuscation",
              "hiding"
            ],
            "defense": [
              "reward",
              "policy",
              "manipulation",
              "detection",
              "monitoring",
              "analyzing",
              "detect",
              "analysis",
              "monitor",
              "analyze",
              "verification"
            ]
          }
        },
        {
          "id": "AID-D-014",
          "name": "RAG Content & Relevance Monitoring",
          "description": "This technique involves the real-time monitoring of a Retrieval-Augmented Generation (RAG) system's behavior at inference time. It focuses on two key checks: 1) Content Analysis, where retrieved document chunks are scanned for harmful content or malicious payloads before being passed to the LLM, and 2) Relevance Analysis, which verifies that the retrieved documents are semantically relevant to the user's original query. A significant mismatch in relevance can indicate a vector manipulation or poisoning attack designed to force the model to use unintended context.",
          "pillar": "data",
          "phase": "operation",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0070 RAG Poisoning",
                "AML.T0066 Retrieval Content Crafting",
                "AML.T0071 False RAG Entry Injection",
                "AML.T0051 LLM Prompt Injection (if payload is in RAG source)"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Compromised RAG Pipelines (L2)",
                "Data Poisoning (L2)",
                "Misinformation Generation (Cross-Layer)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM08:2025 Vector and Embedding Weaknesses",
                "LLM04:2025 Data and Model Poisoning"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML02:2023 Data Poisoning Attack"
              ]
            }
          ],
          "implementationStrategies": [],
          "toolsOpenSource": [],
          "toolsCommercial": [],
          "subTechniques": [
            {
              "id": "AID-D-014.001",
              "name": "Post-Retrieval Malicious Content Scanning",
              "description": "Treat retrieved RAG chunks as untrusted input; scan for prompt-injection patterns or malicious payloads before inclusion in LLM context.",
              "pillar": "data",
              "phase": "operation",
              "implementationStrategies": [
                "Re-use inbound prompt safety filters on retrieved chunks."
              ],
              "toolsOpenSource": [
                "Guardrails.ai",
                "Llama Guard",
                "NVIDIA NeMo Guardrails"
              ],
              "toolsCommercial": [
                "Lakera Guard",
                "Protect AI Guardian"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0070 RAG Poisoning",
                    "AML.T0051 LLM Prompt Injection"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Compromised RAG Pipelines (L2)",
                    "Data Poisoning (L2)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM08:2025 Vector and Embedding Weaknesses",
                    "LLM01:2025 Prompt Injection (indirect)"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML02:2023 Data Poisoning Attack"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "stealth attack",
                  "anomalous behavior",
                  "malicious",
                  "untrusted",
                  "context",
                  "prompt",
                  "evasion",
                  "obfuscation",
                  "intrusion"
                ],
                "defense": [
                  "safety filter",
                  "post-retrieval",
                  "malicious",
                  "content",
                  "scanning",
                  "scan",
                  "safety",
                  "detection",
                  "monitoring",
                  "alerting",
                  "analysis",
                  "anomaly",
                  "inspection",
                  "observability",
                  "logging"
                ]
              }
            },
            {
              "id": "AID-D-014.002",
              "name": "Query-Document Semantic Relevance Verification",
              "description": "Verify cosine similarity between the user query and each candidate chunk using the same embedding model; drop low-similarity items to resist poisoning.",
              "pillar": "data",
              "phase": "operation",
              "implementationStrategies": [
                "Thresholded cosine similarity with calibrated cutoffs."
              ],
              "toolsOpenSource": [
                "sentence-transformers",
                "FAISS"
              ],
              "toolsCommercial": [
                "Pinecone",
                "Weaviate"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0070 RAG Poisoning",
                    "AML.T0071 False RAG Entry Injection"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Compromised RAG Pipelines (L2)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM08:2025 Vector and Embedding Weaknesses"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML02:2023 Data Poisoning Attack"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "stealth attack",
                  "anomalous behavior",
                  "embedding",
                  "poisoning",
                  "evasion",
                  "obfuscation",
                  "intrusion"
                ],
                "defense": [
                  "query-document",
                  "semantic",
                  "relevance",
                  "verification",
                  "verify",
                  "detection",
                  "monitoring",
                  "alerting",
                  "analysis",
                  "anomaly",
                  "inspection",
                  "observability",
                  "logging",
                  "audit"
                ]
              }
            },
            {
              "id": "AID-D-014.003",
              "name": "Source Concentration Monitoring",
              "description": "Alert when top-k retrievals are dominated by a single uncommon source, indicating possible answer drift or targeted source poisoning.",
              "pillar": "data",
              "phase": "operation",
              "implementationStrategies": [
                "Calculate source distribution per query window and alert on concentration thresholds (e.g., >80%)."
              ],
              "toolsOpenSource": [
                "pandas",
                "collections"
              ],
              "toolsCommercial": [
                "Datadog",
                "New Relic",
                "Splunk Observability"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0070 RAG Poisoning",
                    "AML.T0071 False RAG Entry Injection"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Compromised RAG Pipelines (L2)",
                    "Misinformation Generation (Cross-Layer)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM08:2025 Vector and Embedding Weaknesses",
                    "LLM09:2025 Misinformation"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML02:2023 Data Poisoning Attack"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "stealth attack",
                  "persistent threat",
                  "anomalous behavior",
                  "poisoning",
                  "evasion",
                  "undetected",
                  "obfuscation",
                  "intrusion"
                ],
                "defense": [
                  "source",
                  "concentration",
                  "monitoring",
                  "alert",
                  "thresholds",
                  "detection",
                  "alerting",
                  "analysis",
                  "anomaly",
                  "inspection",
                  "observability",
                  "logging",
                  "audit"
                ]
              }
            }
          ],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "stealth attack",
              "persistent threat",
              "key",
              "harmful",
              "malicious",
              "vector",
              "manipulation",
              "poisoning",
              "attack",
              "context",
              "evasion",
              "undetected"
            ],
            "defense": [
              "real-time monitoring",
              "rag",
              "content",
              "relevance",
              "monitoring",
              "key",
              "analysis",
              "document",
              "scanned",
              "detection",
              "alerting",
              "anomaly",
              "inspection",
              "observability",
              "logging"
            ]
          }
        },
        {
          "id": "AID-D-015",
          "name": "User Trust Calibration & High-Risk Action Confirmation",
          "description": "Close the last-mile gap for human-agent trust by surfacing backend trust/verification signals to the user experience and by enforcing explicit confirmation flows for high-risk actions. Even with strong backend filtering, users can still be socially engineered by plausible outputs or be surprised by autonomous actions. This technique standardizes trust metadata, UI warnings, and step-up confirmations for actions with real-world impact.",
          "pillar": "app",
          "phase": "operation",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0052: Phishing",
                "AML.T0048.002: External Harms: Societal Harm",
                "AML.T0048.000: External Harms: Financial Harm",
                "AML.T0067: LLM Trusted Output Components Manipulation"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Inaccurate Agent Capability Description (L7)",
                "Agent Tool Misuse (L7)",
                "Data Exfiltration (L2, via coerced user approvals)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM09:2025 Misinformation",
                "LLM06:2025 Excessive Agency",
                "LLM05:2025 Improper Output Handling",
                "LLM02:2025 Sensitive Information Disclosure"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML09:2023 Output Integrity Attack"
              ]
            }
          ],
          "implementationStrategies": [],
          "toolsOpenSource": [],
          "toolsCommercial": [],
          "subTechniques": [
            {
              "id": "AID-D-015.001",
              "name": "Trust Metadata Exposure (Verification/Provenance Signals)",
              "description": "Expose standardized trust metadata in API responses so front-ends can consistently display warnings, provenance, and verification status. Signals may include source diversity, verification state, signed memory validity, and tool attestation status. The goal is consistent trust calibration and reduced susceptibility to targeted misinformation.",
              "pillar": "app",
              "phase": "operation",
              "implementationStrategies": [
                "Return a schema-versioned trust metadata object (trust_score, verification_state, source_diversity, signed_memory_valid, tool_attestation) for every response and tool plan; log it for audit after redaction."
              ],
              "toolsOpenSource": [
                "OpenTelemetry (trace attributes for trust signals)",
                "JSON Schema (contract for trust metadata)",
                "FastAPI (API middleware patterns)"
              ],
              "toolsCommercial": [
                "Datadog (dashboards/alerts for trust signal anomalies)",
                "Splunk (analysis of trust signal distributions)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0052: Phishing",
                    "AML.T0067: LLM Trusted Output Components Manipulation"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM09:2025 Misinformation",
                    "LLM05:2025 Improper Output Handling"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "data poisoning",
                  "supply chain attack",
                  "untrusted data",
                  "forensic evasion",
                  "misinformation",
                  "exposure",
                  "expose",
                  "state",
                  "memory",
                  "tampering",
                  "repudiation",
                  "unauthorized"
                ],
                "defense": [
                  "trust",
                  "metadata",
                  "exposure",
                  "verification",
                  "provenance",
                  "signals",
                  "signed",
                  "attestation",
                  "log",
                  "audit"
                ]
              }
            },
            {
              "id": "AID-D-015.002",
              "name": "High-Risk Action Step-Up & Out-of-Band Confirmation",
              "description": "Require explicit user confirmation (and optionally out-of-band verification) before executing high-risk actions such as transferring funds, changing IAM permissions, deleting resources, or exporting sensitive data. This reduces social engineering and surprise autonomy even if the model is manipulated.",
              "pillar": "app",
              "phase": "operation",
              "implementationStrategies": [
                "Enforce policy-driven confirmation gates for high-risk actions (step-up MFA, two-person approval, or out-of-band confirmation), bound to an immutable plan hash to prevent content swapping."
              ],
              "toolsOpenSource": [
                "Keycloak (step-up authentication patterns)",
                "OPA (policy decisions for when to require step-up)",
                "WebAuthn (strong user confirmation)"
              ],
              "toolsCommercial": [
                "Okta (step-up auth)",
                "Duo Security (MFA/out-of-band confirmation)",
                "Microsoft Entra ID (Conditional Access)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0048.000: Financial Harm",
                    "AML.T0052: Phishing"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM06:2025 Excessive Agency",
                    "LLM02:2025 Sensitive Information Disclosure"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "social engineering",
                  "stealth attack",
                  "anomalous behavior",
                  "manipulated",
                  "evasion",
                  "obfuscation",
                  "intrusion"
                ],
                "defense": [
                  "high-risk",
                  "action",
                  "step-up",
                  "out-of-band",
                  "confirmation",
                  "verification",
                  "iam",
                  "permissions",
                  "enforce",
                  "mfa",
                  "hash",
                  "prevent",
                  "policy"
                ]
              }
            }
          ],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "malicious content",
              "harmful output",
              "data exfiltration",
              "stealth attack",
              "anomalous behavior",
              "gap",
              "injection",
              "evasion",
              "obfuscation",
              "intrusion"
            ],
            "defense": [
              "user",
              "trust",
              "calibration",
              "high-risk",
              "action",
              "confirmation",
              "verification",
              "enforcing",
              "filtering",
              "detection",
              "monitoring",
              "alerting",
              "analysis",
              "anomaly",
              "inspection"
            ]
          }
        },
        {
          "id": "AID-D-016",
          "name": "Rogue Agent Discovery, Reputation & Quarantine Pipeline",
          "description": "Establish continuous governance for agent identity, emergence, and behavior by building a discovery and reputation pipeline that detects unknown or compromised agents, scores risk, and automatically quarantines or evicts them. This creates a closed-loop: discover  score  restrict/quarantine  investigate  restore/evict, with full auditability.",
          "pillar": "infra",
          "phase": "operation",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0053: AI Agent Tool Invocation",
                "AML.T0061: LLM Prompt Self-Replication",
                "AML.T0072: Reverse Shell",
                "AML.T0050: Command and Scripting Interpreter"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Compromised Agent Registry (L7)",
                "Lateral Movement (Cross-Layer)",
                "Agent Tool Misuse (L7)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM06:2025 Excessive Agency",
                "LLM10:2025 Unbounded Consumption",
                "LLM05:2025 Improper Output Handling"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML06:2023 AI Supply Chain Attacks"
              ]
            }
          ],
          "implementationStrategies": [],
          "toolsOpenSource": [],
          "toolsCommercial": [],
          "subTechniques": [
            {
              "id": "AID-D-016.001",
              "name": "Agent Graph Baseline & New-Agent Discovery",
              "description": "Build a baseline of expected agent identities and communication edges (agent graph). Detect new/unknown agents, unusual fan-out patterns, and anomalous call paths using service mesh and registry telemetry. This provides early warning for rogue agents and self-replication patterns.",
              "pillar": "infra",
              "phase": "operation",
              "implementationStrategies": [
                "Maintain a versioned agent registry baseline and alert on unknown agent identities or new communication edges, using stable identity keys (SPIFFE IDs/service accounts) and time-windowed anomaly detection."
              ],
              "toolsOpenSource": [
                "Istio (service mesh telemetry)",
                "Envoy (L7 telemetry primitives)",
                "SPIFFE/SPIRE (workload identity)",
                "OpenTelemetry (traces/metrics/logs)",
                "Prometheus (metrics + alerting)"
              ],
              "toolsCommercial": [
                "Datadog",
                "Splunk",
                "Microsoft Sentinel (SIEM)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0061: LLM Prompt Self-Replication",
                    "AML.T0053: AI Agent Tool Invocation"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Compromised Agent Registry (L7)",
                    "Lateral Movement (Cross-Layer)"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "anomalous behavior",
                  "novel attack",
                  "anomalous",
                  "rogue",
                  "keys",
                  "drift",
                  "deviation",
                  "degradation",
                  "manipulation",
                  "evasion",
                  "bypass",
                  "stealth",
                  "obfuscation",
                  "hiding",
                  "zero-day"
                ],
                "defense": [
                  "anomaly detection",
                  "agent",
                  "graph",
                  "baseline",
                  "new-agent",
                  "discovery",
                  "detect",
                  "registry",
                  "versioned",
                  "alert",
                  "keys",
                  "ids",
                  "detection"
                ]
              }
            },
            {
              "id": "AID-D-016.002",
              "name": "Reputation Scoring  Quarantine  Evict/Restore Closed Loop",
              "description": "Score agent reputation continuously using signals (unknown identity, signature failures, abnormal tool usage, egress anomalies). Automatically quarantine by reducing privileges, isolating network, and limiting tool access. Provide clear paths to evict (kill/disable) or restore (post-incident) with auditable approvals.",
              "pillar": "infra",
              "phase": "operation",
              "implementationStrategies": [
                "Define a policy-auditable reputation score using structured events, time-window aggregation, de-duplication, and time decay; auto-quarantine when thresholds are crossed.",
                "Provide a staged restore/evict procedure with explicit approvals, immutable audit logs, and post-incident safe-mode monitoring."
              ],
              "toolsOpenSource": [
                "OPA (policy-based quarantine decisions)",
                "Kubernetes NetworkPolicy (isolation)",
                "Istio AuthorizationPolicy (L7 enforcement)",
                "OpenTelemetry (security events)",
                "Falco (runtime alerts)"
              ],
              "toolsCommercial": [
                "Palo Alto Prisma Cloud",
                "CrowdStrike (host isolation/containment in some environments)",
                "SentinelOne (process kill/isolation in some environments)",
                "Splunk SOAR (automation playbooks)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0072: Reverse Shell",
                    "AML.T0050: Command and Scripting Interpreter",
                    "AML.T0053: AI Agent Tool Invocation"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Data Exfiltration (L2)",
                    "Agent Tool Misuse (L7)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM06:2025 Excessive Agency",
                    "LLM02:2025 Sensitive Information Disclosure"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "stealth attack",
                  "persistent threat",
                  "forensic evasion",
                  "abnormal",
                  "evasion",
                  "undetected",
                  "repudiation",
                  "tampering",
                  "unauthorized"
                ],
                "defense": [
                  "reputation",
                  "scoring",
                  "quarantine",
                  "evict",
                  "restore",
                  "closed",
                  "loop",
                  "signature",
                  "privileges",
                  "isolating",
                  "thresholds",
                  "audit",
                  "monitoring"
                ]
              }
            }
          ],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "forensic evasion",
              "stealth attack",
              "anomalous behavior",
              "rogue",
              "compromised",
              "risk",
              "repudiation",
              "tampering",
              "unauthorized",
              "evasion",
              "obfuscation",
              "intrusion"
            ],
            "defense": [
              "rogue",
              "agent",
              "discovery",
              "reputation",
              "quarantine",
              "pipeline",
              "governance",
              "restore",
              "detection",
              "monitoring",
              "alerting",
              "analysis",
              "anomaly",
              "inspection",
              "observability"
            ]
          }
        }
      ]
    },
    {
      "id": "isolate",
      "name": "Isolate",
      "description": "The \"Isolate\" tactic involves implementing measures to contain malicious activity and limit its potential spread or impact should an AI system or one of its components become compromised. This includes sandboxing AI processes, segmenting networks to restrict communication, and establishing mechanisms to quickly quarantine or throttle suspicious interactions or misbehaving AI entities.",
      "techniques": [
        {
          "id": "AID-I-001",
          "name": "AI Execution Sandboxing & Runtime Isolation",
          "description": "Execute AI models, autonomous agents, or individual AI tools and plugins within isolated environments such as sandboxes, containers, or microVMs. These environments must be configured with strict limits on resources, permissions, and network connectivity. The primary goal is that if an AI component is compromised or behaves maliciously, the impact is confined to the isolated sandbox, preventing harm to the host system or lateral movement.",
          "pillar": "infra",
          "phase": "operation",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0053: AI Agent Tool Invocation",
                "AML.T0020: Poison Training Data",
                "AML.T0072: Reverse Shell",
                "AML.T0050 Command and Scripting Interpreter",
                "AML.T0029 Denial of AI Service",
                "AML.T0034 Cost Harvesting (limiting rates)"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Compromised Container Images (L4)",
                "Lateral Movement (Cross-Layer)",
                "Agent Tool Misuse (L7)",
                "Resource Hijacking (L4)",
                "Framework Evasion (L3)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM05:2025 Improper Output Handling",
                "LLM06:2025 Excessive Agency",
                "LLM10:2025 Unbounded Consumption"
              ]
            }
          ],
          "implementationStrategies": [],
          "toolsOpenSource": [],
          "toolsCommercial": [],
          "subTechniques": [
            {
              "id": "AID-I-001.001",
              "name": "Container-Based Isolation",
              "description": "Utilizes container technologies like Docker or Kubernetes to package and run AI workloads in isolated user-space environments. This approach provides process and filesystem isolation and allows for resource management and network segmentation.",
              "pillar": "infra",
              "phase": "operation",
              "implementationStrategies": [
                "Deploy AI models and services in hardened, minimal-footprint container images.",
                "Apply Kubernetes security contexts to restrict container privileges (e.g., runAsNonRoot).",
                "Use network policies to enforce least-privilege communication between AI pods.",
                "Set strict resource quotas (CPU, memory, GPU) to prevent resource exhaustion attacks.",
                "Mount filesystems as read-only wherever possible."
              ],
              "toolsOpenSource": [
                "Docker",
                "Podman",
                "Kubernetes",
                "OpenShift (container platform)",
                "Falco (container runtime security)",
                "Trivy (container vulnerability scanner)",
                "Sysdig (container monitoring & security)",
                "Calico (for Network Policies)",
                "Cilium (for Network Policies and eBPF)"
              ],
              "toolsCommercial": [
                "Docker Enterprise",
                "Red Hat OpenShift Container Platform",
                "Aqua Security",
                "Twistlock (Palo Alto Networks)",
                "Prisma Cloud (Palo Alto Networks)",
                "Microsoft Azure Kubernetes Service (AKS)",
                "Google Kubernetes Engine (GKE)",
                "Amazon Elastic Kubernetes Service (EKS)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0053 AI Agent Tool Invocation",
                    "AML.T0072 Reverse Shell",
                    "AML.T0029 Denial of AI Service",
                    "AML.T0034 Cost Harvesting"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Compromised Container Images (L4)",
                    "Lateral Movement (Cross-Layer)",
                    "Agent Tool Misuse (L7)",
                    "Resource Hijacking (L4)",
                    "Runtime Code Injection (L4)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM06:2025 Excessive Agency",
                    "LLM10:2025 Unbounded Consumption"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML06:2023 AI Supply Chain Attacks",
                    "ML05:2023 Model Theft",
                    "ML09:2023 Output Integrity Attack"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "resource exhaustion",
                  "lateral movement",
                  "privilege escalation",
                  "containment escape",
                  "containment bypass",
                  "memory",
                  "breakout",
                  "escape"
                ],
                "defense": [
                  "network segmentation",
                  "vulnerability scanning",
                  "container-based",
                  "isolation",
                  "container",
                  "isolated",
                  "hardened",
                  "privileges",
                  "policies",
                  "enforce",
                  "quotas",
                  "prevent"
                ]
              }
            },
            {
              "id": "AID-I-001.002",
              "name": "MicroVM & Low-Level Sandboxing",
              "description": "Employs lightweight Virtual Machines (MicroVMs) or kernel-level sandboxing technologies to provide a stronger isolation boundary than traditional containers. This is critical for running untrusted code or highly sensitive AI workloads.",
              "pillar": "infra",
              "phase": "operation",
              "implementationStrategies": [
                "Use lightweight VMs like Firecracker or Kata Containers for strong hardware-virtualized isolation.",
                "Apply OS-level sandboxing with tools like gVisor to intercept and filter system calls.",
                "Define strict seccomp-bpf profiles to whitelist only necessary system calls for model inference.",
                "Utilize WebAssembly (WASM) runtimes to run AI models in a high-performance, secure sandbox."
              ],
              "toolsOpenSource": [
                "Kata Containers (using QEMU or Firecracker)",
                "Firecracker (AWS open-source microVM monitor)",
                "gVisor (Google open-source user-space kernel)",
                "seccomp-bpf (Linux kernel feature)",
                "Wasmtime (WebAssembly runtime)",
                "Wasmer (WebAssembly runtime)",
                "eBPF (Extended Berkeley Packet Filter)",
                "Cloud Hypervisor"
              ],
              "toolsCommercial": [
                "AWS Lambda (built on Firecracker)",
                "Google Cloud Run (uses gVisor)",
                "Azure Container Instances (ACI) with confidential computing options",
                "Red Hat OpenShift Virtualization (for Kata Containers management)",
                "WebAssembly-as-a-Service platforms"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0053 AI Agent Tool Invocation",
                    "AML.T0072 Reverse Shell",
                    "AML.T0017 Persistence",
                    "AML.T0029 Denial of AI Service",
                    "AML.T0034 Cost Harvesting",
                    "AML.T0018.002 Manipulate AI Model: Embed Malware"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Compromised Container Images (L4)",
                    "Lateral Movement (Cross-Layer)",
                    "Agent Tool Misuse (L7)",
                    "Resource Hijacking (L4)",
                    "Runtime Code Injection (L4)",
                    "Memory Corruption (L4)",
                    "Privilege Escalation (L6)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM06:2025 Excessive Agency",
                    "LLM10:2025 Unbounded Consumption",
                    "LLM03:2025 Supply Chain"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML06:2023 AI Supply Chain Attacks",
                    "ML09:2023 Output Integrity Attack",
                    "ML05:2023 Model Theft"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "lateral movement",
                  "privilege escalation",
                  "containment escape",
                  "code execution",
                  "containment bypass",
                  "untrusted",
                  "breakout",
                  "escape"
                ],
                "defense": [
                  "microvm",
                  "low-level",
                  "sandboxing",
                  "isolation",
                  "containers",
                  "filter",
                  "whitelist",
                  "secure",
                  "sandbox",
                  "containment",
                  "segmentation",
                  "separation",
                  "boundary",
                  "quarantine",
                  "enclave"
                ]
              }
            },
            {
              "id": "AID-I-001.003",
              "name": "Ephemeral Single-Use Sandboxes for Tools",
              "description": "Run tool executions inside strongly isolated, single-use sandboxes (e.g., microVMs). Destroy the environment immediately after one invocation to prevent persistence and cross-session contamination.",
              "pillar": "infra",
              "phase": "operation",
              "implementationStrategies": [
                "Provision a fresh, single-use sandbox (microVM / gVisor / Kata) for every tool invocation, execute once, then destroy it."
              ],
              "toolsOpenSource": [
                "gVisor",
                "Kata Containers",
                "Firecracker"
              ],
              "toolsCommercial": [
                "AWS Firecracker-backed services (Lambda, Fargate)",
                "Google GKE Sandbox"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0050 Command and Scripting Interpreter"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Runtime Code Injection (L4)",
                    "Lateral Movement (Cross-Layer)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM06:2025 Excessive Agency"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML06:2023 AI Supply Chain Attacks"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "tool execution",
                  "lateral movement",
                  "privilege escalation",
                  "containment bypass",
                  "persistence",
                  "breakout",
                  "escape"
                ],
                "defense": [
                  "ephemeral",
                  "single-use",
                  "sandboxes",
                  "isolated",
                  "prevent",
                  "sandbox",
                  "isolation",
                  "containment",
                  "segmentation",
                  "separation",
                  "boundary",
                  "quarantine",
                  "enclave"
                ]
              }
            },
            {
              "id": "AID-I-001.004",
              "name": "Seccomp-bpf & Network Egress Restrictions",
              "description": "Minimize kernel/system call surface and restrict outbound network destinations for sandboxed executions to reduce post-exploitation blast radius.",
              "pillar": "infra",
              "phase": "operation",
              "implementationStrategies": [
                "Apply minimal seccomp profiles and enforce egress-allowlist NetworkPolicies."
              ],
              "toolsOpenSource": [
                "seccomp-bpf",
                "Kubernetes NetworkPolicy"
              ],
              "toolsCommercial": [
                "Calico Enterprise",
                "Cilium Enterprise"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0072 Reverse Shell"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Runtime Code Injection (L4)",
                    "Lateral Movement (Cross-Layer)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM06:2025 Excessive Agency"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML06:2023 AI Supply Chain Attacks"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "lateral movement",
                  "privilege escalation",
                  "containment bypass",
                  "breakout",
                  "escape"
                ],
                "defense": [
                  "blast radius",
                  "seccomp-bpf",
                  "network",
                  "egress",
                  "restrictions",
                  "enforce",
                  "isolation",
                  "containment",
                  "segmentation",
                  "sandbox",
                  "separation",
                  "boundary",
                  "quarantine",
                  "enclave"
                ]
              }
            },
            {
              "id": "AID-I-001.005",
              "name": "Pre-Execution Behavioral Analysis in Ephemeral Sandboxes",
              "description": "This proactive defense technique subjects any AI-generated executable artifact (e.g., scripts, binaries, container images created by an agent) to mandatory behavioral analysis within a short-lived, strongly isolated sandbox (such as a microVM) *before* it is deployed or executed in a production context. This pre-execution security gate applies to artifacts originating from both automated CI/CD pipelines and interactive developer IDEs, serving as a final vetting step to contain threats from malicious AI-generated code before they can have any impact.",
              "pillar": "infra",
              "phase": "operation",
              "implementationStrategies": [
                "Orchestrate an automated analysis workflow using microVMs for strong isolation.",
                "Define and enforce a strict behavioral security policy within the sandbox.",
                "Generate a signed, verifiable analysis report for CI/CD admission control.",
                "Monitor for anti-analysis and sandbox evasion techniques."
              ],
              "toolsOpenSource": [
                "Firecracker",
                "Kata Containers",
                "gVisor",
                "QEMU/KVM",
                "Falco",
                "Cilium Tetragon",
                "strace",
                "Sysdig",
                "Wazuh (in-guest EDR)"
              ],
              "toolsCommercial": [
                "Joe Sandbox",
                "ANY.RUN",
                "EDR/XDR platforms with sandboxing features",
                "Execution Platforms (Note: AWS Lambda/Fargate are execution platforms that use microVMs; they can host a sandboxing service but do not provide behavioral analysis out-of-the-box.)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0050 Command and Scripting Interpreter",
                    "AML.T0072 Reverse Shell",
                    "AML.T0018.002 Manipulate AI Model: Embed Malware",
                    "AML.T0025 Exfiltration via Cyber Means"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Runtime Code Injection (L4)",
                    "Agent Tool Misuse (L7)",
                    "Lateral Movement (Cross-Layer)",
                    "Resource Hijacking (L4)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM06:2025 Excessive Agency",
                    "LLM05:2025 Improper Output Handling"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML06:2023 AI Supply Chain Attacks",
                    "ML09:2023 Output Integrity Attack"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "supply chain attack",
                  "lateral movement",
                  "privilege escalation",
                  "containment escape",
                  "artifact",
                  "context",
                  "artifacts",
                  "malicious",
                  "evasion",
                  "untrusted",
                  "compromised",
                  "backdoor",
                  "breakout"
                ],
                "defense": [
                  "security policy",
                  "pre-execution",
                  "behavioral",
                  "analysis",
                  "ephemeral",
                  "sandboxes",
                  "defense",
                  "container",
                  "isolated",
                  "sandbox",
                  "contain",
                  "isolation",
                  "enforce",
                  "policy",
                  "signed"
                ]
              }
            }
          ],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "lateral movement",
              "privilege escalation",
              "containment escape",
              "code execution",
              "compromised",
              "maliciously",
              "breakout",
              "escape"
            ],
            "defense": [
              "execution",
              "sandboxing",
              "runtime",
              "isolation",
              "isolated",
              "sandboxes",
              "containers",
              "limits",
              "permissions",
              "sandbox",
              "preventing"
            ]
          }
        },
        {
          "id": "AID-I-002",
          "name": "Network Segmentation & Isolation for AI Systems",
          "description": "Implement network segmentation and microsegmentation strategies using firewalls, proxies, private endpoints, and transport layer security to enforce strict communication boundaries for AI systems. This involves isolating internal components (e.g., training vs. inference environments, data stores) to limit lateral movement, and securing connections to external dependencies (e.g., MaaS APIs) to prevent data exfiltration, SSRF, and MitM attacks. The goal is to reduce the blast radius of a compromise by enforcing least-privilege network access both internally and externally.",
          "pillar": "infra",
          "phase": "operation",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0044 Full AI Model Access (limits access)",
                "AML.T0036 Data from Information Repositories (limits access)",
                "AML.T0025 Exfiltration via Cyber Means",
                "AML.T0049 Exploit Public-Facing Application (e.g. SSRF-driven internal pivot / forced egress abuse)",
                "AML.T0072 Reverse Shell"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Lateral Movement (Cross-Layer)",
                "Compromised RAG Pipelines (L2, isolating internal DB access)",
                "Data Exfiltration (Cross-Layer)",
                "Orchestration Attacks (L4)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM02:2025 Sensitive Information Disclosure (via exfiltration)",
                "LLM06:2025 Excessive Agency",
                "LLM03:2025 Supply Chain (reduces exposure to compromised upstream model providers / API dependencies)"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML05:2023 Model Theft",
                "ML06:2023 ML Supply Chain Attacks"
              ]
            }
          ],
          "implementationStrategies": [],
          "toolsOpenSource": [],
          "toolsCommercial": [],
          "subTechniques": [
            {
              "id": "AID-I-002.001",
              "name": "Internal AI Network Segmentation",
              "description": "Implement network segmentation and microsegmentation strategies to isolate AI systems and their *internal* components (e.g., training environments, model serving endpoints, data stores, agent control planes) from general corporate networks and other critical IT/OT systems. Enforces strict internal communication rules through firewalls, security groups, and network policies to limit lateral movement and reduce the internal blast radius of a compromise. This also isolates high-privilege agent backends (e.g. orchestration layers with access to credentials, vector DBs, or model registries) from lower-trust, user-facing inference frontends, so that a compromised public-facing agent cannot laterally move into data-rich components.",
              "pillar": "infra",
              "phase": "operation",
              "implementationStrategies": [
                "Host critical AI components on dedicated network segments (VLANs, VPCs).",
                "Apply least privilege to *internal* network communications for AI systems.",
                "Implement microsegmentation (SDN, service mesh, host-based firewalls) for fine-grained internal control.",
                "Separate development/testing environments from production using distinct accounts or projects.",
                "Regularly review and audit internal network segmentation rules."
              ],
              "toolsOpenSource": [
                "Linux Netfilter (iptables, nftables), firewalld",
                "Kubernetes Network Policies",
                "Service Mesh (Istio, Linkerd, Kuma) for internal policies",
                "CNI plugins (Calico, Cilium)",
                "Cloud provider CLIs/SDKs (AWS CLI, gcloud, Azure CLI)",
                "Terraform, Ansible, CloudFormation, Pulumi (for IaC)"
              ],
              "toolsCommercial": [
                "Microsegmentation platforms (Illumio, Guardicore [Akamai], Cisco Secure Workload)",
                "Cloud-native firewall services (AWS Security Groups, Azure NSGs, GCP Firewall Rules)",
                "Commercial Service Mesh offerings (e.g., Istio distributions with enterprise support)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0044 Full AI Model Access (limits internal access)",
                    "AML.T0036 Data from Information Repositories (limits internal access)"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Lateral Movement (Cross-Layer)",
                    "Compromised RAG Pipelines (L2, isolating internal DB access)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM02:2025 Sensitive Information Disclosure (limits internal data exposure)",
                    "LLM06:2025 Excessive Agency (limits internal reach of compromised agent)"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML05:2023 Model Theft (limits internal access to model artifacts)",
                    "ML06:2023 ML Supply Chain Attacks (limits blast radius of internally compromised component)"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "lateral movement",
                  "forensic evasion",
                  "compromise",
                  "credentials",
                  "vector",
                  "compromised",
                  "repudiation",
                  "tampering",
                  "unauthorized"
                ],
                "defense": [
                  "least privilege",
                  "network segmentation",
                  "blast radius",
                  "internal",
                  "network",
                  "segmentation",
                  "isolate",
                  "control",
                  "rules",
                  "firewalls",
                  "policies",
                  "limit",
                  "credentials",
                  "registries",
                  "privilege"
                ]
              }
            },
            {
              "id": "AID-I-002.002",
              "name": "Secure External AI Service Connectivity",
              "description": "Applies strict network path control, transport security, policy mediation, and monitoring specifically to connections originating from the AI system and targeting external services, particularly third-party or Model-as-a-Service (MaaS) foundation model APIs. Aims to prevent data exfiltration, Server-Side Request Forgery (SSRF), Man-in-the-Middle (MitM) attacks, and abuse of external dependencies. This also prevents prompt-injected agents from exfiltrating secrets or invoking arbitrary external services; they can only call approved upstreams through a governed path.",
              "pillar": "infra",
              "phase": "operation",
              "implementationStrategies": [
                "Integrate external MaaS/API endpoints via private network connections.",
                "Enforce strict egress controls using firewalls and proxies with verified DNS/SNI allow-lists.",
                "Implement transport layer security (e.g., mTLS, Certificate Pinning) for critical egress connections.",
                "Utilize API Gateways to mediate external AI service traffic and apply security policies.",
                "Monitor external egress traffic for anomalies and policy violations."
              ],
              "toolsOpenSource": [
                "Open-source API Gateways (Kong, Tyk, APISIX)",
                "Open-source Proxies (Squid, Nginx, HAProxy)",
                "OpenSSL (as a library for verification logic)",
                "SPIFFE/SPIRE (for workload identity for mTLS)",
                "Falco, Cilium Tetragon, Sysdig (for egress monitoring)",
                "Terraform, CloudFormation, Pulumi (for IaC of private endpoints)",
                "Requests (Python library), cURL (as clients needing security)"
              ],
              "toolsCommercial": [
                "Cloud Provider Private Connectivity (AWS PrivateLink, Azure Private Link, Google Private Service Connect)",
                "Commercial API Gateway solutions (Apigee, MuleSoft, AWS API Gateway, Azure API Management)",
                "Cloud-native firewall services (AWS Network Firewall, Azure Firewall Premium, Google Cloud Firewall)",
                "Certificate Management Platforms (Venafi, DigiCert)",
                "SIEM/Log Analytics Platforms (Splunk, Datadog, Sentinel, Chronicle)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0025 Exfiltration via Cyber Means",
                    "AML.T0049 Exploit Public-Facing Application (SSRF)",
                    "AML.T0072 Reverse Shell",
                    "AML.T0034 Cost Harvesting"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Data Exfiltration (Cross-Layer)",
                    "Orchestration Attacks (L4)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM02:2025 Sensitive Information Disclosure (via exfiltration)",
                    "LLM03:2025 Supply Chain (securing external connections)"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML05:2023 Model Theft (securing API access)",
                    "ML06:2023 ML Supply Chain Attacks (securing external component access)"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "data exfiltration",
                  "stealth attack",
                  "persistent threat",
                  "exfiltration",
                  "forgery",
                  "abuse",
                  "secrets",
                  "certificate",
                  "prompt",
                  "evasion",
                  "undetected"
                ],
                "defense": [
                  "secure",
                  "external",
                  "connectivity",
                  "control",
                  "policy",
                  "monitoring",
                  "prevent",
                  "secrets",
                  "governed",
                  "private",
                  "enforce",
                  "firewalls",
                  "proxies",
                  "verified",
                  "certificate"
                ]
              }
            }
          ],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "mitm attack",
              "data exfiltration",
              "lateral movement",
              "privilege escalation",
              "containment escape",
              "exfiltration",
              "compromise",
              "breakout"
            ],
            "defense": [
              "network segmentation",
              "blast radius",
              "network",
              "segmentation",
              "isolation",
              "systems",
              "firewalls",
              "proxies",
              "private",
              "enforce",
              "isolating",
              "limit",
              "securing",
              "prevent",
              "enforcing"
            ]
          }
        },
        {
          "id": "AID-I-003",
          "name": "Quarantine & Throttling of AI Interactions",
          "description": "Implement mechanisms to automatically or manually isolate, rate-limit, or place into a restricted \\\"safe mode\\\" specific AI system interactions when suspicious activity is detected. This could apply to individual user sessions, API keys, IP addresses, or even entire AI agent instances. The objective is to prevent potential attacks from fully executing, spreading, or causing significant harm by quickly containing or degrading the capabilities of the suspicious entity. This is an active response measure triggered by detection systems. This can be applied pre-emptively (automatic) or under human approval (SOAR analyst click-to-quarantine) depending on confidence score, and all actions must be logged/auditable for compliance and forensic review.",
          "pillar": "infra",
          "phase": "response",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0024.002 Invert AI Model (rate-limiting)",
                "AML.T0029 Denial of AI Service (throttling)",
                "AML.T0034 Cost Harvesting (limiting rates)",
                "AML.T0040 AI Model Inference API Access",
                "AML.T0046 Spamming AI System with Chaff Data"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Model Stealing (L1, throttling)",
                "DoS on Framework APIs / Data Infrastructure (L3/L2)",
                "Resource Hijacking (L4, containing processes)",
                "Agent Pricing Model Manipulation (L7, rate limiting)",
                "Model Extraction of AI Security Agents (L6)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM10:2025 Unbounded Consumption (throttling/quarantining)",
                "LLM01:2025 Prompt Injection (containment of repeated prompt-injection-driven abuse after detection)"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML05:2023 Model Theft (throttling excessive queries)"
              ]
            }
          ],
          "implementationStrategies": [
            "Automated quarantine based on high-risk behavior alerts (cut access, move to honeypot, disable key/account).",
            "Dynamic rate limiting for anomalous behavior (query spikes, complex queries).",
            "Stricter rate limits for unauthenticated/less trusted users.",
            "Design AI systems with a 'safe mode' or degraded functionality state.",
            "Utilize SOAR platforms to automate quarantine/throttling actions.",
            "Hallucination Circuit Breaker & Degrade Modes.",
            "Throttle GPU/CPU for a suspicious tenant namespace in Kubernetes."
          ],
          "toolsOpenSource": [
            "Fail2Ban (adapted for AI logs)",
            "Custom scripts (Lambda, Azure Functions, Cloud Functions) for automated actions",
            "API Gateways (Kong, Tyk, Nginx) for rate limiting",
            "Kubernetes for resource quotas/isolation"
          ],
          "toolsCommercial": [
            "API Security and Bot Management solutions (Cloudflare, Akamai, Imperva)",
            "ThreatWarrior (automated detection/response)",
            "SIEM/SOAR platforms (Splunk SOAR, Palo Alto XSOAR, IBM QRadar SOAR)",
            "WAFs with advanced rate limiting"
          ],
          "subTechniques": [],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "anomalous behavior",
              "suspicious activity",
              "suspicious",
              "keys",
              "degrading",
              "key",
              "anomalous",
              "degraded",
              "state",
              "degrade"
            ],
            "defense": [
              "rate limiting",
              "circuit breaker",
              "quarantine",
              "throttling",
              "interactions",
              "honeypot",
              "isolate",
              "safe",
              "detected",
              "sessions",
              "keys",
              "prevent",
              "containing",
              "detection",
              "soar"
            ]
          }
        },
        {
          "id": "AID-I-004",
          "name": "Agent Memory & State Isolation",
          "description": "Manage the lifecycle, integrity, and isolation of agent memory in agentic AI systems. Agent memory (runtime context, tool traces, and persistent vector/RAG stores) is uniquely susceptible to (1) prompt injection persistence, (2) memory/KB poisoning, and (3) cross-session or cross-tenant contamination. This technique family enforces isolation across four layers: Runtime Hygiene (App), Persistent Partitioning (Data), Cryptographic Integrity (Security), and Transactional Promotion Gates (Ops/Governance).",
          "pillar": "app",
          "phase": "operation",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0051 LLM Prompt Injection",
                "AML.T0061 LLM Prompt Self-Replication",
                "AML.T0070 RAG Poisoning",
                "AML.T0080.000 AI Agent Context Poisoning: Memory",
                "AML.T0018.001 Manipulate AI Model: Poison LLM Memory"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Agent Goal Manipulation (L7) (via persistent instruction/memory poisoning)",
                "Agent Tool Misuse (L7) (via poisoned recalled context)",
                "Data Poisoning (L2) (when memory/KB is treated as data)",
                "Compromised RAG Pipelines (L2)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM01:2025 Prompt Injection",
                "LLM02:2025 Sensitive Information Disclosure",
                "LLM04:2025 Data and Model Poisoning",
                "LLM08:2025 Vector and Embedding Weaknesses",
                "LLM10:2025 Unbounded Consumption"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML02:2023 Data Poisoning Attack",
                "ML06:2023 AI Supply Chain Attacks",
                "ML09:2023 Output Integrity Attack"
              ]
            }
          ],
          "implementationStrategies": [],
          "toolsOpenSource": [],
          "toolsCommercial": [],
          "subTechniques": [
            {
              "id": "AID-I-004.001",
              "name": "Runtime Context Isolation & Hygiene",
              "description": "Enforces strict boundaries and hygiene for volatile working memory (RAM/Redis). Prevents cross-session/cross-tenant context bleed, limits the temporal blast radius of prompt injections (via windowing/resets), and prevents resource exhaustion (DoS/cost burn) via size/token ceilings and TTL.",
              "pillar": "app",
              "phase": "operation",
              "implementationStrategies": [
                "Enforce per-session isolation with mandatory TTL, deterministic size/token limits, and fail-closed serialization to prevent cross-tenant bleed and DoS.",
                "Use sliding windows and controlled volatile resets for long-running agents; reseed only from a trusted baseline goal/config (fail-closed)."
              ],
              "toolsOpenSource": [
                "Redis (key TTL, eviction policies)",
                "Memcached",
                "OpenTelemetry (distributed tracing for memory events)",
                "LangChain (memory modules) / Semantic Kernel (memory abstractions)"
              ],
              "toolsCommercial": [
                "Redis Enterprise",
                "Momento (serverless cache)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0051 LLM Prompt Injection",
                    "AML.T0061 LLM Prompt Self-Replication"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM01:2025 Prompt Injection",
                    "LLM02:2025 Sensitive Information Disclosure",
                    "LLM10:2025 Unbounded Consumption"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "prompt injection",
                  "resource exhaustion",
                  "token limit",
                  "anomalous behavior",
                  "lateral movement",
                  "privilege escalation",
                  "containment escape",
                  "context",
                  "memory",
                  "prompt",
                  "token",
                  "drift",
                  "deviation",
                  "degradation",
                  "manipulation"
                ],
                "defense": [
                  "blast radius",
                  "runtime",
                  "context",
                  "isolation",
                  "hygiene",
                  "limits",
                  "token",
                  "enforce",
                  "prevent",
                  "controlled",
                  "trusted",
                  "baseline"
                ]
              }
            },
            {
              "id": "AID-I-004.002",
              "name": "Persistent Memory Partitioning (Trust & Tenant Isolation)",
              "description": "Defines structural isolation for long-term memory (Vector DB/RAG). Uses namespaces/collections partitioned by tenant and trust tier, and enforces retrieval authorization via a centralized policy decision (never by agent self-assertion).",
              "pillar": "data",
              "phase": "building",
              "implementationStrategies": [
                "Partition long-term memory by tenant + trust tier; retrieval must consult a central entitlement/policy service and be fully audited."
              ],
              "toolsOpenSource": [
                "Qdrant (collections/tenancy patterns)",
                "Weaviate (multi-tenancy features)",
                "Milvus (partitions/collections)",
                "OPA (policy-as-code for retrieval authorization)"
              ],
              "toolsCommercial": [
                "Pinecone (indexes/namespaces)",
                "Zilliz Cloud"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0070 RAG Poisoning",
                    "AML.T0080.000 AI Agent Context Poisoning: Memory"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Compromised RAG Pipelines (L2)",
                    "Data Poisoning (L2)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM02:2025 Sensitive Information Disclosure",
                    "LLM08:2025 Vector and Embedding Weaknesses"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "privilege escalation",
                  "unauthorized access",
                  "lateral movement",
                  "containment escape",
                  "forensic evasion",
                  "memory",
                  "vector",
                  "bypass",
                  "breakout",
                  "repudiation",
                  "tampering",
                  "unauthorized"
                ],
                "defense": [
                  "tenant isolation",
                  "persistent",
                  "memory",
                  "partitioning",
                  "trust",
                  "tenant",
                  "isolation",
                  "authorization",
                  "policy",
                  "audited"
                ]
              }
            },
            {
              "id": "AID-I-004.003",
              "name": "Cryptographic Memory Integrity (Signed Write/Verify Read)",
              "description": "Establishes an end-to-end integrity loop for persistent memory: a controlled writer issues signed records (content-hash + metadata + key id), and an integrity-first loader verifies signatures and hashes before any content can re-enter agent context. This prevents direct-to-DB poisoning/tampering and forces memory provenance to be verifiable.",
              "pillar": "app",
              "phase": "operation",
              "implementationStrategies": [
                "Controlled writer: sign canonical metadata containing content-hash + namespace + issuer + timestamp + key-id; store detached signature alongside the record.",
                "Integrity-first loader: verify signature (constant-time) -> re-hash content -> enforce schema/size -> only then return content to the agent (fail-closed)."
              ],
              "toolsOpenSource": [
                "HashiCorp Vault (Transit) / SPIFFE-SVID for workload identity",
                "Sigstore/cosign (attestation patterns)",
                "Python stdlib (hashlib, hmac) / PyCA cryptography (asymmetric signing)"
              ],
              "toolsCommercial": [
                "AWS KMS",
                "Azure Key Vault",
                "Google Cloud KMS"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0018.001 Manipulate AI Model: Poison LLM Memory",
                    "AML.T0080.000 AI Agent Context Poisoning: Memory"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM04:2025 Data and Model Poisoning"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML02:2023 Data Poisoning Attack",
                    "ML09:2023 Output Integrity Attack"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "data poisoning",
                  "supply chain attack",
                  "untrusted data",
                  "unauthorized modification",
                  "man in the middle",
                  "memory",
                  "key",
                  "context",
                  "poisoning",
                  "tampering",
                  "direct",
                  "corruption",
                  "manipulation",
                  "forgery",
                  "interception"
                ],
                "defense": [
                  "cryptographic",
                  "memory",
                  "integrity",
                  "signed",
                  "write",
                  "verify",
                  "read",
                  "controlled",
                  "key",
                  "provenance",
                  "sign",
                  "containing",
                  "signature",
                  "enforce"
                ]
              }
            },
            {
              "id": "AID-I-004.004",
              "name": "Transactional Promotion Gates (Quarantine -> Trusted)",
              "description": "Implements a strict state machine and atomic promotion workflow for high-risk memory writes. Items routed into quarantine cannot influence agent behavior until reviewed and promoted. Promotion must be transactional, auditable, and typically re-signed as trusted. Aligns with trust-tiered memory write-gates (e.g., trusted/probation/quarantined).",
              "pillar": "app",
              "phase": "operation",
              "implementationStrategies": [
                "Quarantine state machine with atomic promotion: enforce allowed transitions and re-sign on promotion (fail-closed)."
              ],
              "toolsOpenSource": [
                "PostgreSQL (transactions, row locks, RLS)",
                "Kafka / Redis Streams (promotion queues)",
                "Temporal / Celery (workflow execution)",
                "OPA (policy-as-code for approval rules)"
              ],
              "toolsCommercial": [
                "ServiceNow (approval workflows)",
                "Jira Service Management"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0070 RAG Poisoning",
                    "AML.T0080.000 AI Agent Context Poisoning: Memory"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Data Poisoning (L2)",
                    "Compromised RAG Pipelines (L2)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM04:2025 Data and Model Poisoning",
                    "LLM08:2025 Vector and Embedding Weaknesses"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "forensic evasion",
                  "lateral movement",
                  "privilege escalation",
                  "containment bypass",
                  "state",
                  "memory",
                  "repudiation",
                  "tampering",
                  "unauthorized",
                  "breakout",
                  "escape"
                ],
                "defense": [
                  "transactional",
                  "promotion",
                  "gates",
                  "quarantine",
                  "trusted",
                  "quarantined",
                  "enforce",
                  "isolation",
                  "containment",
                  "segmentation",
                  "sandbox",
                  "separation",
                  "boundary",
                  "enclave"
                ]
              }
            }
          ],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "prompt injection",
              "unauthorized modification",
              "man in the middle",
              "persistence",
              "memory",
              "state",
              "context",
              "vector",
              "prompt",
              "injection",
              "poisoning",
              "tampering",
              "corruption",
              "manipulation",
              "forgery"
            ],
            "defense": [
              "agent",
              "memory",
              "state",
              "isolation",
              "integrity",
              "cryptographic",
              "governance",
              "containment",
              "segmentation",
              "sandbox",
              "separation",
              "boundary",
              "quarantine",
              "enclave"
            ]
          }
        },
        {
          "id": "AID-I-005",
          "name": "Emergency \"Kill-Switch\" / AI System Halt",
          "description": "Establish and maintain a reliable, rapidly invokable mechanism to immediately halt, disable, or severely restrict the operation of an AI model or autonomous agent if it exhibits confirmed critical malicious behavior, goes \\\"rogue\\\" (acts far outside its intended parameters in a harmful way), or if a severe, ongoing attack is detected and other containment measures are insufficient. This is a last-resort containment measure designed to prevent catastrophic harm or further compromise.",
          "pillar": "infra",
          "phase": "response",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0048 External Harms (Societal, Financial, Reputational, User)",
                "AML.T0029 Denial of AI Service (by runaway agent)"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Agent acting on compromised goals/tools leading to severe harm (L7)",
                "Runaway/critically malfunctioning foundation models (L1)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM06:2025 Excessive Agency (ultimate backstop)",
                "LLM10:2025 Unbounded Consumption (preventing catastrophic costs)"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "Any ML attack scenario causing immediate, unacceptable harm requiring emergency shutdown."
              ]
            }
          ],
          "implementationStrategies": [
            "Automated safety monitors and triggers for critical deviations.",
            "Provide secure, MFA-protected manual override for human operators.",
            "Design agents with internal watchdog threads that terminate unresponsive or runaway behavior.",
            "Define and version-control a formal Kill-Switch Activation SOP.",
            "Develop a controlled post-halt restart and verification checklist (cold start procedure).",
            "Document and govern the kill-switch within Human-in-the-Loop (HITL) control mapping (AID-M-006).",
            "Enforce global/tenant halt flags at every inference request path and agent loop (fail-closed)."
          ],
          "toolsOpenSource": [
            "Custom scripts/automation playbooks (Ansible, cloud CLIs) to stop/delete resources",
            "Circuit breaker patterns in microservices"
          ],
          "toolsCommercial": [
            "\\\"Red Button\\\" solutions from AI platform vendors",
            "Edge AI Safeguard solutions",
            "EDR/XDR solutions (SentinelOne, CrowdStrike) to kill processes/isolate hosts"
          ],
          "subTechniques": [],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "lateral movement",
              "attack surface",
              "malicious",
              "rogue",
              "parameters",
              "harmful",
              "attack",
              "compromise",
              "reconnaissance",
              "exposure"
            ],
            "defense": [
              "emergency",
              "kill-switch",
              "system",
              "halt",
              "human-in-the-loop",
              "reliable",
              "detected",
              "containment",
              "prevent",
              "safety",
              "secure",
              "controlled",
              "verification",
              "document",
              "govern"
            ]
          }
        },
        {
          "id": "AID-I-006",
          "name": "Malicious Participant Isolation in Federated Unlearning",
          "description": "Identifies and logically isolates the influence of malicious clients within a Federated Learning (FL) system, particularly during a machine unlearning or model restoration process. Once identified, the malicious participants' data contributions and model updates are excluded from the unlearning or retraining calculations. This technique is critical for preventing attackers from sabotaging the model recovery process and ensuring the final restored model is not corrupted.",
          "pillar": "model",
          "phase": "response",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0020 Poison Training Data",
                "AML.T0019 Publish Poisoned Datasets"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Data Poisoning (L2)",
                "Attacks on Decentralized Learning (Cross-Layer)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM04:2025 Data and Model Poisoning"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML02:2023 Data Poisoning Attack",
                "ML10:2023 Model Poisoning"
              ]
            }
          ],
          "implementationStrategies": [
            "Identify malicious participants by clustering their historical model updates.",
            "Logically exclude contributions from isolated clients during the unlearning or retraining process.",
            "Apply a real-time filtering strategy during the unlearning process to isolate new malicious updates.",
            "Maintain a dynamic reputation score and exclude any client whose score falls below a critical threshold.",
            "Require cryptographic attestation or signed client updates before accepting them into training or unlearning.",
            "Enforce a persistent isolation/blocklist at training-time and unlearning-time (fail-closed)."
          ],
          "toolsOpenSource": [
            "TensorFlow Federated (TFF)",
            "Flower (Federated Learning Framework)",
            "PySyft (OpenMined)",
            "NVIDIA FLARE",
            "scikit-learn (for clustering/anomaly detection)",
            "PyTorch",
            "TensorFlow",
            "SPIFFE/SPIRE (for workload identity attestation and signed workload identities)"
          ],
          "toolsCommercial": [
            "Enterprise Federated Learning Platforms (Owkin, Substra Foundation, IBM)",
            "MLOps Platforms with Federated Learning capabilities (Amazon SageMaker)",
            "AI Security Platforms (Protect AI, HiddenLayer)"
          ],
          "subTechniques": [],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "man in the middle",
              "malicious content",
              "harmful output",
              "data exfiltration",
              "lateral movement",
              "privilege escalation",
              "containment escape",
              "malicious",
              "corrupted",
              "tampering",
              "forgery",
              "interception",
              "injection",
              "breakout",
              "ransomware"
            ],
            "defense": [
              "federated learning",
              "anomaly detection",
              "malicious",
              "participant",
              "isolation",
              "federated",
              "unlearning",
              "restoration",
              "preventing",
              "recovery",
              "restored",
              "isolated",
              "filtering",
              "isolate",
              "threshold"
            ]
          }
        },
        {
          "id": "AID-I-007",
          "name": "Client-Side AI Execution Isolation",
          "description": "This technique focuses on containing a compromised or malicious client-side model, preventing it from accessing sensitive data from other browser tabs, local application context, or the operating system. It addresses the security challenges of AI models that execute in untrusted environments like a user's web browser, Electron shell, hybrid mobile app, or native mobile runtime. This assumes the model or model runtime may already be tampered with or coerced (e.g. prompt-injected, modified weights, wrapped with hostile JS). The goal is not to \"fix\" the model but to strictly confine its blast radius using sandboxing, least capability, and controlled IPC.",
          "pillar": "app",
          "phase": "operation",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0025 Exfiltration via Cyber Means (from client device)",
                "AML.T0037 Data from Local System (stealing browser/app state, session tokens, local storage)"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Data Exfiltration (L2, from the client)",
                "Runtime Code Injection (L4, hostile JS/WASM in the browser process or hybrid app sandbox)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM02:2025 Sensitive Information Disclosure (blocking direct access to other DOM state / tokens / org data)",
                "LLM05:2025 Improper Output Handling (preventing model-produced HTML/JS from gaining privileged DOM execution)"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML06:2023 AI Supply Chain Attacks (containing a malicious downloaded client-side model/runtime so it cannot pivot)"
              ]
            }
          ],
          "implementationStrategies": [
            "Execute AI models in a dedicated Web Worker.",
            "Run untrusted models or their UI components in a sandboxed iframe.",
            "Leverage WebAssembly (WASM) runtimes for a capabilities-based sandbox.",
            "Utilize Content Security Policy (CSP) to restrict model data exfiltration and script execution.",
            "Enforce a minimal, allowlisted native bridge between the AI runtime and device/system capabilities (mobile, Electron, hybrid apps)."
          ],
          "toolsOpenSource": [
            "WebAssembly runtimes (Wasmtime, Wasmer, browser WebAssembly runtime)",
            "TensorFlow.js, ONNX.js",
            "Web Workers (Browser API)",
            "Browser postMessage() channel (structured clone IPC for sandboxed components)",
            "Sandboxed iframes (HTML5 iframe with sandbox attribute)",
            "Content Security Policy (CSP) headers"
          ],
          "toolsCommercial": [
            "Mobile OS sandboxing (iOS App Sandbox, Android Application Sandbox)",
            "Enterprise Mobile Device Management (MDM) solutions with app sandboxing / clipboard control / data loss prevention"
          ],
          "subTechniques": [],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "data exfiltration",
              "lateral movement",
              "privilege escalation",
              "containment escape",
              "code execution",
              "compromised",
              "malicious",
              "context",
              "untrusted",
              "tampered",
              "weights",
              "hostile",
              "exfiltration",
              "breakout",
              "escape"
            ],
            "defense": [
              "blast radius",
              "security policy",
              "client-side",
              "execution",
              "isolation",
              "sandboxing",
              "containing",
              "preventing",
              "controlled",
              "sandbox",
              "policy",
              "enforce"
            ]
          }
        }
      ]
    },
    {
      "id": "deceive",
      "name": "Deceive",
      "description": "The \"Deceive\" tactic involves the strategic use of decoys, misinformation, or the manipulation of an adversary's perception of the AI system and its environment. The objectives are to misdirect attackers away from real assets, mislead them about the system's true vulnerabilities or value, study their attack methodologies in a safe environment, waste their resources, or deter them from attacking altogether.",
      "techniques": [
        {
          "id": "AID-DV-001",
          "name": "Honeypot AI Services & Decoy Models/APIs",
          "description": "Deploy decoy AI systems, such as fake LLM APIs, ML model endpoints serving synthetic or non-sensitive data, or imitation agent services, that are designed to appear valuable, vulnerable, or legitimate to potential attackers. These honeypots are instrumented for intensive monitoring to log all interactions, capture attacker TTPs (Tactics, Techniques, and Procedures), and gather threat intelligence without exposing real production systems or data. They can also be used to slow down attackers or waste their resources. All honeypot services must be logically isolated from production networks, run with read-only/non-destructive behaviors, and be instrumented for forensic retention (full request capture, replayable context).",
          "pillar": "infra",
          "phase": "operation",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.TA0002 Reconnaissance",
                "AML.T0024.002 Invert AI Model (model extraction / inversion attempts)",
                "AML.T0048.004 External Harms: AI Intellectual Property Theft",
                "AML.T0051 LLM Prompt Injection",
                "AML.T0054 LLM Jailbreak"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Model Stealing (L1)",
                "Marketplace Manipulation (L7, decoy agents)",
                "Evasion of Detection (L5, studying evasion attempts)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM01:2025 Prompt Injection (capturing attempts)",
                "LLM10:2025 Unbounded Consumption (studying resource abuse)"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML05:2023 Model Theft (luring to decoy)",
                "ML01:2023 Input Manipulation Attack (observing attempts)"
              ]
            }
          ],
          "implementationStrategies": [
            "Set up AI model instances with controlled weaknesses/attractive characteristics.",
            "Instrument honeypot AI service for detailed logging.",
            "Design honeypots to mimic production services but ensure strict network isolation.",
            "Return believable but resource-draining responses (latency, jitter, soft failures).",
            "Integrate honeypot telemetry with central security monitoring (SIEM/SOC).",
            "Seed LLM honeypots with jailbreak trigger detection and scripted \"fake compliance.\"",
            "Apply controlled friction, deception, and output degradation only inside the honeypot surface (not production)."
          ],
          "toolsOpenSource": [
            "General honeypot frameworks (Cowrie, Dionaea, Conpot) adapted to emulate LLM/agent admin surfaces",
            "Intentionally weakened / instrumented open-source LLM deployment (e.g. Vicuna / Mistral / Llama derivatives) running in an isolated VPC for attacker interaction capture",
            "Mock API tools (MockServer, WireMock)",
            "Reverse proxy / API gateway with custom middleware (Kong / Nginx / Envoy) for deception routing and request capture"
          ],
          "toolsCommercial": [
            "Deception technology platforms (TrapX, SentinelOne ShadowPlex, Illusive, Acalvio)",
            "Specialized AI security vendors with AI honeypot capabilities"
          ],
          "subTechniques": [],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "model endpoint",
              "apt",
              "vulnerable",
              "attacker",
              "threat",
              "exposing",
              "context",
              "weaknesses",
              "jailbreak",
              "deception",
              "degradation"
            ],
            "defense": [
              "security monitoring",
              "network isolation",
              "honeypot",
              "services",
              "decoy",
              "models",
              "apis",
              "monitoring",
              "log",
              "isolated",
              "controlled",
              "logging",
              "isolation",
              "siem",
              "detection"
            ]
          }
        },
        {
          "id": "AID-DV-002",
          "name": "Honey Data, Decoy Artifacts & Canary Tokens for AI",
          "description": "Strategically seed the AI ecosystem (training datasets, model repositories, configuration files, API documentation) with enticing but fake data, decoy model artifacts (e.g., a seemingly valuable but non-functional or instrumented model file), or canary tokens (e.g., fake API keys, embedded URLs in documents). These \\\"honey\\\" elements are designed to be attractive to attackers. If an attacker accesses, exfiltrates, or attempts to use these decoys, it triggers an alert, signaling a breach or malicious activity and potentially providing information about the attacker's actions or location.",
          "pillar": "data",
          "phase": "building",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0025 Exfiltration via Cyber Means (honey data/canaries exfiltrated)",
                "AML.T0024.002 Invert AI Model (decoy model/canary in docs)",
                "AML.T0010 AI Supply Chain Compromise (countering with fake vulnerable models)"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Data Exfiltration (L2, detecting honey data exfil)",
                "Model Stealing (L1, decoy models/watermarked data)",
                "Unauthorized access to layers with honey tokens"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM02:2025 Sensitive Information Disclosure (honey data mimicking sensitive info)",
                "LLM03:2025 Supply Chain (decoy artifacts accessed)"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML05:2023 Model Theft (decoy models/API keys)",
                "ML01:2023 Input Manipulation Attack (observing attempts)"
              ]
            }
          ],
          "implementationStrategies": [
            "Embed unique, synthetic honey records in datasets/databases.",
            "Publish fake/instrumented decoy model artifacts.",
            "Create and embed decoy API keys/access tokens (Canary Tokens).",
            "Embed trackable URLs / web bugs in fake sensitive documents.",
            "Watermark synthetic data in honeypots / decoys.",
            "Ensure honey elements are isolated and cannot impact production.",
            "Integrate honey element alerts into security monitoring."
          ],
          "toolsOpenSource": [
            "Canarytokens.org by Thinkst",
            "Synthetic data generation tools (Faker, SDV)",
            "Custom scripts for decoy files/API keys"
          ],
          "toolsCommercial": [
            "Thinkst Canary (commercial platform)",
            "Deception platforms (Illusive, Acalvio, SentinelOne) with data decoy capabilities",
            "Some DLP solutions adaptable for honey data"
          ],
          "subTechniques": [],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "compliance violation",
              "stealth attack",
              "persistent threat",
              "artifacts",
              "tokens",
              "keys",
              "attacker",
              "breach",
              "malicious",
              "misconfiguration",
              "misuse",
              "unauthorized",
              "evasion",
              "undetected",
              "intrusion"
            ],
            "defense": [
              "security monitoring",
              "canary token",
              "honey",
              "data",
              "decoy",
              "artifacts",
              "canary",
              "tokens",
              "honeypot",
              "repositories",
              "documentation",
              "keys",
              "alert",
              "isolated",
              "monitoring"
            ]
          }
        },
        {
          "id": "AID-DV-003",
          "name": "Dynamic Response Manipulation for AI Interactions",
          "description": "Implement mechanisms where the AI system, upon detecting suspicious or confirmed adversarial interaction patterns (e.g., repeated prompt injection attempts, queries indicative of model extraction), deliberately alters its responses to be misleading, unhelpful, or subtly incorrect to the adversary. This aims to frustrate the attacker's efforts, waste their resources, make automated attacks less reliable, and potentially gather more intelligence on their TTPs without revealing the deception. The AI might simultaneously alert defenders to the ongoing deceptive engagement.",
          "pillar": "app",
          "phase": "response",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0024.002 Invert AI Model (misleading outputs)",
                "AML.T0051 LLM Prompt Injection (unreliable/misleading payloads)",
                "AML.T0054 LLM Jailbreak (unreliable/misleading payloads)",
                "AML.TA0002 Reconnaissance (inaccurate system info)"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Model Stealing (L1, frustrating extraction)",
                "Agent Goal Manipulation / Agent Tool Misuse (L7, agent feigns compliance)",
                "Evasion of Detection (L5, harder to confirm evasion success)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM01:2025 Prompt Injection (unreliable outcome for attacker)",
                "LLM02:2025 Sensitive Information Disclosure (fake/obfuscated data)"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML05:2023 Model Theft (unusable responses)",
                "ML01:2023 Input Manipulation Attack (inconsistent/noisy outputs)"
              ]
            }
          ],
          "implementationStrategies": [
            "Provide subtly incorrect, incomplete, or nonsensical outputs to suspected malicious actors.",
            "Introduce controlled randomization or benign noise into model outputs for suspicious sessions.",
            "For agentic systems, feign compliance with malicious instructions but execute safe no-ops.",
            "Subtly degrade quality or utility of responses for queries that match model extraction patterns.",
            "Ensure all deceptive responses are clearly tagged in internal monitoring and incident telemetry."
          ],
          "toolsOpenSource": [
            "LangChain (custom router / tool interception logic for suspicious sessions)",
            "Microsoft Semantic Kernel (planner and tool-call interception with policy checks)",
            "Custom reverse proxy or FastAPI/Express middleware that routes suspicious requests to deceptive responses instead of the real model"
          ],
          "toolsCommercial": [
            "Commercial LLM firewalls / AI security gateways that can intercept prompts and rewrite or obfuscate responses for high-risk requests",
            "Commercial cyber deception platforms (e.g. TrapX-style / Acalvio-style) extended to AI/LLM endpoints to observe attacker behavior under controlled deception"
          ],
          "subTechniques": [],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "prompt injection",
              "model extraction",
              "response manipulation",
              "automated attack",
              "manipulation",
              "suspicious",
              "adversarial",
              "prompt",
              "injection",
              "extraction",
              "adversary",
              "attacker",
              "deception",
              "deceptive",
              "malicious"
            ],
            "defense": [
              "dynamic",
              "response",
              "manipulation",
              "interactions",
              "detecting",
              "reliable",
              "alert",
              "controlled",
              "sessions",
              "compliance",
              "safe",
              "monitoring"
            ]
          }
        },
        {
          "id": "AID-DV-004",
          "name": "AI Output Watermarking & Telemetry Traps",
          "description": "Embed imperceptible or hard-to-remove watermarks, unique identifiers, or telemetry \\\"beacons\\\" into the outputs generated by AI models (e.g., text, images, code). If these outputs are found externally (e.g., on the internet, in a competitor's product, in leaked documents), the watermark or beacon can help trace the output back to the originating AI system, potentially identifying model theft, misuse, or data leakage. Telemetry traps involve designing the AI to produce specific, unique (but benign) outputs for certain rare or crafted inputs, which, if observed externally, indicate that the model or its specific knowledge has been compromised or replicated. These watermarks and telemetry markers act as high-fidelity leak detectors: if they are observed outside trusted runtime or appear in external systems, that is treated as a security incident signal (possible model theft, supply chain compromise, or enterprise data exfiltration), not just IP branding.",
          "pillar": "data",
          "phase": "operation",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0024.002 Invert AI Model / AML.T0048.004 External Harms: AI Intellectual Property Theft",
                "AML.T0057 LLM Data Leakage (tracing watermarked outputs)",
                "AML.T0048.002 External Harms: Societal Harm (attributing deepfakes/misinfo)"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Model Stealing (L1, identifying stolen outputs)",
                "Data Exfiltration (L2, exfiltrated watermarked data)",
                "Inaccurate Agent Capability Description (L7) (Allows attribution of false claims produced by a malicious/forged agent pretending to be ours)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM02:2025 Sensitive Information Disclosure (leaked watermarked output)",
                "LLM09:2025 Misinformation (identifying AI-generated content)"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML05:2023 Model Theft (traceable models/outputs)",
                "ML09:2023 Output Integrity Attack (watermark destruction reveals tampering)"
              ]
            }
          ],
          "implementationStrategies": [
            "For text, subtly alter word choices, sentence structures, or token frequencies.",
            "For images, embed imperceptible digital watermarks in pixel data.",
            "For AI-generated video, apply imperceptible watermarks to frames before final encoding.",
            "Instrument model APIs with unique telemetry markers for specific queries.",
            "Inject unique, identifiable synthetic data points into training set for provenance.",
            "Ensure watermarks/telemetry don't degrade performance or UX.",
            "Develop robust methods for detecting watermarks/telemetry externally."
          ],
          "toolsOpenSource": [
            "MarkLLM (watermarking LLM text)",
            "SynthID (Google, watermarking AI-generated images/text)",
            "Steganography libraries (adaptable)",
            "Research tools for robust NN output watermarking"
          ],
          "toolsCommercial": [
            "Verance Watermarking (AI content)",
            "Sensity AI (deepfake detection/watermarking)",
            "Commercial digital watermarking solutions",
            "Content authenticity platforms"
          ],
          "subTechniques": [],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "model theft",
              "supply chain compromise",
              "crafted input",
              "data exfiltration",
              "data leakage",
              "theft",
              "misuse",
              "leakage",
              "compromised",
              "leak",
              "compromise",
              "exfiltration",
              "token",
              "inject",
              "degrade"
            ],
            "defense": [
              "output",
              "watermarking",
              "telemetry",
              "traps",
              "trap",
              "trace",
              "trusted",
              "token",
              "provenance",
              "robust",
              "detecting"
            ]
          }
        },
        {
          "id": "AID-DV-005",
          "name": "Decoy Agent Behaviors & Canary Tasks",
          "description": "For autonomous AI agents, design and implement decoy or \\\"canary\\\" functionalities, goals, or sub-agents that appear valuable or sensitive but are actually monitored traps. If an attacker successfully manipulates an agent (e.g., via prompt injection or memory poisoning) and directs it towards these decoy tasks or to exhibit certain predefined suspicious behaviors, it triggers an alert, revealing the compromise attempt and potentially the attacker's intentions, without risking real assets.",
          "pillar": "app",
          "phase": "operation",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0051 LLM Prompt Injection",
                "AML.T0054 LLM Jailbreak (injection leads to canary task)",
                "AML.T0018.001 Manipulate AI Model: Poison LLM Memory (poisoned memory leads to decoy goal)"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Agent Goal Manipulation / Agent Tool Misuse (L7, luring to decoy tools/goals)",
                "Agent Identity Attack (L7) (directing to canary tasks)",
                "Orchestration Attacks (L4) (If an attacker is abusing deployment/runtime orchestration to steer the agent toward high-privilege behavior, the decoy component will be triggered and alert)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM01:2025 Prompt Injection (detecting successful diversion to decoy)",
                "LLM06:2025 Excessive Agency (agent attempts to use decoy tool)"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML01:2023 Input Manipulation Attack (If an attacker injects malicious instructions, the agent will attempt decoy tools/goals, which triggers alerts.)",
                "ML08:2023 Model Skewing (If persistent manipulation alters the agent's decision policy, scheduled audits and canary tasks detect that drift.)"
              ]
            }
          ],
          "implementationStrategies": [
            "Equip agent with shadow/canary goal/tool leading to monitored environment.",
            "Create dummy 'watcher' agent personas.",
            "Issue benign 'test prompts' or 'internal audit' instructions to agent.",
            "Design agents to report attempts to perform actions outside capabilities/ethics.",
            "Ensure decoy behaviors are well-instrumented and isolated."
          ],
          "toolsOpenSource": [
            "Agentic Radar (CLI scanner, adaptable for decoy tests)",
            "Custom logic in agentic frameworks (AutoGen, CrewAI, Langroid) for canary tasks",
            "Integration with logging/alerting systems (ELK, Prometheus)"
          ],
          "toolsCommercial": [
            "Emerging AI safety/agent monitoring platforms",
            "Adaptable deception technology platforms"
          ],
          "subTechniques": [],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "prompt injection",
              "attacker",
              "prompt",
              "injection",
              "memory",
              "poisoning",
              "suspicious",
              "compromise",
              "assets",
              "prompts"
            ],
            "defense": [
              "decoy",
              "agent",
              "behaviors",
              "canary",
              "tasks",
              "trap",
              "monitored",
              "alert",
              "audit",
              "isolated"
            ]
          }
        },
        {
          "id": "AID-DV-006",
          "name": "Deceptive System Information",
          "description": "When probed by unauthenticated or suspicious users, the AI system deliberately returns misleading information about its runtime stack, capabilities, or underlying models (e.g., fake Server headers, generic LLM identity claims, honeypot /debug endpoints). This frustrates reconnaissance and slows attackers' ability to map real assets. Trusted / authenticated traffic is exempt so that observability, auditability, and compliance are not harmed.",
          "pillar": "infra",
          "phase": "operation",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0007 Discover AI Artifacts",
                "AML.T0069 Discover LLM System Information"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Malicious Agent Discovery (L7)",
                "Agent Identity Attack (L7) (Prevents accurate fingerprinting / impersonation of real agents by feeding misleading identity data)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM07:2025 System Prompt Leakage"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML05:2023 Model Theft (Misleads adversaries attempting to fingerprint or clone the model or runtime stack)"
              ]
            }
          ],
          "implementationStrategies": [
            "Modify API server headers (e.g., 'Server', 'X-Powered-By') to return decoy information.",
            "Configure LLMs with system prompts that provide a controlled, non-truthful identity to untrusted requesters.",
            "Expose honeypot /debug or /env endpoints that return fake stack details and raise alerts.",
            "Use an API gateway or proxy to intercept and spoof responses for high-risk reconnaissance paths.",
            "Apply deception only to untrusted traffic and bypass it for trusted/monitoring flows."
          ],
          "toolsOpenSource": [
            "API Gateway configurations (Kong, Tyk, Nginx)",
            "Web server configuration files (.htaccess for Apache, nginx.conf)",
            "Custom code in application logic to handle specific queries."
          ],
          "toolsCommercial": [
            "Deception technology platforms.",
            "API management and security solutions."
          ],
          "subTechniques": [],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "system prompt",
              "deceptive",
              "suspicious",
              "assets",
              "prompts",
              "untrusted",
              "expose",
              "spoof",
              "deception",
              "bypass"
            ],
            "defense": [
              "deceptive",
              "system",
              "information",
              "honeypot",
              "decoy",
              "trusted",
              "authenticated",
              "observability",
              "compliance",
              "controlled",
              "gateway",
              "proxy",
              "monitoring"
            ]
          }
        },
        {
          "id": "AID-DV-007",
          "name": "Training-Phase Obfuscation for Model Inversion Defense",
          "description": "During model training, intentionally introduce controlled statistical noise and regularization (e.g. input perturbation, label smoothing, or differentially private SGD) so that the trained model does not overfit individual training examples. The goal is to reduce how confidently the model can reproduce or expose specific records from the training set. This both frustrates model inversion attacks (reconstructing private examples from outputs) and weakens membership inference signals (whether a specific record was in training). ",
          "pillar": "model",
          "phase": "building",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0024.001 Exfiltration via AI Inference API: Invert AI Model",
                "AML.T0024.000 Exfiltration via AI Inference API: Infer Training Data Membership"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Membership Inference Attacks (L1) (Prevents the attacker from determining if a specific record was in training)",
                "Model Stealing (L1) (Makes direct extraction of training-specific signals less reliable / higher noise)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM02:2025 Sensitive Information Disclosure"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML03:2023 Model Inversion Attack"
              ]
            }
          ],
          "implementationStrategies": [
            "Add calibrated noise directly to input data during each training step.",
            "Apply label smoothing to prevent the model from becoming over-confident in its predictions.",
            "Use differentially private training to cryptographically bound the influence of any single training record."
          ],
          "toolsOpenSource": [
            "PyTorch, TensorFlow (for implementing custom training loops and loss functions)",
            "Opacus (for PyTorch Differential Privacy)",
            "TensorFlow Privacy",
            "NumPy"
          ],
          "toolsCommercial": [
            "Privacy-Enhancing Technology Platforms (Gretel.ai, Tonic.ai, SarUS, Immuta)",
            "AI Security Platforms (Protect AI, HiddenLayer, Robust Intelligence)",
            "MLOps Platforms (Amazon SageMaker, Google Vertex AI, Databricks)"
          ],
          "subTechniques": [],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "model inversion",
              "membership inference",
              "man in the middle",
              "expose",
              "tampering",
              "forgery",
              "interception",
              "reconnaissance",
              "attacker",
              "intrusion",
              "probing",
              "enumeration"
            ],
            "defense": [
              "differential privacy",
              "training-phase",
              "obfuscation",
              "model",
              "inversion",
              "defense",
              "controlled",
              "private",
              "prevent",
              "deception",
              "honeypot",
              "decoy",
              "trap",
              "misdirection",
              "canary"
            ]
          }
        },
        {
          "id": "AID-DV-008",
          "name": "Poisoning Detection Canaries & Decoy Data",
          "description": "This technique involves proactively embedding synthetic 'canary' or 'sentinel' data points into a training set to deceive and detect data poisoning attacks. These canaries are specifically crafted to be easily learned by the model under normal conditions. During training, the model's behavior on these specific points is monitored. If a data poisoning attack disrupts the overall data distribution or the training process, it will cause an anomalous reaction on these canaries (e.g., a sudden spike in loss, a change in prediction), triggering a high-fidelity alert that reveals the attack without the adversary realizing their method has been detected.",
          "pillar": "data",
          "phase": "building",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0020 Poison Training Data",
                "AML.T0031 Erode AI Model Integrity",
                "AML.T0059 Erode Dataset Integrity (Detects downstream model-behavior anomalies caused by altered / injected data, even if the raw poisoned records are not explicitly identified)"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Data Poisoning (L2)",
                "Data Tampering (L2) (Detects malicious manipulation of training data / distribution shift via canary anomalies)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM04:2025 Data and Model Poisoning"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML02:2023 Data Poisoning Attack",
                "ML10:2023 Model Poisoning"
              ]
            }
          ],
          "implementationStrategies": [
            "Craft and inject synthetic canary data points into the training set.",
            "Continuously monitor the model's loss and prediction accuracy specifically on the canary data points during training.",
            "Design canaries as 'gradient traps' that produce anomalously large gradients if perturbed.",
            "Ensure canary data is statistically similar to real data to evade attacker filtering."
          ],
          "toolsOpenSource": [
            "MLOps platforms with real-time metric logging (MLflow, Weights & Biases)",
            "Data generation libraries (Faker, NumPy)",
            "Deep learning frameworks (PyTorch, TensorFlow)",
            "Monitoring and alerting tools (Prometheus, Grafana)"
          ],
          "toolsCommercial": [
            "AI Observability Platforms (Arize AI, Fiddler, WhyLabs)",
            "MLOps platforms (Databricks, SageMaker, Vertex AI)",
            "Data-centric AI platforms (Snorkel AI)"
          ],
          "subTechniques": [],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "data poisoning",
              "poisoning",
              "embedding",
              "attack",
              "anomalous",
              "adversary",
              "inject",
              "gradient",
              "gradients",
              "evade",
              "attacker"
            ],
            "defense": [
              "poisoning",
              "detection",
              "canaries",
              "decoy",
              "data",
              "trap",
              "detect",
              "monitored",
              "alert",
              "detected",
              "monitor",
              "filtering"
            ]
          }
        }
      ]
    },
    {
      "id": "evict",
      "name": "Evict",
      "description": "The \"Evict\" tactic focuses on the active removal of an adversary's presence from a compromised AI system and the elimination of any malicious artifacts they may have introduced. Once an intrusion or malicious activity has been detected and contained, eviction procedures are executed to ensure the attacker is thoroughly expelled, their access mechanisms are dismantled, and any lingering malicious code, data, or configurations are purged.",
      "techniques": [
        {
          "id": "AID-E-001",
          "name": "Credential Revocation & Rotation for AI Systems",
          "description": "Immediately revoke, invalidate, or rotate any credentials (e.g., API keys, access tokens, user account passwords, service account credentials, certificates) that are known or suspected to have been compromised or used by an adversary to gain unauthorized access to or interact maliciously with AI systems, models, data, or MLOps pipelines. This action aims to cut off the attacker's current access and prevent them from reusing stolen credentials.",
          "pillar": "infra",
          "phase": "response",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0012 Valid Accounts",
                "AML.T0055 Unsecured Credentials"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Agent Identity Attack (L7)",
                "Compromised Agent Registry (L7)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM02:2025 Sensitive Information Disclosure (if creds stolen)",
                "LLM06:2025 Excessive Agency (if agent creds compromised)"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML05:2023 Model Theft (if via compromised creds)"
              ]
            }
          ],
          "implementationStrategies": [],
          "toolsOpenSource": [],
          "toolsCommercial": [],
          "subTechniques": [
            {
              "id": "AID-E-001.001",
              "name": "Foundational Credential Management",
              "description": "This sub-technique covers the standard, proactive lifecycle management and incident response for credentials associated with human users and traditional services (e.g., database accounts, long-lived service account keys). It includes essential security hygiene practices like regularly rotating secrets, as well as reactive measures such as forcing password resets and cleaning up unauthorized accounts after a compromise has been detected.",
              "pillar": "infra",
              "phase": "response",
              "implementationStrategies": [
                "Implement a rapid rotation process for all secrets.",
                "Force password resets for compromised user accounts.",
                "Remove unauthorized accounts or API keys created by an attacker."
              ],
              "toolsOpenSource": [
                "Cloud provider CLIs/SDKs (AWS CLI, gcloud, Azure CLI)",
                "HashiCorp Vault",
                "Keycloak",
                "Ansible, Puppet, Chef (for orchestrating credential updates)"
              ],
              "toolsCommercial": [
                "Privileged Access Management (PAM) solutions (CyberArk, Delinea, BeyondTrust)",
                "Identity-as-a-Service (IDaaS) platforms (Okta, Ping Identity, Auth0)",
                "Cloud Provider Secret Managers (AWS Secrets Manager, Azure Key Vault, GCP Secret Manager)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0012 Valid Accounts",
                    "AML.T0055 Unsecured Credentials"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Lateral Movement (Cross-Layer)",
                    "Resource Hijacking (L4)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM02:2025 Sensitive Information Disclosure (if via compromised user credentials)"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML05:2023 Model Theft (if via compromised user credentials)"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "credential",
                  "credentials",
                  "keys",
                  "secrets",
                  "unauthorized",
                  "compromise",
                  "compromised",
                  "attacker"
                ],
                "defense": [
                  "credential management",
                  "incident response",
                  "secret management",
                  "foundational",
                  "credential",
                  "management",
                  "credentials",
                  "keys",
                  "secrets",
                  "detected"
                ]
              }
            },
            {
              "id": "AID-E-001.002",
              "name": "Automated & Real-time Invalidation",
              "description": "This sub-technique covers the immediate, automated, and reactive side of credential eviction. It focuses on integrating security alerting with response workflows to automatically disable compromised credentials the moment they are detected. It also addresses the challenge of ensuring that revocations for stateless tokens (like JWTs) are propagated and enforced in real-time to immediately terminate an attacker's session.",
              "pillar": "infra",
              "phase": "response",
              "implementationStrategies": [
                "Automate credential invalidation upon security alert.",
                "Ensure prompt propagation of revocation for stateless tokens."
              ],
              "toolsOpenSource": [
                "Cloud provider automation (AWS Lambda, Azure Functions, Google Cloud Functions)",
                "SOAR platforms (Shuffle, TheHive with Cortex)",
                "In-memory caches (Redis, Memcached) for revocation lists",
                "API Gateways (Kong, Tyk)"
              ],
              "toolsCommercial": [
                "SOAR Platforms (Palo Alto XSOAR, Splunk SOAR, Torq)",
                "Cloud-native alerting/eventing (Amazon EventBridge, Azure Event Grid)",
                "EDR/XDR solutions with automated response (CrowdStrike, SentinelOne)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0012 Valid Accounts (by immediately disabling the account)"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Unauthorized access via stolen credentials (Cross-Layer)",
                    "Lateral Movement (Cross-Layer)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM02:2025 Sensitive Information Disclosure (by stopping an active breach)",
                    "LLM06:2025 Excessive Agency (by disabling a compromised agent's credentials)"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML05:2023 Model Theft (by terminating the session used for theft)"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "malicious input",
                  "invalid input",
                  "credential",
                  "compromised",
                  "credentials",
                  "tokens",
                  "attacker",
                  "prompt",
                  "injection",
                  "bypass",
                  "exploit"
                ],
                "defense": [
                  "automated",
                  "real-time",
                  "invalidation",
                  "credential",
                  "alerting",
                  "credentials",
                  "detected",
                  "tokens",
                  "enforced",
                  "session",
                  "alert"
                ]
              }
            },
            {
              "id": "AID-E-001.003",
              "name": "AI Agent & Workload Identity Revocation",
              "description": "This sub-technique covers the specialized task of revoking credentials and identities for non-human, AI-specific entities. It addresses modern, ephemeral identity types like those used by autonomous agents and containerized workloads, such as short-lived mTLS certificates, cloud workload identities (e.g., IAM Roles for Service Accounts), and SPIFFE Verifiable Identity Documents (SVIDs). The goal is to immediately evict a compromised AI workload from the trust domain.",
              "pillar": "infra",
              "phase": "response",
              "implementationStrategies": [
                "Revoke/reissue compromised AI agent cryptographic identities (SVIDs).",
                "Rotate credentials for cloud-based AI workloads (e.g., IAM Roles for Service Accounts).",
                "Use short-lived certificates and rely on expiration for mTLS revocation."
              ],
              "toolsOpenSource": [
                "Workload Identity Systems (SPIFFE/SPIRE)",
                "Service Mesh (Istio, Linkerd)",
                "Cloud provider IAM for workloads (AWS IRSA, GCP Workload Identity)",
                "Certificate management tools (cert-manager, OpenSSL)"
              ],
              "toolsCommercial": [
                "Enterprise Service Mesh (Istio-based platforms like Tetrate, Solo.io)",
                "Public Key Infrastructure (PKI) solutions (Venafi, DigiCert)",
                "Cloud Provider IAM"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0073 Impersonation"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Agent Identity Attack (L7)",
                    "Compromised Agent Registry (L7)",
                    "Lateral Movement (Cross-Layer, from a compromised agent)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM06:2025 Excessive Agency (by revoking the identity of the overreaching agent)"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML06:2023 ML Supply Chain Attacks (if a compromised workload is the result)"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "man in the middle",
                  "credentials",
                  "certificates",
                  "compromised",
                  "tampering",
                  "forgery",
                  "interception",
                  "persistence",
                  "backdoor",
                  "malware",
                  "compromise",
                  "intrusion"
                ],
                "defense": [
                  "agent",
                  "workload",
                  "identity",
                  "revocation",
                  "credentials",
                  "certificates",
                  "iam",
                  "roles",
                  "trust",
                  "cryptographic"
                ]
              }
            }
          ],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "unauthorized access",
              "credential",
              "credentials",
              "keys",
              "tokens",
              "certificates",
              "compromised",
              "adversary",
              "unauthorized",
              "maliciously",
              "attacker",
              "stolen"
            ],
            "defense": [
              "credential",
              "revocation",
              "rotation",
              "systems",
              "credentials",
              "keys",
              "tokens",
              "certificates",
              "prevent",
              "eviction",
              "removal",
              "remediation",
              "cleanup",
              "eradication",
              "purge"
            ]
          }
        },
        {
          "id": "AID-E-002",
          "name": "AI Process & Session Eviction",
          "description": "Terminate any running AI model instances, agent processes, user sessions, or containerized workloads that are confirmed to be malicious, compromised, or actively involved in an attack. This immediate action halts the adversary's ongoing activities within the AI system and removes their active foothold.",
          "pillar": "infra",
          "phase": "response",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0051 LLM Prompt Injection",
                "AML.T0054 LLM Jailbreak (terminates manipulated session)"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Agent Tool Misuse (L7)",
                "Agent Goal Manipulation (L7, terminating rogue agent)",
                "Resource Hijacking (L4, killing resource-abusing processes)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM01:2025 Prompt Injection (ending manipulated session)",
                "LLM06:2025 Excessive Agency (terminating overreaching agent)"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML06:2023 ML Supply Chain Attacks (a malicious container image introduced via the ML supply chain)"
              ]
            }
          ],
          "implementationStrategies": [
            "Identify and terminate malicious AI model or inference server processes.",
            "Evict hijacked AI agent sessions and purge poisoned runtime state.",
            "Forcefully delete or quarantine compromised Kubernetes pods/containers.",
            "Invalidate active user sessions involved in malicious or hijacked activity.",
            "Record every eviction action in a structured, tamper-resistant audit log."
          ],
          "toolsOpenSource": [
            "OS process management (kill, pkill, taskkill)",
            "Container orchestration CLIs (kubectl delete pod --force)",
            "HIPS (OSSEC, Wazuh)",
            "Custom eviction scripts for targeted session invalidation (Redis key purge by prefix)"
          ],
          "toolsCommercial": [
            "EDR solutions (CrowdStrike, SentinelOne, Carbon Black)",
            "Cloud provider management consoles/APIs for instance termination",
            "APM tools with session management"
          ],
          "subTechniques": [],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "forensic evasion",
              "foothold",
              "malicious",
              "compromised",
              "attack",
              "adversary",
              "hijacked",
              "poisoned",
              "state",
              "repudiation",
              "tampering",
              "unauthorized"
            ],
            "defense": [
              "process",
              "session",
              "eviction",
              "sessions",
              "quarantine",
              "containers",
              "audit",
              "log",
              "removal",
              "remediation",
              "cleanup",
              "eradication",
              "purge",
              "elimination",
              "termination"
            ]
          }
        },
        {
          "id": "AID-E-003",
          "name": "AI Backdoor & Malicious Artifact Removal",
          "description": "Systematically scan for, identify, and remove any malicious artifacts introduced by an attacker into the AI system. This includes backdoors in models, poisoned data, malicious code, or configuration changes designed to grant persistent access or manipulate AI behavior.",
          "pillar": "model",
          "phase": "improvement",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0011.001 User Execution: Malicious Package",
                "AML.T0018 Manipulate AI Model",
                "AML.T0018.002 Manipulate AI Model: Embed Malware",
                "AML.T0020 Poison Training Data",
                "AML.T0059 Erode Dataset Integrity",
                "AML.T0070 RAG Poisoning",
                "AML.T0071 False RAG Entry Injection (Poisoned data detection & cleansing - Scan vector databases for embeddings from known malicious contents)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM03:2025 Supply Chain",
                "LLM04:2025 Data and Model Poisoning",
                "LLM08:2025 Vector and Embedding Weaknesses"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML02:2023 Data Poisoning Attack",
                "ML06:2023 ML Supply Chain Attacks",
                "ML10:2023 Model Poisoning"
              ]
            }
          ],
          "implementationStrategies": [],
          "toolsOpenSource": [],
          "toolsCommercial": [],
          "subTechniques": [
            {
              "id": "AID-E-003.001",
              "name": "Neural Network Backdoor Detection & Removal",
              "description": "Focuses on identifying and removing backdoors embedded within neural network model parameters, including trigger-based backdoors that cause misclassification on specific inputs.",
              "pillar": "model",
              "phase": "improvement",
              "implementationStrategies": [
                "Apply neural cleanse (reverse-engineer minimal triggers) to detect backdoor classes.",
                "Use activation clustering to isolate trojan neurons and poisoned samples.",
                "Perform fine-pruning to disable malicious neurons.",
                "Retrain / fine-tune on trusted clean data to overwrite hidden backdoors.",
                "Differential testing between a suspect model and a known-good baseline model."
              ],
              "toolsOpenSource": [
                "Adversarial Robustness Toolbox (ART) by IBM (includes Neural Cleanse, Activation Defence)",
                "Foolbox (for generating triggers for testing)",
                "PyTorch",
                "TensorFlow",
                "NumPy",
                "Scikit-learn (for clustering/statistical analysis)"
              ],
              "toolsCommercial": [
                "Protect AI (ModelScan)",
                "HiddenLayer MLSec Platform",
                "Adversa.AI",
                "Bosch AIShield",
                "CognitiveScale (Cortex Certifai)",
                "IBM Watson OpenScale"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0018 Manipulate AI Model",
                    "AML.T0018.000 Manipulate AI Model: Poison AI Model",
                    "AML.T0018.002 Manipulate AI Model: Embed Malware",
                    "AML.T0020 Poison Training Data",
                    "AML.T0076 Corrupt AI Model"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Backdoor Attacks (L1)",
                    "Model Tampering (L1)",
                    "Data Poisoning (L2)",
                    "Runtime Code Injection (L4)",
                    "Evasion of Auditing/Compliance (L6)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM03:2025 Supply Chain",
                    "LLM04:2025 Data and Model Poisoning"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML10:2023 Model Poisoning",
                    "ML02:2023 Data Poisoning Attack",
                    "ML06:2023 AI Supply Chain Attacks",
                    "ML09:2023 Output Integrity Attack"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "model parameters",
                  "anomalous behavior",
                  "backdoor",
                  "parameters",
                  "trojan",
                  "poisoned",
                  "malicious",
                  "drift",
                  "deviation",
                  "degradation",
                  "manipulation",
                  "evasion",
                  "bypass",
                  "stealth",
                  "obfuscation"
                ],
                "defense": [
                  "neural",
                  "network",
                  "backdoor",
                  "detection",
                  "removal",
                  "detect",
                  "isolate",
                  "trusted",
                  "baseline",
                  "eviction",
                  "remediation",
                  "cleanup",
                  "eradication",
                  "purge",
                  "elimination"
                ]
              }
            },
            {
              "id": "AID-E-003.002",
              "name": "Poisoned Data Detection & Cleansing",
              "description": "Identifies and removes maliciously crafted data points from training sets, vector databases, or other data stores that could influence model behavior or enable attacks.",
              "pillar": "data",
              "phase": "improvement",
              "implementationStrategies": [
                "Detect statistically anomalous (poisoned) samples with outlier detection.",
                "Track provenance and block compromised data sources.",
                "Cluster embeddings (DBSCAN / density methods) to locate dense poison clusters and tiny trigger clusters.",
                "Continuously scan your RAG vector database for semantically malicious content.",
                "Verify dataset integrity against known-good cryptographic hashes before training.",
                "Remove suspicious data in controlled batches and retrain incrementally to avoid catastrophic accuracy loss."
              ],
              "toolsOpenSource": [
                "scikit-learn (for Isolation Forest, DBSCAN)",
                "Alibi Detect (for outlier and drift detection)",
                "Great Expectations (for data validation)",
                "DVC (Data Version Control)",
                "Apache Spark, Dask (for large-scale data processing)",
                "OpenMetadata, DataHub (for data provenance)",
                "FlashText (for efficient keyword matching)",
                "Sentence-Transformers (for embedding malicious concepts)",
                "Qdrant, Pinecone, Weaviate (vector databases for scanning)"
              ],
              "toolsCommercial": [
                "Databricks (Delta Lake for data quality, lineage, time travel)",
                "Alation, Collibra, Informatica (data governance, lineage, quality)",
                "Gretel.ai (synthetic data, data anonymization)",
                "Tonic.ai (data anonymization)",
                "Protect AI (for data-centric security)",
                "Fiddler AI (for data integrity monitoring)",
                "Arize AI (for data quality monitoring)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0020 Poison Training Data",
                    "AML.T0031 Erode AI Model Integrity",
                    "AML.T0059 Erode Dataset Integrity",
                    "AML.T0070 RAG Poisoning",
                    "AML.T0010.002 AI Supply Chain Compromise: Data",
                    "AML.T0018.000 Manipulate AI Model: Poison ML Model"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Data Poisoning (L2)",
                    "Data Tampering (L2)",
                    "Compromised RAG Pipelines (L2)",
                    "Model Skewing (L2)",
                    "Supply Chain Attacks (Cross-Layer)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM04:2025 Data and Model Poisoning",
                    "LLM08:2025 Vector and Embedding Weaknesses",
                    "LLM03:2025 Supply Chain"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML02:2023 Data Poisoning Attack",
                    "ML10:2023 Model Poisoning",
                    "ML08:2023 Model Skewing",
                    "ML07:2023 Transfer Learning Attack"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "vector database",
                  "poisoned",
                  "maliciously",
                  "vector",
                  "anomalous",
                  "compromised",
                  "embeddings",
                  "poison",
                  "malicious",
                  "suspicious"
                ],
                "defense": [
                  "data validation",
                  "poisoned",
                  "data",
                  "detection",
                  "cleansing",
                  "detect",
                  "track",
                  "provenance",
                  "block",
                  "scan",
                  "verify",
                  "integrity",
                  "cryptographic",
                  "controlled"
                ]
              }
            },
            {
              "id": "AID-E-003.003",
              "name": "Malicious Code & Configuration Cleanup",
              "description": "Removes malicious scripts, modified configuration files, unauthorized tools, or persistence mechanisms that attackers may have introduced into the AI system infrastructure.",
              "pillar": "infra",
              "phase": "improvement",
              "implementationStrategies": [
                "Detect unauthorized changes to ML frameworks, system binaries, and configs using File Integrity Monitoring (FIM).",
                "Statically analyze model loading code for malicious deserialization or custom loader hooks.",
                "Verify integrity of AI agent tools/plugins against a signed hash manifest at startup.",
                "Audit and remove unauthorized cron jobs / scheduled tasks used for persistence.",
                "Continuously enforce golden configuration via config management / GitOps.",
                "Scan for and remove web shells / reverse shells in agent surfaces and API-facing code."
              ],
              "toolsOpenSource": [
                "AIDE (Advanced Intrusion Detection Environment)",
                "Tripwire Open Source",
                "OSSEC (Host-based Intrusion Detection System)",
                "Wazuh (fork of OSSEC)",
                "ClamAV (antivirus engine)",
                "YARA (pattern matching tool for malware)",
                "grep (Linux utility)",
                "Ansible, Puppet, Chef (Configuration Management)",
                "Git (for configuration version control)",
                "Kubernetes (for self-healing deployments via GitOps)"
              ],
              "toolsCommercial": [
                "CrowdStrike Falcon Insight (EDR)",
                "SentinelOne Singularity (EDR)",
                "Carbon Black (VMware Carbon Black Cloud)",
                "Trellix Endpoint Security (HX)",
                "Microsoft Defender for Endpoint",
                "Splunk Enterprise Security (SIEM)",
                "Palo Alto Networks Cortex XSOAR (SOAR)",
                "Forensic tools (e.g., Magnet AXIOM, EnCase)",
                "Configuration Management Database (CMDB) solutions"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0011.001 User Execution: Malicious Package",
                    "AML.T0018 Manipulate AI Model",
                    "AML.T0018.002 Manipulate AI Model: Embed Malware",
                    "AML.T0072 Reverse Shell"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Runtime Code Injection (L4)",
                    "Memory Corruption (L4)",
                    "Misconfigurations (L4)",
                    "Backdoor Attacks (L1)",
                    "Compromised Framework Components (L3)",
                    "Compromised Container Images (L4)",
                    "Data Tampering (L2)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM03:2025 Supply Chain",
                    "LLM08:2025 Vector and Embedding Weaknesses"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML06:2023 AI Supply Chain Attacks",
                    "ML09:2023 Output Integrity Attack"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "agent tools",
                  "unauthorized modification",
                  "stealth attack",
                  "persistent threat",
                  "forensic evasion",
                  "persistence",
                  "malicious",
                  "unauthorized",
                  "tampering",
                  "corruption",
                  "manipulation",
                  "forgery",
                  "evasion",
                  "undetected",
                  "repudiation"
                ],
                "defense": [
                  "intrusion detection",
                  "malicious",
                  "code",
                  "configuration",
                  "cleanup",
                  "detect",
                  "integrity",
                  "monitoring",
                  "analyze",
                  "verify",
                  "signed",
                  "hash",
                  "audit",
                  "enforce",
                  "scan"
                ]
              }
            },
            {
              "id": "AID-E-003.004",
              "name": "Malicious Node Eviction in Graph Datasets",
              "description": "After a detection method identifies nodes that are likely poisoned or part of a backdoor trigger, this eviction technique systematically removes those nodes and their associated edges from the graph dataset. This cleansing action is performed before the final, clean Graph Neural Network (GNN) model is trained or retrained, ensuring the malicious artifacts and their influence are fully purged from the training process.",
              "pillar": "data",
              "phase": "improvement",
              "implementationStrategies": [
                "Remove confirmed malicious nodes entirely from the graph dataset before retraining.",
                "Isolate (but keep) malicious nodes by removing all their edges.",
                "Automate the detect  evict  retrain loop as an MLOps pipeline.",
                "Validate eviction effectiveness before redeploying the retrained GNN."
              ],
              "toolsOpenSource": [
                "PyTorch Geometric, Deep Graph Library (DGL)",
                "NetworkX (for graph manipulation and node/edge removal)",
                "MLOps pipelines (Kubeflow Pipelines, Apache Airflow)",
                "DVC (for versioning the cleansed graph datasets)"
              ],
              "toolsCommercial": [
                "Graph Databases (Neo4j, TigerGraph, using their query languages for removal)",
                "ML Platforms (Amazon SageMaker, Google Vertex AI, Databricks)",
                "AI Security Platforms (Protect AI, HiddenLayer)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0020 Poison Training Data",
                    "AML.T0018 Manipulate AI Model"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Data Poisoning (L2)",
                    "Backdoor Attacks (L1)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "Not directly applicable"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML02:2023 Data Poisoning Attack",
                    "ML10:2023 Model Poisoning"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "malicious",
                  "poisoned",
                  "backdoor",
                  "artifacts",
                  "evasion",
                  "bypass",
                  "stealth",
                  "obfuscation",
                  "hiding"
                ],
                "defense": [
                  "malicious",
                  "node",
                  "eviction",
                  "graph",
                  "datasets",
                  "detection",
                  "isolate",
                  "detect",
                  "validate",
                  "removal",
                  "remediation",
                  "cleanup",
                  "eradication",
                  "purge",
                  "elimination"
                ]
              }
            }
          ],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "backdoor",
              "malicious",
              "artifact",
              "artifacts",
              "attacker",
              "poisoned",
              "manipulate",
              "persistence",
              "malware",
              "compromise",
              "intrusion"
            ],
            "defense": [
              "backdoor",
              "malicious",
              "artifact",
              "removal",
              "scan",
              "eviction",
              "remediation",
              "cleanup",
              "eradication",
              "purge",
              "elimination",
              "termination"
            ]
          }
        },
        {
          "id": "AID-E-004",
          "name": "Post-Eviction System Patching & Hardening",
          "description": "After an attack vector has been identified and the adversary evicted, rapidly apply necessary security patches to vulnerable software components (e.g., ML libraries, operating systems, web servers, agent frameworks) and harden system configurations that were exploited or found to be weak. This step aims to close the specific vulnerabilities used by the attacker and strengthen overall security posture to prevent reinfection or similar future attacks.",
          "pillar": "infra",
          "phase": "improvement",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "Any technique exploiting software vulnerability or misconfiguration (e.g., AML.TA0005.001 ML Code Injection, AML.TA0004 Initial Access)",
                "AML.T0031 Erode AI Model Integrity (if due to vulnerability exploitation)"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Re-exploitation of vulnerabilities in any layer (L1-L4)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM03:2025 Supply Chain (patching vulnerable component)",
                "LLM05:2025 Improper Output Handling (patching downstream component)"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML06:2023 ML Supply Chain Attacks (if vulnerable library was entry point)"
              ]
            }
          ],
          "implementationStrategies": [
            "Apply security patches for exploited CVEs in AI stack.",
            "Review and harden abused or insecure system configurations.",
            "Strengthen IAM policies, I/O validation, and network segmentation around the compromised component.",
            "Disable unnecessary or vulnerable services, plugins, and agent tool capabilities.",
            "Codify new IOCs and TTP-based detections into SIEM/SOAR.",
            "Security regression testing: verify patches and hardening measures actually block the original exploit."
          ],
          "toolsOpenSource": [
            "Package managers (apt, yum, pip, conda)",
            "Configuration management tools (Ansible, Chef, Puppet)",
            "Vulnerability scanners (OpenVAS, Trivy)",
            "Static analysis tools (Bandit)"
          ],
          "toolsCommercial": [
            "Automated patch management solutions (Automox, ManageEngine)",
            "CSPM tools",
            "Vulnerability management platforms (Tenable, Rapid7)",
            "SCA tools (Snyk, Mend)"
          ],
          "subTechniques": [],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "attack",
              "vector",
              "adversary",
              "vulnerable",
              "exploited",
              "vulnerabilities",
              "attacker",
              "abused",
              "insecure",
              "compromised",
              "exploit"
            ],
            "defense": [
              "network segmentation",
              "vulnerability scanning",
              "static analysis",
              "post-eviction",
              "system",
              "patching",
              "hardening",
              "harden",
              "prevent",
              "iam",
              "policies",
              "validation",
              "siem",
              "soar",
              "verify"
            ]
          }
        },
        {
          "id": "AID-E-005",
          "name": "Compromised Session Termination & State Purging",
          "description": "When communication channels or user/agent sessions are suspected or confirmed compromised, immediately expel the adversary and dismantle all footholds. This includes terminating active sessions, revoking tokens, purging tainted conversational memory, and disabling malicious execution paths like unknown webhooks or queued jobs. The goal is to prevent any residual access so the attacker cannot continue or instantly re-enter.",
          "pillar": "infra",
          "phase": "response",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0012 Valid Accounts (evicting hijacked sessions)",
                "AML.TA0006 Persistence (dismantling live footholds / manipulated state)"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Agent Identity Attack (L7, cutting compromised sessions/tokens)",
                "Session Hijacking affecting any AI layer"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM01:2025 Prompt Injection (purging poisoned states)",
                "LLM02:2025 Sensitive Information Disclosure (stopping leaks from ongoing sessions)"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "Any attack involving session hijacking or manipulation of ongoing ML API interactions."
              ]
            }
          ],
          "implementationStrategies": [
            "Expire or mass-invalidate active sessions / cookies / API tokens linked to the compromise.",
            "Revoke stateless tokens (JWT, API tokens) and rotate signing keys.",
            "Purge tainted conversational memory / agent state so the attackers injected goals cannot respawn.",
            "Dismantle runtime footholds: malicious webhooks, rogue tool registrations, queued jobs, or background workers."
          ],
          "toolsOpenSource": [
            "Application server admin interfaces for session expiration",
            "Custom scripts using JWT libraries or flushing session stores (Redis, Memcached)",
            "IAM systems (Keycloak) with session termination APIs"
          ],
          "toolsCommercial": [
            "IDaaS platforms (Okta, Auth0, Ping Identity) with global session termination features",
            "API Gateways with advanced session management",
            "SIEM/SOAR platforms for orchestrating automated eviction actions (Splunk SOAR, Palo Alto XSOAR)"
          ],
          "subTechniques": [],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "foothold",
              "compromised",
              "state",
              "adversary",
              "tokens",
              "memory",
              "malicious",
              "attacker",
              "compromise",
              "keys",
              "rogue"
            ],
            "defense": [
              "compromised",
              "session",
              "termination",
              "state",
              "purging",
              "sessions",
              "tokens",
              "prevent",
              "signing",
              "keys"
            ]
          }
        },
        {
          "id": "AID-E-006",
          "name": "End-of-Life (EOL) of Models, Configurations and Data",
          "description": "Implements formal, verifiable technical decommissioning procedures to securely and permanently delete or dispose of AI models, configurations, and their associated data at the end of their lifecycle or upon a transfer of ownership. This technique ensures that residual data cannot be recovered and that security issues from a decommissioned system cannot be transferred to another.",
          "pillar": "data",
          "phase": "improvement",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0025 Exfiltration via Cyber Means",
                "AML.T0036 Data from Information Repositories",
                "AML.T0048.004 External Harms: AI Intellectual Property Theft"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Data Exfiltration (L2)",
                "Model Stealing (L1, from old artifacts)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM02:2025 Sensitive Information Disclosure"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML03:2023 Model Inversion Attack",
                "ML04:2023 Membership Inference Attack",
                "ML05:2023 Model Theft"
              ]
            }
          ],
          "implementationStrategies": [],
          "toolsOpenSource": [],
          "toolsCommercial": [],
          "subTechniques": [
            {
              "id": "AID-E-006.001",
              "name": "Cryptographic Erasure & Media Sanitization",
              "description": "Employs cryptographic and physical methods to render AI data and models on storage media permanently unrecoverable. This is the core technical process for decommissioning AI assets, ensuring compliance with data protection regulations and preventing future data leakage.",
              "pillar": "data",
              "phase": "improvement",
              "implementationStrategies": [
                "Perform crypto-shredding by destroying the encryption keys for model/data storage at end-of-life.",
                "Sanitize or physically destroy storage media using standards-compliant wiping."
              ],
              "toolsOpenSource": [
                "shred, nwipe (for command-line data wiping)",
                "Cryptsetup (for LUKS key management and destruction on Linux systems)"
              ],
              "toolsCommercial": [
                "Cloud Provider KMS (AWS KMS, Azure Key Vault, GCP Secret Manager)",
                "Hardware Security Modules (HSMs)",
                "Enterprise data destruction software and services (Blancco, KillDisk)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0025 Exfiltration via Cyber Means",
                    "AML.T0036 Data from Information Repositories"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Data Exfiltration (L2)",
                    "Model Stealing (L1, from residual artifacts)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM02:2025 Sensitive Information Disclosure"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML03:2023 Model Inversion Attack",
                    "ML04:2023 Membership Inference Attack",
                    "ML05:2023 Model Theft"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "data leakage",
                  "man in the middle",
                  "malicious input",
                  "data exfiltration",
                  "assets",
                  "leakage",
                  "keys",
                  "tampering",
                  "forgery",
                  "interception",
                  "injection",
                  "poisoning",
                  "exploit",
                  "xss",
                  "sqli"
                ],
                "defense": [
                  "secret management",
                  "key management",
                  "cryptographic",
                  "erasure",
                  "media",
                  "sanitization",
                  "compliance",
                  "protection",
                  "preventing",
                  "encryption",
                  "keys",
                  "sanitize"
                ]
              }
            },
            {
              "id": "AID-E-006.002",
              "name": "Secure Asset Transfer & Ownership Change",
              "description": "Defines the technical process for securely transferring ownership of an AI asset to another entity. This involves cryptographic verification of the transferred artifact and a corresponding secure deletion of the original asset to prevent residual security risks.",
              "pillar": "model",
              "phase": "improvement",
              "implementationStrategies": [
                "Package, encrypt, sign, and attest AI assets before transfer to a new owner.",
                "After confirmed receipt, eradicate all original copies (including backups) and revoke original access paths."
              ],
              "toolsOpenSource": [
                "GnuPG (GPG)",
                "OpenSSL",
                "sha256sum, md5sum",
                "rsync (over SSH for secure transport)"
              ],
              "toolsCommercial": [
                "Secure File Transfer Protocol (SFTP) solutions",
                "Managed File Transfer (MFT) platforms",
                "Data Loss Prevention (DLP) systems to monitor the transfer"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0025 Exfiltration via Cyber Means",
                    "AML.T0048.004 External Harms: AI Intellectual Property Theft",
                    "AML.T0010 AI Supply Chain Compromise"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Data Exfiltration (L2)",
                    "Model Stealing (L1, from retained copies)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM02:2025 Sensitive Information Disclosure",
                    "LLM03:2025 Supply Chain"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML05:2023 Model Theft",
                    "ML06:2023 AI Supply Chain Attacks"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "man in the middle",
                  "data destruction",
                  "availability attack",
                  "artifact",
                  "risks",
                  "assets",
                  "tampering",
                  "forgery",
                  "interception",
                  "ransomware",
                  "corruption"
                ],
                "defense": [
                  "secure",
                  "asset",
                  "transfer",
                  "ownership",
                  "change",
                  "cryptographic",
                  "verification",
                  "prevent",
                  "encrypt",
                  "sign"
                ]
              }
            }
          ],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "persistence",
              "backdoor",
              "malware",
              "compromise",
              "intrusion"
            ],
            "defense": [
              "end-of-life",
              "eol",
              "models",
              "configurations",
              "data",
              "recovered",
              "eviction",
              "removal",
              "remediation",
              "cleanup",
              "eradication",
              "purge",
              "elimination",
              "termination"
            ]
          }
        }
      ]
    },
    {
      "id": "restore",
      "name": "Restore",
      "description": "The \"Restore\" tactic focuses on recovering normal AI system operations and data integrity following an attack and subsequent eviction of the adversary. This phase involves safely bringing AI models and applications back online, restoring any corrupted or lost data from trusted backups, and, crucially, learning from the incident to reinforce defenses and improve future resilience.",
      "techniques": [
        {
          "id": "AID-R-001",
          "name": "Secure AI Model Restoration & Retraining",
          "description": "After an incident that may have compromised AI model integrity (e.g., through data poisoning, model poisoning, backdoor insertion, or unauthorized modification), securely restore affected models to a known-good state. This may involve deploying models from trusted, verified backups taken prior to the incident, or, if necessary, retraining or fine-tuning models on clean, validated datasets to eliminate any malicious influence or corruption.",
          "pillar": "model",
          "phase": "improvement",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0018 Manipulate AI Model / AML.T0019 Publish Poisoned Datasets",
                "AML.T0020 Poison Training Data",
                "AML.T0031 Erode AI Model Integrity"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Backdoor Attacks (L1)",
                "Data Poisoning (L2, retraining)",
                "Model Skewing (L2, restoring/retraining)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM04:2025 Data and Model Poisoning"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML10:2023 Model Poisoning",
                "ML02:2023 Data Poisoning Attack"
              ]
            }
          ],
          "implementationStrategies": [],
          "toolsOpenSource": [],
          "toolsCommercial": [],
          "subTechniques": [
            {
              "id": "AID-R-001.001",
              "name": "Versioned Model Rollback & Restoration",
              "description": "Restores a compromised AI model to a known-good state by deploying a trusted, previously saved version from a secure artifact repository or model registry. This technique is the primary recovery method when a deployed model artifact has been tampered with post-deployment or when an incident requires reverting to the last known-secure version. It relies on maintaining immutable, versioned, and verifiable backups of all production models.",
              "pillar": "model",
              "phase": "improvement",
              "implementationStrategies": [
                "Maintain immutable, versioned backups of all production model artifacts with recorded hashes and attestations.",
                "Identify the last known-good model version that predates the incident window.",
                "Verify the integrity and provenance of the backup artifact before redeployment.",
                "Use an audited, parameterized rollback pipeline to redeploy the known-good model and its serving environment."
              ],
              "toolsOpenSource": [
                "MLflow Model Registry, DVC",
                "Cloud provider CLIs/SDKs (for S3, GCS, Azure Blob Storage)",
                "CI/CD systems (GitHub Actions, GitLab CI, Jenkins)",
                "Cryptographic tools (sha256sum, GnuPG)"
              ],
              "toolsCommercial": [
                "Enterprise MLOps platforms (Databricks, Amazon SageMaker, Google Vertex AI)",
                "Enterprise artifact repositories (JFrog Artifactory)",
                "Backup and recovery solutions (Veeam, Rubrik, Cohesity)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0018 Manipulate AI Model",
                    "AML.T0076 Corrupt AI Model"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Model Tampering (L1)",
                    "Compromised Container Images (L4)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM03:2025 Supply Chain",
                    "LLM04:2025 Data and Model Poisoning"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML06:2023 AI Supply Chain Attacks",
                    "ML10:2023 Model Poisoning"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "data poisoning",
                  "supply chain attack",
                  "untrusted data",
                  "unauthorized modification",
                  "compromised",
                  "state",
                  "artifact",
                  "tampered",
                  "artifacts",
                  "tampering",
                  "corruption",
                  "manipulation",
                  "forgery",
                  "repudiation",
                  "unauthorized"
                ],
                "defense": [
                  "model registry",
                  "versioned",
                  "model",
                  "rollback",
                  "restoration",
                  "trusted",
                  "secure",
                  "repository",
                  "registry",
                  "recovery",
                  "reverting",
                  "verify",
                  "integrity",
                  "provenance",
                  "backup"
                ]
              }
            },
            {
              "id": "AID-R-001.002",
              "name": "Model Retraining for Remediation",
              "description": "This sub-technique covers the restoration workflow for when a model's integrity has been compromised by its training data (e.g., via data poisoning or backdoor attacks). It involves a multi-stage process: first, identifying and removing the malicious data or client influence; second, retraining a new model from scratch using only the resulting sanitized dataset; and third, validating that the new model is both secure and performant. This approach is more comprehensive than a simple rollback and is necessary when no trusted model backup exists.",
              "pillar": "model",
              "phase": "improvement",
              "implementationStrategies": [
                "Identify and evict malicious data points, poisoned graph nodes, or compromised federated clients before retraining.",
                "Trigger a secure, isolated full retraining job using only the cleansed dataset.",
                "Apply approximate unlearning for Federated Learning or large-scale distributed models when full retraining is prohibitively expensive.",
                "Run a post-remediation validation suite to prove the new model is both safe and still useful."
              ],
              "toolsOpenSource": [
                "MLOps platforms (MLflow, Kubeflow Pipelines, DVC)",
                "Federated Learning frameworks (TensorFlow Federated, Flower, PySyft)",
                "Graph ML libraries (PyTorch Geometric, DGL)",
                "Data cleansing/validation tools (Great Expectations, Pandas)",
                "Deep learning frameworks (PyTorch, TensorFlow)"
              ],
              "toolsCommercial": [
                "Enterprise MLOps platforms (Databricks, Amazon SageMaker, Google Vertex AI, Azure ML)",
                "Data quality and governance platforms (Alation, Collibra)"
              ],
              "defendsAgainst": [
                {
                  "framework": "MITRE ATLAS",
                  "items": [
                    "AML.T0020 Poison Training Data",
                    "AML.T0018 Manipulate AI Model",
                    "AML.T0019 Publish Poisoned Datasets",
                    "AML.T0059 Erode Dataset Integrity"
                  ]
                },
                {
                  "framework": "MAESTRO",
                  "items": [
                    "Data Poisoning (L2)",
                    "Backdoor Attacks (L1)",
                    "Model Skewing (L2)",
                    "Attacks on Decentralized Learning (Cross-Layer)"
                  ]
                },
                {
                  "framework": "OWASP LLM Top 10 2025",
                  "items": [
                    "LLM04:2025 Data and Model Poisoning"
                  ]
                },
                {
                  "framework": "OWASP ML Top 10 2023",
                  "items": [
                    "ML02:2023 Data Poisoning Attack",
                    "ML10:2023 Model Poisoning"
                  ]
                }
              ],
              "keywords": {
                "attack": [
                  "data poisoning",
                  "backdoor attack",
                  "unauthorized modification",
                  "malicious input",
                  "compromised",
                  "poisoning",
                  "backdoor",
                  "malicious",
                  "poisoned",
                  "tampering",
                  "corruption",
                  "manipulation",
                  "forgery",
                  "injection",
                  "bypass"
                ],
                "defense": [
                  "model backup",
                  "federated learning",
                  "model",
                  "retraining",
                  "remediation",
                  "restoration",
                  "integrity",
                  "sanitized",
                  "validating",
                  "secure",
                  "rollback",
                  "trusted",
                  "backup",
                  "isolated",
                  "validation"
                ]
              }
            }
          ],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "data poisoning",
              "unauthorized modification",
              "data destruction",
              "availability attack",
              "compromised",
              "poisoning",
              "backdoor",
              "unauthorized",
              "state",
              "malicious",
              "corruption",
              "tampering",
              "manipulation",
              "forgery",
              "ransomware"
            ],
            "defense": [
              "secure",
              "model",
              "restoration",
              "retraining",
              "integrity",
              "restore",
              "trusted",
              "verified",
              "validated",
              "recovery",
              "backup",
              "rollback",
              "checkpoint",
              "failover",
              "resilience"
            ]
          }
        },
        {
          "id": "AID-R-002",
          "name": "Data Integrity Recovery for AI Systems",
          "description": "Restore the integrity of any datasets used by or generated by AI systems that were corrupted, tampered with, or maliciously altered during a security incident. This includes training data, validation data, vector databases for RAG, embeddings stores, configuration data, or logs of AI outputs. Recovery typically involves reverting to known-good backups, using data validation tools to identify and correct inconsistencies, or, in some cases, reconstructing data if backups are insufficient or also compromised.",
          "pillar": "data",
          "phase": "improvement",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0020 Poison Training Data (restoring clean dataset)",
                "AML.T0031 Erode AI Model Integrity (restoring corrupted data stores)",
                "AML.T0059 Erode Dataset Integrity"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Data Poisoning / Data Tampering (L2)",
                "Compromised RAG Pipelines (L2, restoring vector DBs)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM04:2025 Data and Model Poisoning (restoring dataset integrity)",
                "LLM08:2025 Vector and Embedding Weaknesses (if vector DBs corrupted)"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML02:2023 Data Poisoning Attack (restoring clean training data)"
              ]
            }
          ],
          "implementationStrategies": [
            "Identify all affected data stores, downstream assets, and derived artifacts.",
            "Restore each affected data store from the most recent trusted, pre-incident backup or snapshot.",
            "If no clean backup exists, reconstruct or repair data in place using deterministic rules and validation logic.",
            "Re-validate integrity, schema, and statistical consistency of restored/repaired data before reuse.",
            "Harden ingestion and update pipelines to prevent recurrence of the same corruption pattern."
          ],
          "toolsOpenSource": [
            "Database backup/restore utilities (pg_dump, mysqldump)",
            "Cloud provider snapshot/backup services (S3 versioning, Azure Blob snapshots)",
            "Great Expectations",
            "Filesystem backup tools (rsync, Bacula)",
            "Vector DB export/import utilities"
          ],
          "toolsCommercial": [
            "Enterprise backup/recovery solutions (Rubrik, Cohesity, Veeam)",
            "Data quality/integration platforms (Informatica, Talend)",
            "Cloud provider managed backup services (AWS Backup, Azure Backup)"
          ],
          "subTechniques": [],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "vector database",
              "corrupted",
              "tampered",
              "maliciously",
              "vector",
              "embeddings",
              "compromised",
              "assets",
              "artifacts",
              "corruption"
            ],
            "defense": [
              "data validation",
              "data",
              "integrity",
              "recovery",
              "systems",
              "restore",
              "validation",
              "reverting",
              "trusted",
              "backup",
              "snapshot",
              "rules",
              "restored",
              "harden",
              "update"
            ]
          }
        },
        {
          "id": "AID-R-003",
          "name": "Secure Session & Identity Re-establishment",
          "description": "After an eviction (AID-E-005) is complete, this technique re-establishes clean, trusted interactions for users and AI agents. It focuses on hardening the recovery process by enforcing strong re-authentication (MFA/step-up), ensuring modern TLS, restoring clean conversational context from trusted snapshots, and progressively re-enabling capabilities. This includes clear communication with legitimate users and maintaining heightened monitoring to detect and prevent immediate re-compromise.",
          "pillar": "infra",
          "phase": "improvement",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0012 Valid Accounts (by hardening the re-authentication of a previously hijacked account)"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Agent Identity Attack (L7, by forcing re-authentication and restoring trusted state)",
                "Evasion of Auditing/Compliance (L6, by ensuring communication and monitoring)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM01:2025 Prompt Injection (restoring clean context)",
                "LLM02:2025 Sensitive Information Disclosure (by preventing re-compromise)"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "Helps prevent re-exploitation of attacks that rely on credential theft."
              ]
            }
          ],
          "implementationStrategies": [
            "Ensure re-established sessions use strong authentication (MFA / step-up) and hardened transport (TLS/HSTS).",
            "Restore clean conversational / agent state cautiously and progressively re-enable capabilities.",
            "Communicate the session reset and new login requirements clearly to legitimate users.",
            "Monitor newly established sessions at elevated sensitivity for signs of re-compromise."
          ],
          "toolsOpenSource": [
            "IAM systems (Keycloak, FreeIPA) for MFA policy enforcement",
            "Customer communication scripting libraries (for notifications)",
            "SIEM/log analytics platforms (ELK Stack, OpenSearch, Grafana Loki) for monitoring"
          ],
          "toolsCommercial": [
            "IDaaS platforms (Okta, Auth0, Ping Identity) for MFA policies",
            "Customer communication platforms (Twilio, SendGrid)",
            "SIEM/SOAR platforms for monitoring and correlation (Splunk, Microsoft Sentinel, Datadog)"
          ],
          "subTechniques": [],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "credential theft",
              "unauthorized access",
              "stealth attack",
              "persistent threat",
              "data destruction",
              "denial of service",
              "context",
              "state",
              "impersonation",
              "spoofing",
              "evasion",
              "undetected",
              "ransomware",
              "corruption",
              "exploitation"
            ],
            "defense": [
              "policy enforcement",
              "secure",
              "session",
              "identity",
              "re-establishment",
              "authentication",
              "trusted",
              "hardening",
              "recovery",
              "enforcing",
              "mfa",
              "tls",
              "restoring",
              "snapshots",
              "monitoring"
            ]
          }
        },
        {
          "id": "AID-R-004",
          "name": "Post-Incident Hardening, Verification & Institutionalization",
          "description": "Following containment and recovery, convert the incident into durable security improvements. This includes: (1) producing a formal, version-controlled Post-Incident Review (PIR) that documents root cause and precise TTPs; (2) updating threat models and risk scores based on real evidence; (3) enforcing fixes through engineering tickets with measurable acceptance tests; (4) validating fixes via targeted security testing and storing proof; (5) updating policy-as-code, IaC modules, shared security libraries, and CI/CD lint rules so that the same class of failure cannot silently recur; and (6) generating auditable communication/notification artifacts for legal, compliance, and (if needed) customers without leaking exploit detail.",
          "pillar": "data",
          "phase": "improvement",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "Reduces likelihood of repeat execution of previously successful techniques such as AML.T0020 Poison Training Data, AML.T0018 Manipulate AI Model, AML.T0031 Erode AI Model Integrity, by turning those observed TTPs into hardened controls, SIEM/SOAR detections, and regression tests.",
                "Improves long-term resilience against Persistence (AML.T0017) and Valid Accounts abuse (AML.T0012) by forcing re-authentication policy updates and session hygiene to become standard practice after incidents."
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "L1 / L2 model manipulation and data poisoning attacks become part of automated regression tests and MLOps gating, reducing the chance of re-poisoning or model tampering on redeploy.",
                "L6 Security & Compliance: creates auditable PIR artifacts, legal/regulatory notification bundles, and communication matrices so that future high-severity incidents can be handled consistently and provably."
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM01:2025 Prompt Injection (lessons are codified into hardened input/output sanitizers and enforced via shared libraries and CI lint rules).",
                "LLM02:2025 Sensitive Information Disclosure (post-incident rollout of mandatory PII redaction filters and corresponding SIEM alerts).",
                "LLM04:2025 Data and Model Poisoning (threat model likelihood raised, regression tests added for data poisoning vectors).",
                "LLM03:2025 Supply Chain (root-caused attacks on components/tooling are converted into policy-as-code hardening and artifact integrity verification steps)."
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML02:2023 Data Poisoning Attack and ML10:2023 Model Poisoning: produces cleansed training pipelines, adds validation gates, and requires cryptographic integrity checks before redeployment.",
                "ML06:2023 AI Supply Chain Attacks: forces codebase / IaC / dependency hardening and artifact signing to become default in future releases."
              ]
            }
          ],
          "implementationStrategies": [
            "Run a structured Post-Incident Review (PIR) / Root Cause Analysis (RCA) and store it as an auditable artifact in version control.",
            "Maintain a pre-approved, version-controlled incident communication and escalation matrix consumable by SOAR.",
            "Produce two reports per incident: an internal forensic report (full TTP, hashes, affected assets) and an external/regulator-safe summary (impact + remediation, no exploit PoC).",
            "Trigger regulatory / legal notification workflow when regulated data or protected jurisdictions are implicated, and generate an auditable evidence bundle.",
            "Update threat models, risk scoring, and AIDEFEND control prioritization based on real incident evidence, and commit those changes to version control.",
            "Translate PIR action items into enforceable engineering tickets with mapped AIDEFEND techniques, SLAs, owners, and machine-checkable acceptance tests.",
            "Perform targeted regression validation of fixes using AI-specific security testing and archive the results as evidence.",
            "Institutionalize the lessons: update policy-as-code, shared security libraries, IaC modules, and CI/CD lint rules so that the fixed control becomes the new default.",
            "Share sanitized TTPs and mitigations with trusted intelligence channels (e.g. ISAC, MISP) without exposing proprietary details."
          ],
          "toolsOpenSource": [
            "MITRE ATLAS Navigator (to map observed TTPs)",
            "AI red teaming / LLM security testing frameworks (garak, Counterfit, vigil-llm)",
            "Breach and Attack Simulation tooling / replay harnesses for AI systems",
            "Great Expectations / data quality validation suites for post-recovery data checks",
            "SIEM / log analytics platforms (ELK Stack, OpenSearch, Grafana Loki)",
            "MISP (Malware Information Sharing Platform) for sanitized TTP & IOC sharing",
            "Version control systems (Git, GitHub/GitLab) for PIR, threat model diffs, and evidence artifacts"
          ],
          "toolsCommercial": [
            "SOAR/SIEM platforms (Splunk, Microsoft Sentinel, Datadog) for correlation rules and automated paging",
            "BAS / adversarial simulation platforms for regression validation of fixes",
            "Enterprise MLOps / model registries (SageMaker, Vertex AI, Databricks) to tie incident timeline to model versions",
            "GRC / compliance platforms for audit tracking and regulatory notification evidence bundles",
            "Managed threat intelligence / ISAC-style sharing channels for sanitized TTP distribution"
          ],
          "subTechniques": [],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "malicious input",
              "invalid input",
              "threat",
              "risk",
              "artifacts",
              "leaking",
              "exploit",
              "artifact",
              "assets",
              "exposing",
              "injection",
              "bypass",
              "repudiation",
              "tampering",
              "unauthorized"
            ],
            "defense": [
              "version control",
              "threat modeling",
              "post-incident",
              "hardening",
              "verification",
              "institutionalization",
              "containment",
              "recovery",
              "updating",
              "enforcing",
              "validating",
              "rules",
              "compliance",
              "analysis",
              "control"
            ]
          }
        },
        {
          "id": "AID-R-005",
          "name": "Rapid Vector Index Rollback & Quarantine",
          "description": "Provide fast, controlled recovery for Retrieval-Augmented Generation (RAG) pipelines after index poisoning or malicious content injection is detected. The vector index is treated as an immutable, versioned artifact. Recovery happens in three stages: (1) atomically roll back to a last known-good snapshot, (2) quarantine and preserve suspicious chunks for forensics while removing them from production retrieval, and (3) rebuild and re-promote a clean index that enforces provenance, policy, and security approvals. Goal: restore trustworthy retrieval quality quickly while preventing re-serving tainted data.",
          "pillar": "data",
          "phase": "improvement",
          "defendsAgainst": [
            {
              "framework": "MITRE ATLAS",
              "items": [
                "AML.T0070 RAG Poisoning (malicious or manipulative content injected into the retrieval corpus)",
                "AML.T0059 Erode Dataset Integrity (attacker alters knowledge base quality/accuracy to steer model behavior)",
                "AML.T0025 Exfiltration via Cyber Means (attacker plants prompts/instructions that coerce the model to leak internal secrets)"
              ]
            },
            {
              "framework": "MAESTRO",
              "items": [
                "Compromised RAG Pipelines (L2, untrusted or adversarial retrieval context fed to the model/agent)",
                "Data Poisoning (L2, subversion of the retrieval knowledge source to mislead downstream reasoning)"
              ]
            },
            {
              "framework": "OWASP LLM Top 10 2025",
              "items": [
                "LLM08:2025 Vector and Embedding Weaknesses (poisoned or weaponized embeddings/chunks inside the vector DB)",
                "LLM04:2025 Data and Model Poisoning (index corruption that influences model behavior through retrieval context)"
              ]
            },
            {
              "framework": "OWASP ML Top 10 2023",
              "items": [
                "ML02:2023 Data Poisoning Attack (insertion of polluted or malicious data into training/evaluation/retrieval stores)"
              ]
            }
          ],
          "implementationStrategies": [
            "Use immutable versioned indices and an alias/pointer for atomic cutover and rollback.",
            "Quarantine and preserve suspicious chunks before removal from production retrieval.",
            "Rebuild a clean candidate index from trusted sources with provenance, policy scanning, and approval gating before promotion.",
            "Continuously monitor retrieval context for high-risk patterns and auto-trigger rollback/quarantine."
          ],
          "toolsOpenSource": [
            "Qdrant (collections, scroll/delete)",
            "Milvus / FAISS (vector storage, offline rebuild and re-ingest)",
            "OpenSearch / Elasticsearch (index aliases for atomic cutover)",
            "Great Expectations or custom content/policy scanners for pre-ingest validation",
            "Sigstore / cosign / GPG for signed manifests and change approval evidence",
            "sha256sum for per-chunk integrity tracking"
          ],
          "toolsCommercial": [
            "Pinecone (namespaces / collection versions / delete-by-ID)",
            "Weaviate Enterprise (schema hooks, metadata policies, ACLs)",
            "Amazon OpenSearch Service (managed aliases, snapshots)",
            "Datadog / Splunk / Microsoft Sentinel for retrieval anomaly alerting and correlation",
            "Cloud object storage with Object Lock / WORM mode for forensic quarantine (e.g. S3 Object Lock)"
          ],
          "subTechniques": [],
          "url": "https://aidefend.net",
          "keywords": {
            "attack": [
              "data poisoning",
              "supply chain attack",
              "untrusted data",
              "data destruction",
              "denial of service",
              "vector",
              "poisoning",
              "malicious",
              "injection",
              "artifact",
              "suspicious",
              "context",
              "tampering",
              "ransomware",
              "corruption"
            ],
            "defense": [
              "rapid",
              "vector",
              "index",
              "rollback",
              "quarantine",
              "controlled",
              "recovery",
              "detected",
              "versioned",
              "snapshot",
              "provenance",
              "policy",
              "restore",
              "trustworthy",
              "preventing"
            ]
          }
        }
      ]
    }
  ]
}